6
1
0
2

 
r
a

M
3

 

 
 
]

R
C
.
s
c
[
 
 

1
v
4
4
2
1
0

.

3
0
6
1
:
v
i
X
r
a

Principles of Layered Attestation

Paul D. Rowe

prowe@mitre.org

The MITRE Corporation

Abstract. Systems designed with measurement and attestation in mind
are often layered, with the lower layers measuring the layers above them.
Attestations of such systems, which we call layered attestations, must
bundle together the results of a diverse set of application-speciï¬c mea-
surements of various parts of the system. Some methods of layered at-
testation are more trustworthy than others, so it is important for system
designers to understand the trust consequences of diï¬€erent system con-
ï¬gurations. This paper presents a formal framework for reasoning about
layered attestations, and provides generic reusable principles for achiev-
ing trustworthy results.

1 Introduction

Security decisions often rely on trust. Many computing architectures have been
designed to help establish the trustworthiness of a system through remote attes-
tation. They gather evidence of the integrity of a target system and report it to a
remote party who appraises the evidence as part of a security decision. A simple
example is a network gateway that requests evidence that a target system has
recently run antivirus software before granting it access to a network. If the virus
scan indicates a potential infection, or does not oï¬€er recent evidence, the gate-
way might decide to deny access, or perhaps divert the system to a remediation
network. Of course the antivirus software itself is part of the target system, and
the gateway may require integrity evidence for the antivirus for its own security
decision. This leads to the design of layered systems in which deeper layers are
responsible for generating integrity evidence of the layers above them.

A simple example of a layered system is one that supports â€œtrusted bootâ€ in
which a chain of boot-time integrity evidence is generated for a trusted comput-
ing base that supports the upper layers of the system. A more complex example
might be a virtualized cloud architecture. The virtual machines (VMs) at the
top are supported at a lower layer by a hypervisor or virtual machine monitor.
Such an architecture may be augmented with additional VMs at an intermedi-
ate layer that are responsible for measuring the main VMs to generate integrity
evidence. These designs oï¬€er exciting possibilities for remote attestation. They
allow for specialization and diversity of the components involved, tailoring the
capabilities of measurers to their targets of measurement, and composing them
in novel ways.

However, the resulting layered attestations are typically more complex and
challenging to analyze. Given a target system, what set of evidence should an
appraiser request? What extra guarantees are provided if it receives integrity
evidence of the measurers themselves? Does the order in which the measurements
are taken matter? Can the appraiser tell if the correct sequence of measurements
was taken?

This paper begins to tame the complexity surrounding attestations of these
layered systems. We provide a formal model of layered measurement and attes-
tation systems that abstracts away the underlying details of the measurements
and focuses on the causal relationships among component corruption, measure-
ment, and reporting. The model allows us to provide and justify generic, reusable
strategies both for measuring system components and reporting the resulting in-
tegrity evidence.

Limitations of measurement. Our starting point for this paper is the recog-
nition of the fact that measurement cannot prevent corruption; at best, mea-
surement only detects corruption. In particular, the runtime corruption of a
component can occur even if it is launched in a known good state. An appraiser
must therefore always be wary of the gap between the time a component is mea-
sured and the time at which a trust decision is made. If the gap is large then so
is the risk of a time-of-check-to-time-of-use (TOCTOU) attack in which an ad-
versary corrupts a component during the critical time window to undermine the
trust decision. A successful measurement strategy will limit the risk of TOCTOU
attacks by ensuring the time between a measurement and a security decision is
suï¬ƒciently small. The appraiser can then conclude that if the measured compo-
nent is currently corrupted, it must be because the adversary performed a recent
attack.

Shortening the time between measurement and security decision, however,
is eï¬€ective only if the measurement component can be trusted. By corrupting
the measurer, an adversary can lie about the results of measurement making
a corrupted target component appear to be in a good state. This aï¬€ords the
adversary a much larger window of opportunity to corrupt the target. The cor-
ruption no longer has to take place in the small window between measurement
and security decision because the target can already be corrupted at the time of
(purported) measurement. However, in a typical layered system design, deeper
components such as a measurer have greater protections making it harder for an
adversary to corrupt them. This suggests that to escape the burden performing
a recent corruption, an adversary should have to pay the price of corrupting a
deep component.

Formal model of measurement and attestation. With this in mind, our
ï¬rst main contribution is a formal model designed to aid in reasoning about
what an adversary must do in order to defeat a measurement and attestation
strategy. Rather than forbid the adversary from performing TOCTOU attacks
in small windows or from corrupting deep components, we consider an attesta-
tion to be successful if the only way for the adversary to defeat its goals is to

2

perform such diï¬ƒcult tasks. Thus our model accounts for the possibility that an
adversary might corrupt (and repair) arbitrary system components at any time.
The model also features a true concurrency execution semantics which allows us
to reason more directly about the causal eï¬€ects of corruptions on the outcomes
of measurement without having to reason about unnecessary interleavings of
events. It has an added beneï¬t of admitting a natural, graphical representation
that helps an analyst quickly understand the causal relationships between events
of an execution.

We demonstrate the utility of this formal model by validating the eï¬€ective-
ness of two strategies, one for the order in which to take measurements, the
other for how to report the results in quotes from Trusted Platform Modules
(TPMs). TPM is not the only technology available that provides a hardware
root of trust for reporting. Indeed solutions may be conceived that use other
external hardware security modules or emerging hardware support for trusted
execution environments such as Intelâ€™s SGX. However, most of the research on
attestation is based on using a TPM as the hardware root of trust for report-
ing, and in this work, we follow that trend. We formally prove that under some
assumptions about measurement and the behavior of uncorrupted components,
in order for the adversary to defeat an attestation, he must perform some cor-
ruption which is â€œdiï¬ƒcult.â€ The result is relatively concrete advice that can be
applied by those building and conï¬guring attestation systems. By implement-
ing our general strategies and assumptions, layered systems can engage in more
trustworthy attestations than might otherwise result.

Strategy for measurement. An intuition manifest in much of the literature
on measurement and attestation is that trust in a system should be based on
a bottom-up chain of measurements starting with a hardware root of trust for
measurement. This is the core idea behind trusted boot processes, in which one
component in the boot sequence measures the next component before launching
it. Theorem 1, which we refer to as the â€œrecent or deepâ€ theorem, validates
this common intuition and solidiï¬es exactly how an adversary can defeat such
bottom-up measurement strategies. It roughly says the following:

If a system has measured deeper components before more shallow ones,
then the only way for the adversary to corrupt a component t without
detection is either by recently corrupting one of tâ€™s dependencies, or else
by corrupting a component even deeper in the system.

Strategy for bundling evidence. Given the importance of the order of mea-
surement, it is also important for an attestation to reliably convey not only the
outcome of measurements, but the order in which they were taken. This point is
frequently overlooked in the literature on TPM-based attestation. Unfortunately,
the structure of TPM quotes does not always reï¬‚ect this ordering information,
especially if some of the components depositing measurement values might be
dynamically corrupted. We thus propose a particular strategy for creating a bun-
dle of evidence in TPM quotes designed to give evidence that measurements were

3

indeed taken bottom up. We show in Theorem 3 that, under certain assump-
tions about the uncorrupted measurers in the system, this strategy preserves the
guarantees of bottom-up measurement in the following sense:

If the system satisï¬es certain assumptions, and the TPM quote formed
according to our bundling strategy indicates no corruptions, then either
the measurement were really taken bottom-up, or the adversary recently
corrupted one of tâ€™s dependencies, or else the adversary corrupted an
even deeper component.

Thus, any attempt the adversary makes to avoid the conditions for the hypothesis
of Theorem 1 force him to validate its conclusion nonetheless.

Paper structure. The rest of the paper is structured as follows. Section 2 puts
this paper in the context of related research from the literature. We motivate
our intuitions and informally introduce our model in Section 3. We formalize
these intuitions with deï¬nitions in Section 4, and also apply the formalism to
justify the intuition that it is better to measure â€œbottom-up.â€ In Section 5, we
discuss the basics of TPMs and provide examples of how TPMs can be misused,
not providing the guarantees one might expect. We extend our model with more
deï¬nitions in Section 6 and in Section 7 we demonstrate an eï¬€ective strategy for
using TPMs to bundle evidence. We conclude in Section 8 pointing to directions
for future work.

2 Related work.

There has been much research into measurement and attestation. While a com-
plete survey is infeasible for this paper, we mention the most relevant highlights
in order to describe how the present work ï¬ts into the larger context of research
in this area. We divide the work into several broad categories. Although the
boundaries between the categories can be quite blurry, we believe it helps to
structure the various approaches.

Measurement techniques. Much of the early work was focused on tech-
niques for measuring low-level components that make up a trusted computing
base (TCB). These ideas have matured into implementations such as Trusted
Boot [12]. Recognizing that many security failures cannot be traced back to the
TCB, Sailer et al. [14] proposed an integrity measurement architecture (IMA) in
which each application is measured (by hashing its code) before it is launched.
More recently, there has been work trying to identify and measure dynamic prop-
erties of system components in order to create a more comprehensive picture of
the runtime state of a system [11,10,5,15]. All these eï¬€orts try to establish what
evidence is useful for inferring system state relevant to security decisions. The
present work takes for granted that such special purpose measurements can be
taken and that they will accurately reï¬‚ect the system state. Rather, our focus
is on developing principles for how to combine a variety of these measurers in
a layered attestation. We envision a system designer choosing the measurement

4

capabilities that best suit her needs and using our work to ensure an appraiser
can trust the integrity of the result.

Modular attestation frameworks. Cabuk and others [1] have proposed an ar-
chitecture designed to support layered platforms with hierarchical dependencies.
Their design introduces trusted software into the TCB as a software-based root
of trust for measurement (SRTM). Although they explain how measurements
by the SRTM integrate with the chain of measurements stored in a TPM, they
do not study the eï¬€ect corruptions of various components have on the outcome
of attestations. In [2], Coker et al. identify ï¬ve guiding principles for designing
an architecture to support remote attestation. They also describe the design of
a (layered) virtualized system based on these principles, although there does
not appear to be a publicly available implementation at the time of writing. Of
particular interest is a section that describes a component responsible for man-
aging attestations. The emphasis is on the mechanics of selecting measurement
agents by matching the evidence they can generate to the evidence requested
by an appraiser. There is no discussion or advice regarding the relative order of
measurements or the creation of an evidence bundle to reï¬‚ect the order. More
recently, modular attestation frameworks instantiating [2]â€™s principles have been
implemented [9,7,3]. These are integrated frameworks that oï¬€er plug-and-play
capabilities for measurement and attestation for speciï¬c usage scenarios. It is
precisely these types of systems (in implementation or design) to which our
analysis techniques would be most useful. We have not been able to ï¬nd a dis-
cussion of the potential pitfalls of misconï¬guring these complex systems. Our
work should be able to help guide the conï¬guration of such systems and analyze
particular attestation scenarios for each architecture.

Attestation Protocols. Finally, works such as [2,6,4,13] study the properties
of attestation protocols, typically protocols that use a TPM to report on in-
tegrity evidence provided by measurement agents. They tend to focus on the
cryptographic protections required to secure the evidence as it is sent over a
network. [2] proposes a protocol that binds the evidence to a session key, so that
an appraiser can be guaranteed that subsequent communications will occur with
the appraised system, and not a corrupted substitute. [6] and [13] examine the
ways in which cryptographic protections for network events interact with the
long-term state of a TPM. None of these consider the measurement activities on
the target platform itself and how corruptions of components can aï¬€ect the out-
come of the protocol. In [4], Datta et al. introduce a formalism that accounts for
actions local to the target machine as well as network events such as sending and
receiving messages. Although they give a very careful treatment of the eï¬€ect of a
corrupted component on an attestation, their work diï¬€ers in two key ways. First,
the formalism represents many low-level details making their proof rather com-
plex, sometimes obscuring the underlying principles. Second, their framework
only accounts for static corruptions, while ours is speciï¬cally designed around
the possibility of dynamic corruption and repair of system components.

5

3 Motivating Examples of Measurement

Consider an enterprise that would like to ensure that systems connecting to
its network provide a fresh system scan by the most up-to-date virus checker.
The network gateway should ask systems to perform a system scan on demand
when they attempt to connect. We may suppose the systems all have some
component A1 that is capable of accurately reporting the running version of the
virus checker. Because this enterprise values high assurance, the systems also
come equipped with another component A2 capable of measuring the runtime
state of the kernel. This is designed to detect any rootkits that might try to
undermine the virus checkerâ€™s system scan. We may assume that A1 and A2 are
both measured by a root of trust for measurement (rtm) as part of a secure boot
process.

We are thus interested in a system consisting of the following components:
{sys, vc, ker , A1, A2, rtm}, where sys represents the collective parts of the system
scanned by the virus checker vc, and ker represents the kernel. Based on the sce-
nario described above, we may be interested in the following set of measurement
events

{ms(rtm, A1), ms(rtm, A2), ms(A1, vc), ms(A2, ker ), ms(vc, sys)}

where ms(o1, o2) represents the measurement of o2 by o1. These measurement
events generate the raw evidence that the network gateway can use to make a
determination as to whether or not to admit the system to the network.

If any of the measurements indicate a problem, such as a failed system scan,
then the gateway has good reason to believe it should deny the system access to
the network. But what if all the evidence it receives looks good? How conï¬dent
can the gateway be that the version and signature ï¬les are indeed up to date?
The answer will depend on the order in which the evidence was gathered. To get
some intuition for why this is the case, consider the three diï¬€erent speciï¬cations
pictured in Fig. 1 for how to order the measurements. (The bullet after the ï¬rst
three events is inserted only for visible legibility, to avoid crossing arrows.)

Speciï¬cation S1 ensures that both vc and ker are measured before vc runs its
system scan. Speciï¬cations S2 and S3 each relax one of those ordering require-
ments. Letâ€™s now consider some executions that respect the order of measure-
ments in each of these speciï¬cations in which the adversary manages to avoid
detection.

Execution E1 of Fig. 2 is compatible with Speciï¬cation S1. The adversary
manages to corrupt the system by installing some user-space malware sometime
in the past. If we assume the up-to-date virus checker is capable of detecting
this malware, then the adversary must corrupt either vc or ker before the virus
scan represented by ms(vc, sys). That is, either a corrupted vc will lie about the
results of measurement, or else a corrupted ker can undermine the integrity of
the system scan, for example, by hiding the directory containing the malware
from vc. In the case of E1, the adversary corrupts vc in order to lie about the
results of the system scan, but it does so after ms(A1, vc) in order to avoid
detection by this measurement event.

6

ms(rtm, A1)

att-start(n)

*â¯â¯â¯â¯â¯
tâœâœâœâœâœ

â€¢

tâœâœâœâœâœ
*â¯â¯â¯â¯â¯

ms(rtm, A2)

ms(rtm, A1)

ms(A2, ker )

ms(A1, vc)

att-start(n)

*â¯â¯â¯â¯â¯
tâœâœâœâœâœ

â€¢

tâœâœâœâœâœ
*â¯â¯â¯â¯â¯

ms(rtm, A2)

ms(A2, ker )

ms(A1, vc)

)âšâšâš

uâ¥â¥â¥

ms(vc, sys)

ms(vc, sys)

Speciï¬cation S1

Speciï¬cation S2

ms(rtm, A1)

ms(A1, vc)

att-start(n)

*â¯â¯â¯â¯â¯
tâœâœâœâœâœ

â€¢

tâœâœâœâœâœ
*â¯â¯â¯â¯â¯

ms(rtm, A2)

ms(A2, ker )

ms(vc, sys)

Speciï¬cation S3

Fig. 1. Three orders for measurement

In Execution E2, which is consistent with Speciï¬cation S2, the adversary
is capable of avoiding detection while corrupting vc much earlier. The system
scan ms(vc, sys) is again undermined by the corrupted vc. Since vc will also be
measured by A1, the adversary has to restore vc to an acceptable state before
ms(A1, vc). Execution E3 is analagous to E2, but the adversary corrupts ker
instead of vc, allowing it to convince the uncorrupted vc that the system has
no malware. Since Speciï¬cation S3 allows ms(A1, vc) to occur after the system
scan, the adversary can leverage the corrupted vc to lie about the scan results,
but must restore vc to a good state before it is measured.

Execution E1 is ostensibly harder to achieve for the adversary than either
E2 or E3, because the adversary has to work quickly to corrupt vc during the
attestation. In E2 and E3, the adversary can corrupt vc and ker respectively at
any time in the past. He still must perform a quick restoration of the corrupted
component during the attestation, but there are reasons to believe this may be
easier than corrupting the component to begin with. Is it true that all executions
respecting the measurement order of S1 are harder to achieve than E2 and
E3? What if the adversary corrupts vc before the start of the attestation? It
would seem that he would also have to corrupt A1 to avoid detection by A1â€™s
measurement of vc, ms(A1, vc).

One major contribution of this paper is to provide a formal framework in
which to ask and answer such questions. Within this framework we can begin
to characterize what the adversary must do in order to avoid detection by mea-
surement. We will show that there is a precise sense in which Speciï¬cation S1 is
strictly stronger than S2 or S3. This is an immediate corollary of a more general

7

*


t
t
*
)
u
*


t
t
*


*


t
t
*


ms(rtm, A1)

att-start(n)

ms(rtm, A2)

ms(rtm, A1)

att-start(n)

ms(rtm, A2)

cor(sys)

)âšâšâš

uâ¥â¥â¥

*â¯â¯â¯â¯â¯
tâœâœâœâœâœ

â€¢

tâœâœâœâœâœ

$âââââââââ

ms(A2, ker )

ms(A1, vc)

cor(vc)

)âšâšâšâš

uâ¥â¥â¥

ms(vc, sys)

cor(sys), cor(vc)

*â¯â¯â¯

tâœâœâœ

+â²â²â²â²â²â²â²

sâ£â£â£â£â£â£â£

â€¢

ms(A2, ker )

ms(vc, sys)

rep(vc)

ms(A1, vc)

Execution E1

Execution E2

ms(rtm, A1)

att-start(n)

ms(rtm, A2)

cor(sys), cor(ker )

*â±â±â±

tâ¤â¤â¤

+â²â²â²â²â²â²â²

sâ£â£â£â£â£â£â£

â€¢

ms(A1, vc)

ms(vc, sys)

rep(ker )

ms(A2, ker )

Execution E3

Fig. 2. Three system executions

8

)
u


*


t
t
$


)
u
t
*


+


s








t
*


+


s








result (Theorem 1) that validates a strong intuition that pervades much of the
literature on measurement and attestation: Attestations are more trustworthy
if the lower-level components of a system are measured before the higher-level
components. The next section lays the groundwork for this result.

4 Measurement Systems

4.1 Preliminaries and Deï¬nitions

In this section we formalize the intuitions we used for the examples in the previ-
ous section. We start by deï¬ning measurement systems which perform the core
functions of creating evidence for attestation.

System architecture.

Deï¬nition 1. We deï¬ne a measurement system to be a tuple MS = (O, M, C),
where O is a set of objects (e.g. software components) with a distinguished ele-
ment rtm. M and C are binary relations on O. We call

M the measures relation, and
C the context relation.

We say M is rooted when for every o âˆˆ O \ {rtm}, M +(rtm, o), where M + is
the transitive closure of M .

M represents who can measure whom, so that M (o1, o2) iï¬€ o1 can measure
o2. rtm is the root of trust for measurement. For this reason we henceforth
always assume M is rooted and M + is acyclic (i.e. Â¬M +(o, o) for any o âˆˆ O).
This guarantees that every object can potentially trace its measurements back
to the root of trust, and there are no measurement cycles. As a consequence, rtm
cannot be the target of measurement, i.e. for rooted, acyclic M , Â¬M (o, rtm) for
any o âˆˆ O. The relation C represents the kind of dependency between ker and
vc in the example above in which one object provides a clean runtime context for
another. Thus, C(o1, o2) iï¬€ o1 contributes to maintaining a clean runtime context
for o2. (C stands for context.) We henceforth always assume C is transitive (i.e.
if C(o1, o2) and C(o2, o3) then C(o1, o3)) and acyclic. This means that no object
(transitively) relies on itself for its own clean runtime context.

Given an object o âˆˆ O we deï¬ne the measurers of o to be M âˆ’1(o) = {oâ€² |
M (oâ€², o)}. We similarly deï¬ne the context for o to be C âˆ’1(o). We extend these
deï¬nitions to sets in the natural way.

We additionally assume M âˆª C is acyclic. This ensures that the combina-
tion of the two dependency types does not allow an object to depend on itself.
Such systems are stratiï¬ed, in the sense that we can deï¬ne an increasing set of
dependencies as follows.

D1(o) = M âˆ’1(o) âˆª C âˆ’1(M âˆ’1(o))

Di+1(o) = D1(Di(o))

9

So D1(o) consists of the measurers of o and their context. As we will see later,
D1(o) represents the set of components that must be uncompromised in order
to trust the measurement of o.

We can represent measurement systems pictorially as a graph whose vertices
are the objects of MS and whose edges encode the M and C relations. We use
the convention that M (o1, o2) is represented by a solid arrow from o1 to o2, while
C(o1, o2) is represented by a dotted arrow from o1 to o2. The representation of
the system described in Section 3 is shown in Figure 3.

rtm

{âœ‡âœ‡âœ‡

#â—â—â—

A2

ker

A1

vc

sys

MS 1

Fig. 3. Visual representation of an example measurement system.

Terms and derivability. It is called a measurement system because the pri-
mary activity of these components is to measure each other. The results of mea-
surement are expressed using elements of a term algebra, the crucial features of
which we present next.

Terms are constructed from some base V of atomic terms using constructors
in a signature Î£. The set of terms is denoted TÎ£(V ). We assume Î£ includes at
least some basic constructors such as pairing (Â·, Â·), signing [[ (Â·) ]](Â·), and hashing
#(Â·). The set V is partitioned into public atoms P, random nonces N , and
private keys K.

Our analysis will sometimes depend on what terms an adversary can derive
(or construct). We say that term t is derivable from a set of term T âŠ† V iï¬€
t âˆˆ TÎ£(T ), and we write T âŠ¢ t. We assume the adversary knows all the public
atoms P, and so can derive any term in TÎ£(P) at any time. For each o âˆˆ O, we
assume there is a distinguished set of (public) measurement values MV(o) âŠ‚ P.

Events, outputs, and executions. The components o âˆˆ O and the adversary
on this system perform actions. In particular, objects can measure each other
and the adversary can corrupt and repair components in an attempt to inï¬‚uence
the outcome of future measurement actions. Additionally, an appraiser has the
ability to inject a random nonce n âˆˆ N into an attestation in order to control
the recency of events.

Deï¬nition 2 (Events). Let MS be a target system. An event for MS is a
node e labeled by one of the following.

10

#
{






o
o
a. A measurement event is labeled by ms(o2, o1) such that M (o2, o1). We say
such an event measures o1, and we call o1 the target of e. We let Supp(e)
denote the set {o2} âˆª C âˆ’1(o2).

b. An adversary event is labeled by either cor(o) or rep(o) for o âˆˆ O \ {rtm}.
c. The attestation start event is labeled by att-start(n), where n is a term.

When an event e is labeled by â„“ we will write e = â„“. We will often refer to the
label â„“ as an event when no confusion will arise.

An event e touches o, iï¬€ either

i. o is an argument to the label of e, or
ii. o âˆˆ Supp(e).

The att-start(n) event will serve to bound events in time. It represents the
random choice by the appraiser of the value n. The appraiser will know that
anything occurring after this event can reasonably be said to occur â€œrecentlyâ€.
Regarding the measurement events, the rtm is typically responsible for measur-
ing components at boot-time. All other measurements are load-time or runtime
measurements of one component in O by another. Adversary events represent
the corruption (cor(Â·)) and repair (rep(Â·)) of components. Notice that we have
excluded rtm from corruption and repair events. This is not because we assume
the rtm to be immune from corruption, but rather because all the trust in the
system relies on the rtm: Since it roots all measurements, if it is corrupted, none
of the measurements of other components can be trusted.

As we saw in the motivational examples, an execution can be described as a
partially ordered set (poset) of these events. We choose a partially ordered set
rather than a totally ordered set because the latter unnecessarily obscures the
diï¬€erence between causal orderings and coincidental orderings. However, due
to the causal relationships between components, we must slightly restrict our
partially ordered sets in order to make sense of the eï¬€ect that corruption and
repair events have on measurement events. To that end, we next introduce a
sensible restriction to these partial orders.

A poset is a pair (E, â‰º), where E is any set and â‰º is a transitive, acyclic rela-
tion on E. When no confusion arises, we often refer to (E, â‰º) by its underlying set
E and use â‰ºE for its order relation. Given a poset (E, â‰º), let eâ†“= {eâ€² | eâ€² â‰º e},
and eâ†‘= {eâ€² | e â‰º eâ€²}. Given a set of events E, we denote the set of adversary
events of E by adv (E) and the set of measurement events by meas(E).

Let (E, â‰º) be a partially ordered set of events for MS = (O, M, C) and let
(Eo, â‰ºo) be the substructure consisting of all and only events that touch o. We
say (E, â‰º) is adversary-ordered iï¬€ for every o âˆˆ O, (Eo, â‰ºo) has the property
that if e and eâ€² are incomparable events, then neither e nor eâ€² are adversary
events.

Lemma 1. Let (E, â‰º) be a ï¬nite, adversary-ordered poset for MS, and let
(Eo, â‰ºo) be its restriction to some o âˆˆ O. Then for any non-adversarial event
e âˆˆ Eo, the set adv (eâ†“) (taken in Eo) is either empty or has a unique maximal
element.

11

Proof. Since (E, â‰º) is adversary-ordered, adv (Eo) is partitioned by adv (eâ†“) and
adv (eâ†‘). Suppose eâ†“ is not empty. Then since Eo is ï¬nite, it has at least one
maximal element. Suppose eâ€² and eâ€²â€² are distinct maximal elements. Thus they
must be â‰ºo-incomparable. However, since (E, â‰º) is adversary-ordered, either
eâ€² â‰ºo eâ€²â€² or eâ€²â€² â‰ºo eâ€², yielding a contradiction.
âŠ“âŠ”

Deï¬nition 3 (Corruption state). Let (E, â‰º) be a ï¬nite, adversary-ordered
poset for MS. For each event e âˆˆ E and each object o the corruption state
of o at e, written cs(e, o), is an element of {âŠ¥, r, c} and is deï¬ned as follows.
cs(e, o) = âŠ¥ iï¬€ e 6âˆˆ Eo. Otherwise, we deï¬ne cs(e, o) inductively:

cs(e, o) =

ï£±ï£´ï£´ï£²
ï£´ï£´ï£³

c

r

r

: e = cor(o)
: e = rep(o)
: e âˆˆ meas(E) âˆ§ adv (eâ†“) âˆ© Eo = âˆ…

cs(eâ€², o) : e âˆˆ meas(E) âˆ§ eâ€² maximal in adv (eâ†“) âˆ© Eo

When cs(e, o) takes the value c we say o is corrupt at e; when it takes the value
r we say o is uncorrupt or regular at e; and when it takes the value âŠ¥ we say
the corruption state is undeï¬ned.

We assume measurement events produce evidence of the corruption state of
the component. The question of measurement is tricky though, because what
counts as evidence of corruption for one appraiser might pass as evidence of
regularity by another. It is the job of measurement to produce evidence not
to evaluate it. Furthermore, evidence of regularity (or corruption) might take
many forms. In our analysis we bracket most of these questions by making a
simplifying assumption about measurements. In particular, we assume a given
appraiser can accurately determine the corruption state of a target given that
the measurement was taken by a regular component with a regular context. More
formally, we assume the following.

Assumption 1 (Measurement Accuracy) Let G(o) and B(o) be a partition
for MV(o). Let e = ms(o2, o1). The output of e, written out(e), is deï¬ned as
follows. out(e) = v âˆˆ B(o1) iï¬€ cs(e, o1) = c and for every o âˆˆ {o2} âˆª {oâ€² |
C(oâ€², o2)}, cs(e, o) = r. Otherwise out (e) = v âˆˆ G(o1).

If out(e) âˆˆ B(o1) we say e detects a corruption. If out (e) âˆˆ G(o1) but

cs(e, o1) = c, we say the adversary avoids detection at e.

If e = att-start(n), then out (e) = n.

Thus, the appraiser partitions the possible measurement values of o into those
that she believes indicate regularity (G(o)) and those that indicate corruption
(B(o)). The output of a measurement by regular components is in G(o) if o is
regular at the measurement event, and in B(o) if it is corrupt. We view this
assumption as allowing us to explore the best one can hope for with measure-
ment. Of course, in reality, things are not so rosy. Simple measurement schemes
like hashing the code can cause components to look corrupt when, in fact, a
small change that is irrelevant to security has changed the outcome of the hash.

12

Conversely, a runtime measurement scheme that only looks at a subset of the
componentâ€™s data structures may fail to detect a corruption and report a mea-
surement value that looks acceptable. One could imagine relaxing this assump-
tion by accounting for probabilities of detection depending on which components
have been corrupted. We leave such investigations for future work with the un-
derstanding that the results in this paper represent, in a sense, the strongest
conclusions one can expect from any measurement system.

We can now deï¬ne what it means to be an execution of a measurement

system.

Deï¬nition 4 (Executions, Speciï¬cations). Let MS be a measurement sys-
tem.

1. An execution of MS is any ï¬nite, adversary-ordered poset E for MS.
2. A speciï¬cation for MS is any execution that contains no adversary events.

Speciï¬cation S admits an execution E iï¬€ there is an injective, label-preserving
map of partial orders Î± : S â†’ E. The set of all executions admitted by S is
denoted E(S).

Measurement speciï¬cations are the way an appraiser might ask for measure-
ments to be taken in a particular order. The set E(S) is just the set of executions
in which the given events have occurred in the desired order. The appraiser can
thus analyze E(S) in advance to determine what an adversary has to do to avoid
detection, given that the events in S were performed as speciï¬ed.

The question of how an appraiser learns whether or not the actual execution
performed is in E(S) is an important one. The second half of the paper is dedi-
cated to that problem. For now, we consider what an appraiser can infer about
an execution E given that E âˆˆ E(S).

4.2 A Strategy for Measurement

We now turn to a formalization of the rule of thumb at the end of Section 3.
By ensuring that speciï¬cations have certain structural aspects, we can conclude
the executions they admit satisfy useful constraints. In particular, it is useful to
measure components from the bottom up with respect to the dependencies of the
system. That is, if whenever o1 depends on o2 we measure o2 before measuring
o1, then we can usefully narrow the range of actions the adversary must take in
order to avoid detection. For this discussion we ï¬x a target system MS. Recall
that D1(o) represents the measurers of o and their runtime context.

Deï¬nition 5. A measurement event e = ms(o2, o1) in execution E is well-
supported iï¬€ either

i. o2 = rtm, or
ii. for every o âˆˆ D1(o1), there is a measurement event eâ€² â‰ºE e such that o is

the target of eâ€².

13

When e is well-supported, we call the set of eâ€² from Condition ii above the support
of e. An execution E measures bottom-up iï¬€ each measurement event e âˆˆ E is
well-supported.

Theorem 1 (Recent or deep). Let E be an execution with well-supported
measurement event e = ms(o1, ot) where o1 6= rtm. Suppose that E detects no
corruptions. If the adversary avoids detection at e, then either

1. there exist o âˆˆ D1(ot) and oâ€² âˆˆ M âˆ’1(o) such that ms(oâ€², o) â‰ºE cor(o) â‰ºE e
2. there exists o âˆˆ D2(ot) such that cor(o) â‰ºE e.

Proof. Since the adversary avoids detection at e, ot is corrupt at e, and there
is some o âˆˆ {o1} âˆª C âˆ’1(o1) âŠ† D1(ot) that is also corrupt at e. Also, since e is
well-supported, and o1 6= rtm, we know there exists eâ€² = ms(oâ€², o) with eâ€² â‰ºE e.
We now take cases on cs(eâ€², o).

If cs(eâ€², o) = r then there must be a corruption cor(o) between eâ€² and e

satisfying Clause 1 to change its corruption state from r to c.

If cs(eâ€², o) = c, then since E detects no corruptions, there must be some
oâˆ— âˆˆ {oâ€²} âˆª C âˆ’1(oâ€²) âŠ† D2(ot) such that cs(eâ€², oâˆ—) = c. Thus there must be a
previous corruption cor(oâˆ—) â‰ºE eâ€² â‰ºE e satisfying Clause 2.
âŠ“âŠ”

This theorem says, roughly, that if measurements indicate things are good
when they are not, then there must either be a recent corruption or a deep
corruption. This tag line of â€œrecent or deepâ€ is particularly apt if the system
dependencies also reï¬‚ect the relative diï¬ƒculty for an adversary to corrupt them.
By ordering the measurements so that more robust ones are measured ï¬rst,
it means that for an adversary to avoid detection for an easy compromise, he
must have compromised a measurer since it itself was measured, or else, he must
have previously (though not necessarily recently) compromised a more robust
component. In this way, the measurement of a component can raise the bar for
the adversary. If, for example, a measurer sits in a privileged location outside of
some VM containing a target, it means that the adversary would also have to
break out of the target VM and compromise the measurer to avoid detection.
The skills and time necessary to perform such an attack are much greater than
simply compromising the end target.

Letâ€™s illustrate this result in the context of the example of Section 3. The
speciï¬cation S1 satisï¬es the main hypothesis of Theorem 1. Execution E1 illus-
trates an example of the ï¬rst clause of the conclusion being satisï¬ed. There is a
â€œrecentâ€ corruption of vc in the sense that vc is corrupted after it is measured.
Since the measurement of vc occurs after the start of the attestation, this is
truly recent, in that the adversary has very little time to work. The appraiser
can control this by ensuring that attestations time out after some ï¬xed amount
of time.

Theorem 1 also indicates other possible executions in which the adversary can
undetectably corrupt sys. There could be a recent corruption of vc, or else there
could be some previous corruption of either A1 or A2. All the various options are
shown in Figure 4 in which the measurement events at which the adversary avoids

14

att-start(n)

ms(rtm, A2)

ms(rtm, A1)

att-start(n)

ms(rtm, A1)

ms(A1, vc)

*â¯â¯â¯â¯â¯â¯
tâœâœâœâœâœâœ

â€¢

tâœâœâœâœâœâœ

$ââââââââââ

(â˜â˜â˜â˜

vâ§â§â§â§

ms(vc, sys)âˆ—

ms(rtm, A2)

ms(A2, ker )

cor(ker )

â€¢

tâœâœâœâœâœâœ
*â¯â¯â¯â¯â¯â¯

*â¯â¯â¯â¯â¯â¯

zâœˆâœˆâœˆâœˆâœˆâœˆâœˆâœˆâœˆâœˆ

(â˜â˜â˜â˜

vâ§â§â§

ms(vc, sys)âˆ—

cor(vc)

cor(sys)

ms(A2, ker )

ms(A1, vc)

cor(sys)

E1
1

E2
1

ms(rtm, A1)

ms(rtm, A2)

cor(A1)

att-start(n)

ms(rtm, A2)

ms(rtm, A1)

att-start(n)

cor(vc)

(â˜â˜â˜â˜â˜â˜â˜
uâ¥â¥â¥â¥â¥â¥

â€¢

vâ§â§â§â§â§â§â§
)âšâšâšâšâšâš

)âšâšâš

uâ¥â¥â¥

ms(vc, sys)âˆ—

cor(A2)

cor(ker )

(â˜â˜â˜â˜â˜â˜â˜
uâ¥â¥â¥â¥â¥â¥

â€¢

uâ§â§â§â§â§â§â§
*âšâšâšâšâšâšâš

)âšâšâš

tâ¥â¥â¥

ms(vc, sys)âˆ—

ms(A1, vc)âˆ—

cor(sys)

ms(A2, ker )

ms(A1, vc)

cor(sys)

ms(A2, ker )âˆ—

E3
1

E4
1

Fig. 4. Executions that do not detect corruption of sys.

15

*


t
$
t


(


v
*


t
z
*


(


v


(


v


u
)
)


u


(


u
u
*


)


t
detection are marked with an asterisk, and the corruption events guaranteed by
the theorem are boxed. Our theorem allows us to know that these executions
essentially characterize all the cases in which a corrupted sys goes undetected.

5 Motivating Examples of Bundling

The previous section discusses how to constrain adversary behavior using the
order of measurements. However, implicit in the analysis is the assumption that
an appraiser is able to verify the order and outcome of the measurement events.
Since a remote appraiser cannot directly observe the target system, this assump-
tion must be discharged in some way. A measurement system must be augmented
with the ability to record and report the outcome and order of measurement
events. We refer to these additional activities as bundling evidence. Our focus
for this paper is on using the Trusted Platform Module for this purpose. While
there are techniques and technologies that can be used as roots of trust for
reporting (e.g. hardware-based trusted execution environments such as Intelâ€™s
SGX) there has been a lot of research into TPMs and their use for attestation.
Much of that work does not pay close attention to the importance of faithfully
reporting the order in which measurements have taken place. Thus, we believe
that studying TPM-based attestation is a fruitful place to start, and we leave
investigations of other techniques and technologies for future work.

5.1 TPM Background

Trusted Platform Modules (TPMs) are small hardware processors that are de-
signed to provide secure crypto processing and limited storage of information
isolated from software. Its technical speciï¬cation was written by the Trusted
Computing Group (TCG) [8]. While TPMs have many features designed to sup-
port subtle properties, we only brieï¬‚y review those features relevant for our
purposes.

TPMs have a bank of isolated storage locations called Platform Conï¬guration
Registers (PCRs) designed to store measurements of a platformâ€™s state. These
PCRs have a very limited interface. They start in some well-known state and
each PCR can only be updated by extending a new value v which has the eï¬€ect
of updating the contents of the PCR to be the hash of v with the previous
contents. Thus the contents of each PCR serve as a historical record of all the
measurements extended into them since the most recent system boot.

TPMs also have the ability to securely report the values in their PCRs by
creating a digital signature over their contents using a private key that is only
accessible inside the TPM. This operation is known as a quote. Since the PCRs
are isolated from software, any remote party that has access to the corresponding
public key can verify the contents of the PCRs. In order to protect against replay
attacks and ensure the recency of the information, TPM quotes also sign some
externally provided data, typically a random nonce chosen by an appraiser.

16

Finally, TPMs have a limited form of access control for their PCRs known as
locality. Some PCRs may only be extended by particular privileged components.
Thus if a PCR with access control enabled contains some sequence of measure-
ments, it must have been (one of) the privileged component(s) that extended
those values. Currently TPMs have ï¬ve localities so that they can diï¬€erentiate
between ï¬ve groups of components.

Currently TPMs are widely available in commodity computers although the
surrounding architectures are such that they are rarely easy to access and use.
There has been some research into â€œvirtualizingâ€ TPMs. This entails providing
robust protections for a software TPM emulator that ensure it can achieve com-
parable levels of isolation among other properties. Such a technology would be
particularly useful in virtualized cloud environments where one would like to
provide the beneï¬ts of a TPM to virtual machines that may be instantiated on
diï¬€erent physical hardware. Virtual TPMs (vTPMs) are currently unavailable,
however the TCG is currently producing a speciï¬cation that details the neces-
sary protections, and there are some preliminary implementations that will likely
be modiï¬ed as the details of the speciï¬cation become more clear.

vTPMs provide two additional beneï¬ts over hardware TPMs (assuming the
necessary protections are guaranteed) that we will take advantage of here. While
hardware TPMs typically only have 24 PCRs, there is essentially no limit on the
number of PCRs a vTPM might have. Furthermore, vTPMs would be able to
implement many more than ï¬ve localities. These two features combine to al-
low many components to each have dedicated access to their own PCRs. As we
will see, this is advantageous. However, given the current state of the technol-
ogy, assuming these features exist is â€œforward thinking.â€ The distinction between
hardware TPMs and vTPMs will not aï¬€ect the core of our analysis, so we hence-
forth use TPM without specifying if it is a hardware TPM or vTPM.

PCR values and quotes. We represent both the values stored in PCRs and the
quotes as terms in TÎ£(V ). Since PCRs can only be updated by extending new val-
ues, their contents form a hash chain #(vn, #(..., #(v1, rst))). We abbreviate such
a hash chain as seq(v1, . . . , vn). So for example, seq(v1, v2) = #(v2, #(v1, rst)).
We say a hash chain seq(v1, . . . , vn) contains vi for each i â‰¤ n. Thus the contents
of a PCR contain exactly those values that have been extended into it. We also
say vi is contained before vj in seq(v1, . . . , vn) when i < j â‰¤ n. That is, vi is
contained before vj in the contents of p exactly when vi was extended before vj.
A quote from TPM t is a term of the form [[ n, (pi)iâˆˆI , (vi)iâˆˆI ]]sk (t). It is a
signature over a nonce n, a list of PCRs (pi)iâˆˆI and their respective contents
(vi)iâˆˆI using sk (t), the secret key of t. We always assume sk (t) âˆˆ K the set of
non-public, atomic keys. That means the adversary does not know sk (t) and
hence cannot forge quotes.

5.2 Pitfalls of TPM-Based Bundling.

The two key features of TPMs (protected storage and secure reporting) allow
components to store the results of their measurements and later report the results

17

to a remote appraiser. The resulting quote (or set of quotes) is a bundle of
evidence that the appraiser must use to evaluate the state of the target system.
Indeed, this bundle is the only evidence the appraiser receives. In the rest of
this section we present various examples that demonstrate how the structure of
this bundle aï¬€ects the trust inferences a remote appraiser is justiï¬ed in making
about the target.

Consider MS1 found in Section 3, and pictured in Fig. 3. Ideally a remote
appraiser would be able to verify that an execution that produces a particular
set of quotes Q is in E(S1) (from Fig. 1). The appraiser must be able to do
this on the basis of Q only. The possibilities for Q depend somewhat on how
MS1 is divided. For example, if MS1 is a virtualized system, rtm might sit in
an administrative VM, and A1 and A2 could be in a privileged â€œhelperâ€ VM
separated from the main VM that hosts ker , vc, and sys. If each of these VMs
is supported by its own TPM, then Q would have to contain at least three
quotes just to convey the raw measurement evidence. However, if MS 1 is not
virtualized, they might all share the same TPM and a single quote might suï¬ƒce.
For our purposes it suï¬ƒces to consider a simple architecture in which all the
components share a single TPM.

Strategy 1: A single hash chain. Since PCRs contain an ordered history
of the extended values, the ï¬rst natural idea is for all the components to share
a PCR p, each extending their measurements into p. The intuition is that the
contents of p should represent the order in which the measurements occurred
on the system. To make this more concrete, assume the measurement events
of S1 have the following output: out(ms(rtm, A1)) = v1, out (ms(rtm, A2)) =
v2, out(ms(A1, vc)) = v3, out (ms(A2, ker )) = v4, out(ms(vc, ker )) = v5. Then
this strategy would produce a single quote Q = [[ n, p, seq(v1, v2, v3, v4, v5) ]]sk (t ).
To satisfy the order of S1, any linearization of the measurements would do, so the
appraiser should also be willing to accept Qâ€² = [[ n, p, seq(v2, v1, v3, v4, v5) ]]sk (t )
in which v1 and v2 were generated in the reverse order.

Figure 5 depicts an execution that produces the expected quote Q, but does
not satisfy the desired order. Since all the measurement components have access
to the same PCR, if any of those components is corrupted, it can extend values
to make it look as though other measurements were taken although they were
not. This is particularly troublesome when a relatively exposed component like
vc can impersonate the lower-level components that measure it.

This motivates our desire to have strict access control for PCRs. This would
allow the appraiser to correctly infer which component has provided each piece
of evidence. The locality feature of TPMs could be used for this purpose. Given
the limitations of locality in the current technology, however, it may be neces-
sary to introduce another component that is responsible for disambiguating the
source of each measurement into a PCR. Such a strategy would require careful
consideration of the eï¬€ect of a corruption of that component, and to include
measurement evidence that it is functioning properly. For simplicity of our main
analysis we freely take advantage of the assumption that TPMs can provided
dedicated access to one PCR per component of the system it supports, leaving

18

cor(sys)

)â˜â˜â˜

vâ§â§â§

att-start(n)

cor(vc)

ext(vc, p, v1)

ext(vc, p, v2)

ext(vc, p, v3)

ext(vc, p, v4)

ext(vc, p, v5)

qt(n, p) = Q

Output of quote is Q = [[ n, p, seq(v1, v2, v3, v4, v5) ]]sk (t).

Fig. 5. Defeating Strategy 1

an analysis of the more complicated architecture for a more complete treatment
of the subject.

Strategy 2: Separate hash chains. A natural next attempt given this as-
sumption would be to produce a single quote over the set of PCRs that con-
tain the measurement evidence. This would produce quotes with the struc-
ture Q = [[ n, (pr, p1, p2, pvc), (s1, s2, s3, s4) ]]sk (t), in which s1 = seq(v1, v2), s2 =
seq(v3), s3 = seq(v4), s4 = seq(v5). Figure 6 demonstrates a failure of this strat-
egy. The problem, of course, is that, since the PCRs may be extended concur-
rently, the relative order of events is not captured by the structure of the quote.

ms(rtm, A1)

ms(rtm, A2)

ms(A1, vc)

ms(A2, ker )

ms(vc, sys)

ext(rtm, pr, v1)

ext(rtm, pr, v2)
-â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬
+â²â²â²â²

ext(A1, p1, v3)

att-start(n)

ext(A2, p2, v4)
qââââââââââââââââ
sâ¤â¤â¤â¤

ext(vc, pvc, v5)

qt(n, (pi)iâˆˆI ) = Q

Output of quote is Q = [[ n, (pr, p1, p2, pvc), (s1, s2, s3, s4) ]]sk (t)

s1 = seq(v1, v2), s2 = seq(v3), s3 = seq(v4), s4 = seq(v5)).

Fig. 6. Defeating Strategy 2

Strategy 3: Tiered, nested quotes. We thus require a way to re-introduce
evidence about the order of events while maintaining the strict access control on

19

)
v






















-
+


s
q


PCRs. That is, we should incorporate measurement evidence from lower layers
before generating the evidence for higher layers. This suggests a tiered and nested
strategy for bundling the evidence. In the case of MS1, to demonstrate the order
speciï¬ed in S1, our strategy might produce a collection of quotes of the following
form.

Q1 = [[ n, pr, seq(v1, v2) ]]sk (t)
Q2 = [[ n, (p1, p2), (seq(Q1, v3), seq(Q1, v4)) ]]sk (t)
Q3 = [[ n, pvc, seq(Q2, v5) ]]sk (t)

The quote Q1 provides evidence that rtm has measured A1 and A2. This quote is
itself extended into the PCRs of A1 and A2 before they take their measurements
and extend the results. Q2 thus represents evidence that rtm took its measure-
ments before A1 and A2 took theirs. Similarly, Q3 is evidence that vc took its
measurement after A1 and A2 took theirs since Q2 is extended into pvc before
the measurement evidence.

Unfortunately, this quote structure is not quite enough to ensure that the
proper order is respected. Figure 7 illustrates the problem. In that execution,
all the measurements are generated concurrently at the beginning, and each
component waits to extend the result until it gets the quote from the layer below.
The quotes give accurate evidence for the order in which evidence was recorded
but not for the order in which the evidence was generated. It must be the job of
regular components to ensure that the order of extend events accurately reï¬‚ects
the order of measurement events. We make precise our assumptions for regular
components in Section 7. Under those extra assumptions we can prove that a
quote generated according to this ï¬nal strategy is suï¬ƒcient to ensure that the
execution it came from meets the guarantees of Theorem 1.

6 Attestation Systems

In this section we augment the earlier deï¬nitions for measurement systems to
account for the use of TPMs to record and report on the evidence generated by
measurement. The following deï¬nitions closely parallel those of Section 4. We
begin by expanding a measurement system into an attestation system.

Deï¬nition 6. We deï¬ne an attestation system to be AS = (O, M, C, P, L)
where MS = (O, M, C) is a measurement system, P = T Ã— R for some set
T of TPMs and some index set R of their PCR registers, and L is a relation on
O Ã— P .

Elements of P have the form p = t.i for t âˆˆ T and i âˆˆ R. The relation L repre-
sents the access control constraints for extending values into TPM PCRs. Each
component in O can only access a single TPM, so we assume that if L(o, t.i)
and L(o, tâ€².iâ€²), then t = tâ€². As we discussed in the previous section, it is ad-
vantageous to assume the access control mechanism dedicates a PCR to each

20

ms(rtm, A1)

-â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬â¬ ms(rtm, A2)

+â²â²â²â²â²

ms(A1, vc)

att-start(n)

ms(A2, ker )

qâââââââââââââââââ
sâ£â£â£â£â£

ms(vc, sys)

ext(rtm, pr, v1)

ext(rtm, pr, v2)

qt(n, pr) = Q1

+â²â²â²â²

ext(A2, p2, Q1)

ext(A2, p2, v4)

ext(A1, p1, Q1)

ext(A1, p1, v3)

sâ£â£â£â£

+â²â²â²â²

qt(n, (p1, p2)) = Q2

sâ£â£â£â£

ext(vc, pvc, Q2)

ext(vc, pvc, v5)

qt(n, pvc) = Q3

Outputs of quotes are Q1 = [[ n, pr, seq(v1, v2) ]]sk (t),

Q2 = [[ n, (p1, p2), (seq(Q1, v3), seq(Q1, v4)) ]]sk (t),

Q3 = [[ n, pvc, seq(Q2, v5) ]]sk (t).

Fig. 7. Defeating Strategy 3

21

-
+


s
q






s
+




+
s






component that needs one. We formalize this by assuming L is injective in the
sense that if L(o, p) and L(oâ€², p) then o = oâ€².

The extra structure of an attestation system over a measurement system
allows us to formalize the activities of recording and reporting evidence using
events for extending values into PCRs and quoting the results.

Deï¬nition 7 (Events). Let AS be an attestation system. An event is either
an event of the included measurement system or it is a node labeled by one of
the following.

a. An extend event is labeled by ext(o, v, p), such that L(o, p) and v is a term.
b. A quote event is labeled by qt(v, tI ), where v is a term, and tI = {t.i | i âˆˆ I}
is a sequence of PCRs belonging to the same TPM t. We say a quote event
reports on p, or is over p, if p âˆˆ tI .

The second argument to extend events and the ï¬rst argument to quote events is
called the input.

An event e touches PCR p, iï¬€ either

i. e = ext(o, v, p) for some o and v, or
ii. e = qt(v, tI ) for some v and p âˆˆ tI .

Notice that a quote event has no corresponding component o âˆˆ O. This is
because TPMs may produce quotes in response to a request by any component
that has access to it.

Just as with measurement systems, we must impose some constraints on the
partially ordered sets of these events if we expect the result of quote and extend
events to accurately represent the eï¬€ects of prior extend events. The following
restriction is completely analogous to our deï¬nition of adversary-ordered sets of
events, this time focusing on the state changes of PCRs.

Recall that for e âˆˆ (E, â‰º), eâ†“ is the set of events preceding e in E, and eâ†‘
is the set of events occurring after e in E. Let ext(E) denote the set of extend
events of E and qt (E) denote the set of quote events of E.

Let (E, â‰º) be a partially ordered set of events for AS = (O, M, C, P, L) and
let (Ep, â‰ºp) be the substructure consisting of all and only events that touch PCR
p. We say (E, â‰º) is extend-ordered iï¬€ for every p âˆˆ P , (Ep, â‰ºp) has the property
that if e and eâ€² are incomparable events, then they are both quote events.

Lemma 2. Let (E, â‰º) be a ï¬nite extend-ordered poset for AS, and let (Ep, â‰ºp)
be its restriction to some p âˆˆ P . Then for every event e âˆˆ Ep, ext(eâ†“) is either
empty, or it has a unique maximal event eâ€².

Proof. Because (E, â‰º) is extend-ordered, ext(Ep) is partitioned by ext (eâ†“), {e},
and ext(eâ†‘) for any e âˆˆ Ep. (The singleton {e} forms part of the partition exactly
when e is an extend event.) Suppose ext(e â†“) is not empty. Since E is ï¬nite,
ext(eâ†“) has at least one maximal element. Suppose eâ€² and eâ€²â€² are two distinct
maximal elements. Thus they are â‰ºp-incomparable. However, since (E, â‰º) is
extend-ordered, either eâ€² â‰ºp eâ€²â€² or eâ€²â€² â‰ºp eâ€², yielding a contradiction.
âŠ“âŠ”

22

This lemma allows us to unambiguously deï¬ne the value in a PCR at any

event that touches the PCR.

Deï¬nition 8 (PCR Value). We deï¬ne the value in a PCR p at event e touch-
ing p to be the following, where eâ†“ is taken in Ep.

val (e, p) =

ï£±ï£´ï£´ï£²
ï£´ï£´ï£³

rst

#(v, rst)
state(eâ€², p)

: ext(eâ†“) = âˆ…, e = qt(n, tI )
: ext(eâ†“) = âˆ…, e = ext(o, v, p)
: eâ€² = max (ext (eâ†“)), e = qt(n, tI )

#(v, state(eâ€², p)) : eâ€² = max (ext (eâ†“)), e = ext(o, v, p)

When e = ext(o, v, p) we say e is the event recording the value v.

We next formalize the output of a quote event. Deï¬nition 8 allows us compute
all the relevant information that must be included in a digital signature. Recall
that, to ensure the signature cannot be forged, we must assume the signing key
is not available to the adversary.

Deï¬nition 9 (Quote Outputs). Let e = qt(n, tI ). Then its output is out(e) =
[[ n, (t.i)iâˆˆI , (vi)iâˆˆI ]]sk (t), where for each i âˆˆ I, val (e, t.i) = vi, and sk (t) âˆˆ K (the
set of atomic, non-public keys). We say a quote Q indicates a corruption iï¬€ some
vi contains a v âˆˆ B(o) for some o.

Deï¬nition 10 (Executions). Let AS be a target system. An execution of AS
is any adversary-ordered and extend-ordered poset E for AS such that whenever
e has input v, then v is derivable from the set P âˆª {out(eâ€²) | eâ€² â‰ºE e}, i.e. the
public terms together with the output of previous events.

An execution E produces a quote Q (written E âˆˆ E(Q)), iï¬€ E contains a

quote event with output Q.

7 Bundling Evidence for Attestation

In this section we present several results that demonstrate some key inferences
an appraiser can make about an execution that produces a given quote. We then
formalize Strategy 3 from Section 5 for bundling evidence. Another sequence of
results demonstrates that, under certain assumptions about the design of regular
components, the guarantees of Theorem 1 are preserved for executions producing
quotes according to Strategy 3. In particular, if a corrupted component o avoids
detection, then the adversary must either have performed a recent corruption or
a deep corruption (relative to o).

7.1 Principles for TPM-based bundling.

For the remainder of this section we ï¬x an arbitrary attestation system AS =
(O, M, C, P, L). Our ï¬rst lemma allows us to infer the existence of some extend
events in an execution.

23

Lemma 3. Let e be a quote event in execution E with output Q. For each PCR
p reported on by Q, and for each v contained in val (e, p) there is some extend
event ev â‰ºE e recording v.

Proof. By deï¬nition, the values contained in a PCR are exactly those that were
previously extended into it. Thus, since ext events are the only way to extend
âŠ“âŠ”
values into PCRs, there must be some event ev = ext(o, v, p) with ev â‰ºE e.

Lemma 4. Let e âˆˆ E be an event with input parameter v. If v âˆˆ N or if v is
a signature using key sk (t) âˆˆ K, then there is a prior event eâ€² â‰ºE e such that
out(eâ€²) = v.

Proof. Deï¬nition 10 requires v to be derivable from the public terms P and the
output of previous messages. Call those outputs O.

First suppose v âˆˆ N . Since v is atomic, the only way to derive it is if v âˆˆ PâˆªO.

Since P âˆ© N = âˆ…, v 6âˆˆ P, hence v âˆˆ O as required.

Now suppose v is a signature using key sk (t) âˆˆ K. Then v can be derived in
two ways. The ï¬rst is if v âˆˆ P âˆª O. In this case, since v 6âˆˆ P it must be in O
instead as required. The other way to derive v is to construct it from the key
sk (t) and the signed message, say m. That is, we must ï¬rst derive sk (t). Arguing
as above, the only way to derive sk (t) is to ï¬nd it in O, but there are no events
âŠ“âŠ”
that output such a term.

Lemma 5. Let E be an execution producing quote Q. Assume vi is contained
before vj in PCR p reported on by Q, and let ei and ej be the events recording
vi and vj respectively. Then ei â‰ºE ej.

Proof. This is an immediate consequence of how PCR state evolves according
âŠ“âŠ”
to ext events.

Corollary 1. Let E be an execution producing quotes Q, and Qâ€² where Q reports
on PCR p. Suppose Qâ€² is contained in p before v. Then every event recording
values contained in Qâ€² occurs before the event recording v.

Proof. By Lemma 5, the event eQâ€² recording Qâ€² is before the event ev recording
v. Qâ€² is an input to eQâ€² satisfying the hypotheses of Lemma 4, hence there must
be a prior quote event eq â‰ºE eQâ€² with out (eq) = Qâ€². By Lemma 3 all events evi
recording values vi contained in Qâ€² must occur before eq. By the transitivity of
â‰ºE we conclude evi â‰ºE ev for each vi.
âŠ“âŠ”

7.2 Formalizing and justifying a bundling strategy.

Using these lemmas, we aim to understand the properties of an execution E if
it produces a set of quotes constructed according to Strategy 3 from Section 5.
We ï¬rst formalize the tiered, nested structure of this bundling strategy.

Deï¬nition 11. Let e = ext(o, v, p) be an extend event in execution E such that
v âˆˆ MV(ot) for some ot âˆˆ O. We say e is well-supported iï¬€ either

24

i. o = rtm, or
ii. for every o âˆˆ D1(ot) there is an extend event eâ€² â‰ºE e such that eâ€² =

ext(oâ€², vâ€², pâ€²) with vâ€² âˆˆ MV(o).

A collection of extend events X extends bottom-up iï¬€ each e âˆˆ X is well-
supported.

Bundling Strategy. Let Q be a set of quotes. We describe how to create a
measurement speciï¬cation S(Q). For each Q âˆˆ Q, and each p that Q reports on,
and each v âˆˆ MV(o2) contained in p, S(Q) contains an event ev = ms(o1, o2)
where M (o1, o2) and L(o1, p). Similarly, for each n in the nonce ï¬eld of some
Q âˆˆ Q, S(Q) contains the event att-start(n). Let SQ denote the set of events
derived in this way from Q âˆˆ Q. Then e â‰ºS(Q) ev iï¬€ Q is contained before v and
e âˆˆ SQ. Q complies with the bundling strategy iï¬€ S(Q) measures bottom-up.

Proposition 1. Suppose E âˆˆ E(Q) where S(Q) measures bottom-up. Then E
contains an extension substructure XQ that extends bottom-up.

Proof. Let XQ be the subset of events of E guaranteed by Lemma 3. That is,
XQ consists of all the events e = ext(o, v, p) that record measurement values
v reported in Q. For any such event e, if o = rtm then e is well-supported by
deï¬nition. Otherwise, since S(Q) measures bottom-up, Lemma 3 and Corollary 1
ensure XQ contain events eâ€² = ext(oâ€², vâ€², pâ€²) for every oâ€² âˆˆ D1(o) where eâ€² â‰ºE e.
âŠ“âŠ”
Thus e is also well supported in that case.

We make two key assumptions about executions of attestation systems.

Assumption 2 If E contains an event e = ext(o, v, p) with v âˆˆ MV(t), where
o is regular at that event, then there is an event eâ€² = ms(o, t) such that eâ€² â‰ºE e.
Furthermore, the most recent such event eâ€² satisï¬es out(eâ€²) = v.

Assumption 3 Suppose E has events e â‰ºE eâ€² where e = ms(o2, o1) and eâ€² =
ext(o, v, p) where v âˆˆ MV(t), o1 âˆˆ D1(t). Then either

1. o is corrupt at eâ€², or
2. there is some eâ€²â€² = ms(o, t) with e â‰ºE eâ€²â€² â‰ºE eâ€².

The ï¬rst assumption says that when extending measurement values regular
components only extend the value they most recently generated through mea-
surement. The second assumption is more complex. It is meant to guarantee that
measurements at higher layers are at least as fresh as the measurements of the
lower layers they depend on. Thus, whenever a deeper component takes a mea-
surement, there must be some signal to the upper layer to tell those components
to expire any measurements they have taken.

These two assumptions will not be validated in all attestation systems. These
are relatively subtle properties that can be expressed in, say, SELinux policies,
but would be diï¬ƒcult to implement in a less constrained architecture based
on a more commodity operating system. We show that these assumptions are

25

suï¬ƒcient to ensure Strategy 3 for bundling evidence is a good one, but they
may not be necessary. Furthermore, if a technology other than a TPM is used
for bundling, say Intelâ€™s SGX, then another set of assumptions may be more
appropriate.

Theorem 2. Let E be an execution satisfying Assumptions 2 and 3 that also
contains an extension substructure X that extends bottom-up. For each extend
event e = ext(o1, vt, p1), suppose that vt âˆˆ G(ot). Then for each such e, either

1. e reï¬‚ects a measurement event that is well-supported by measurement events

reï¬‚ected by the support of e.

2. a. some o2 âˆˆ D2(ot) gets corrupted in E, or

b. some o1 âˆˆ D1(ot) gets corrupted in E after being measured.

Proof. The proof considers an exhaustive list of cases, demonstrating that each
one falls into one of Conditions 1, 2a, or 2b. The following diagram summarizes
the proof by representing the case structure and indicating which condition each
case satisï¬es.

2

â€¢
1 
C1

2

2

/ â€¢
1 
C2a

/ â€¢
1 
C1

/ â€¢
1 
C2a

2

#âââââ

C2b

Consider any extend event e = ext(o1, vt, p1) of X extending a measurement

value for some ot âˆˆ O. The ï¬rst case distinction is whether or not o1 = rtm.

Case 1: Assume o1 = rtm. Since rtm cannot be corrupted, it is regular at
e, and by Assumption 2, e reï¬‚ects the measurement event ms(rtm, ot) which is
trivially well-supported, so Condition 1 is satisï¬ed.

Case 2: Assume o1 6= rtm. Since X extends bottom-up, it has events ei =
2 for every oi âˆˆ D1(ot), and for
2, vi
2 is

2 is corrupt at ei (Case 2.1), or each oi

2, pi

2) extending measurement values vi

ext(oi
each i, ei â‰ºE e. Now either some oi
regular at ei (Case 2.2).

Case 2.1: Assume some oi

2 is corrupt at ei. Then there must have been a

prior corruption of oi

2 âˆˆ D2(ot), and hence we are in Condition 2a.

Case 2.2: Assume each oi

each ei, so each one reï¬‚ects a measurement event eâ€²
regular at e (Case 2.2.1), or o1 is corrupt at e (Case 2.2.2).

2 is regular at ei. Then Assumption 2 applies to
i. In this setting, either o1 is

Case 2.2.1: Assume o1 is regular at e. Then since the events eâ€²

i together
with e satisfy the hypothesis of Assumption 3, we can conclude that e reï¬‚ects a
measurement event eâ€² = ms(o1, ot) such that eâ€²
i â‰ºE eâ€² for each i. That is, eâ€² is
well-supported by the eâ€²
i events which are reï¬‚ected by the support of e, putting
us in Condition 1.

Case 2.2.2: Assume o1 is corrupt at e. Since o1 âˆˆ D1(ot) one of the eâ€²
i
is a measurement event of o1 with output v1 âˆˆ G(o1) since X only extends
measurement values that do not indicate corruption. Call this event eâ€²
âˆ—. The
ï¬nal case distinction is whether o1 is corrupt at this event eâ€²
âˆ— (Case 2.2.2.1) or
regular at eâ€²

âˆ— (Case 2.2.2.2).

26


/

/

/

#
Case 2.2.2.1: Assume o1 is corrupt at eâ€²

good value, some element o2 âˆˆ D1(o1) âŠ† D2(ot) is corrupt at eâ€²
Condition 2a.

âˆ—. Since the measurement outputs a
âˆ—. This satisï¬es

Case 2.2.2.2: Assume o1 is regular at eâ€²

o1 is corrupt at e with eâ€²
event for o1. Since eâ€²

âˆ—. By the assumption of Case 2.2.2,
âˆ— â‰ºE e. Thus there must be an intervening corruption
âˆ— is a measurement event of o1, this satisï¬es Condition 2b.
âŠ“âŠ”

Assumption 3 can only guarantee that an object is remeasured whenever one
of its dependencies is remeasured. It cannot ensure that all orderings of S(Q)
are preserved in E(Q). For this reason we introduce the notion of the core of a
bottom-up speciï¬cation. The core of a bottom-up speciï¬cation S is the result
of removing any orderings between measurement events ei â‰ºS ej whenever ei is
not in the support of ej. That is, the core of S ignores all orderings that do not
contribute to S measuring bottom-up.

Theorem 3. Let E âˆˆ E(Q) such that S(Q) measures bottom-up, and let S â€² be its
core. Suppose that Q detects no corruptions, and that E satisï¬es Assumptions 2
and 3. Then one of the following holds:

1. E âˆˆ E(S â€²),
2. there is some ot âˆˆ O such that

a. some o2 âˆˆ D2(ot) is corrupted, or
b. some o1 âˆˆ D1(ot) is corrupted after being measured.

Proof. By Proposition 1, E contains a substructure XQ of extend events that
extends bottom-up. Thus by Theorem 2, Conditions 2a and 2b are possibilities.
So suppose instead that E satisï¬es Condition 1 of Theorem 2. We must show
that E âˆˆ E(S â€²). In particular, we construct Î± : S â€² â†’ E and show that it is label-
and order-preserving.

for each i.

Consider the measurement events es

i of S â€². By construction, each one comes
from some measurement value vi contained in Q. Similarly, the well-supported
measurement events em
i of E guaranteed by Theorem 2 are reï¬‚ected by extend
events ei of E which are, in turn, those events that record each vi in Q. We let
Î±(es

i ) = em
i
To see that Î± is label-preserving, consider ï¬rst the label of es

i . It corresponds
to a measurement value vi contained in some pi of Q. So es
i is labeled ms(o, oâ€²)
where M (o, oâ€²), vi âˆˆ MV(oâ€²), and L(o, pi). The event em
i also corresponds to
the same vi. Lemma 3 ensures that ei = ext(o, v, pi) with L(o, pi), and so the
measurement event it reï¬‚ects is em
i = ms(o, oâ€²) with M (o, oâ€²) and vi âˆˆ MV(oâ€²).
Thus es

i and em

We now show that if es

j . The former ordering exists
in S â€² because some quote Q âˆˆ Q is contained in pj before vj and vi is contained
in Q, and because es
j. By Corollary 1 ei â‰ºE ej and ei is in
the support of ej and therefore Theorem 2 ensures that the measurements they
reï¬‚ect are also ordered, i.e. em

i is in the support of es

j then em

i â‰ºE em

i â‰ºE em
j .

i have the same label.
i â‰ºS(Q) es

27

Finally, consider any events e = att-start(n) in S â€². They come from nonces
n found as inputs to quotes Q âˆˆ Q. By Lemma 4, E also has a corresponding
event eâˆ— with out(eâˆ—) = n. Since att-start events are the only ones with output
of the right kind, eâˆ— = att-start(n) as well. Thus we can extend Î± by mapping
each such e to the corresponding eâˆ—. The rules for S(Q) say that e â‰ºS(Q) eâ€² only
when Q has n in the nonce ï¬eld, and Q occurs before the value recorded by eâ€².
In E, eâˆ— precedes the event producing Q (by Lemma 4) which in turn precedes
eâ€² by Lemmas 4 and 5. Thus the orderings in S(Q) involving att-start events are
âŠ“âŠ”
also preserved by Î±.

8 Conclusion

In this paper we have developed a formalism for reasoning about layered attes-
tations. Within this framework we have justiï¬ed the intuition (pervasive in the
literature on measurement and attestation) that it is important to measure a
layered system from the bottom up (Theorem 1). We also proposed and justiï¬ed
a strategy for using TPMs to bundle evidence (Theorem 2). If used in conjunc-
tion, these two results guarantee an appraiser that if an adversary has corrupted
a component and managed to avoid detection, then it must have performed a
recent or deep corruption (Theorem 3).

Although we used our model to justify the proposed general and reusable
strategies for layered attestations, we believe our model has a wider applicability.
It admits a natural graphical interpretation that is straightforward to understand
and interpret. Future work to develop reasoning methods within the model could
lead to more automated analysis of attestation systems. We believe a tool that
leverages automated reasoning and the graphical interpretation would be a useful
asset.

For the present work we made several simplifying assumptions. For instance,
we assumed that if measurers (or their supporting components) are corrupted,
then they can always forge the results of measurement. This conservative, worst-
case view does not account for a situation in which, say, even if the OS kernel is
corrupted, it may still be hard to forge the results of a virus scan. Conversely,
we also assumed that uncorrupted measurers can always detect corruptions.
This is certainly not true in most systems. Adapting the model to account for
probabilities of detection would be an interesting line of research that would
make the model applicable to a wider class of systems.

Another issue of layered attestations that we did not address here, is the
question of what to do when the system components fall into diï¬€erent adminis-
trative domains. This would be typical of a cloud architecture in which the lower
layers are administered by the cloud provider, but the customers may provide
their own set of measurement capabilities as well. A remote appraiser must be
able to negotiate an attestation according several policies. Our model might be
extended to account for the complexities that arise.

Finally, we chose to study the use of TPMs for bundling evidence. We be-
lieve other approaches leveraging timing-based techniques or other emerging

28

technologies including hardware-supported trusted execution environments such
as Intelâ€™s new SGX instruction set could be captured similarly. This would al-
low us to formally demonstrate the security advantages of one approach over
another, or understand how to build attestation systems that leverage several
technologies.

Acknowledgments

I would like to thank Pete Loscocco for suggesting and guiding the direction of
this research. Many thanks also to Perry Alexander and Joshua Guttman for
their valuable feedback on earlier versions of this work. Finally, thanks also to
Sarah Helble and Aaron Pendergrass for lively discussions about measurement
and attestation systems.

References

1. Serdar Cabuk, Liqun Chen, David Plaquin, and Mark Ryan. Trusted integrity
measurement and reporting for virtualized platforms. In Trusted Systems, First
International Conference, INTRUST 2009, Beijing, China, December 17-19, 2009.
Revised Selected Papers, pages 180â€“196, 2009.

2. George Coker, Joshua D. Guttman, Peter Loscocco, Amy L. Herzog, Jonathan K.
Millen, Brian Oâ€™Hanlon, John D. Ramsdell, Ariel Segall, Justin Sheehy, and
Brian T. Sniï¬€en. Principles of remote attestation. Int. J. Inf. Sec., 10(2):63â€“81,
2011.

3. Intel Corporation. Open attestation. Accessed: 2015-12-16.
4. Anupam Datta, Jason Franklin, Deepak Garg, and Dilsun Kirli Kaynar. A logic of
secure systems and its application to trusted computing. In 30th IEEE Symposium
on Security and Privacy (S&P 2009), 17-20 May 2009, Oakland, California, USA,
pages 221â€“236, 2009.

5. Lucas Davi, Ahmad-Reza Sadeghi, and Marcel Winandy. Dynamic integrity mea-
surement and attestation: towards defense against return-oriented programming
attacks. In Proceedings of the 4th ACM Workshop on Scalable Trusted Computing,
STC 2009, Chicago, Illinois, USA, November 13, 2009, pages 49â€“54, 2009.

6. StÃ©phanie Delaune, Steve Kremer, Mark Dermot Ryan, and Graham Steel. Formal
analysis of protocols based on TPM state registers. In Proceedings of the 24th IEEE
Computer Security Foundations Symposium, CSF 2011, Cernay-la-Ville, France,
27-29 June, 2011, pages 66â€“80, 2011.

7. Charles Fisher, Dave Bukovick, Rene Bourquin, and Robert Dobry. SAMSON -

Secure Authentication Modules. Accessed: 2015-12-16.

8. Trusted Computing Group. TPM Main Speciï¬cation Level 2 version 1.2, 2011.
9. Trusted Computing Group. TCG Trusted Network Connect Architecture for In-

teroperability version 1.5, 2012.

10. Chongkyung Kil, Emre Can Sezer, Ahmed M. Azab, Peng Ning, and Xiaolan
Zhang. Remote attestation to dynamic system properties: Towards providing
complete system integrity evidence.
In Proceedings of the 2009 IEEE/IFIP In-
ternational Conference on Dependable Systems and Networks, DSN 2009, Estoril,
Lisbon, Portugal, June 29 - July 2, 2009, pages 115â€“124, 2009.

29

11. Peter Loscocco, Perry W. Wilson, J. Aaron Pendergrass, and C. Durward Mc-
Donell. Linux kernel integrity measurement using contextual inspection. In Pro-
ceedings of the 2nd ACM Workshop on Scalable Trusted Computing, STC 2007,
Alexandria, VA, USA, November 2, 2007, pages 21â€“29, 2007.

12. Richard Maliszewski, Ning Sun, Shane Wang, Jimmy Wei, and Ren Qiaowei.

Trusted boot (tboot). Accessed: 2015-12-16.

13. John D. Ramsdell, Daniel J. Dougherty, Joshua D. Guttman, and Paul D. Rowe.
A hybrid analysis for security protocols with state. In Integrated Formal Methods -
11th International Conference, IFM 2014, Bertinoro, Italy, September 9-11, 2014,
Proceedings, pages 272â€“287, 2014.

14. Reiner Sailer, Xiaolan Zhang, Trent Jaeger, and Leendert van Doorn. Design and
implementation of a tcg-based integrity measurement architecture. In Proceedings
of the 13th USENIX Security Symposium, August 9-13, 2004, San Diego, CA, USA,
pages 223â€“238, 2004.

15. Jinpeng Wei, Calton Pu, Carlos V. Rozas, Anand Rajan, and Feng Zhu. Mod-
eling the runtime integrity of cloud servers: A scoped invariant perspective.
In
Cloud Computing, Second International Conference, CloudCom 2010, November
30 - December 3, 2010, Indianapolis, Indiana, USA, Proceedings, pages 651â€“658,
2010.

30

