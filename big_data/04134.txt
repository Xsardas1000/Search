RISAS: A Novel Rotation, Illumination, Scale Invariant

Appearance and Shape Feature

Xiaoyang Liâˆ—1, Kanzhi Wuâˆ—2, Yong Liu1â€ , Ravindra Ranasinghe2, Gamini Dissanayake2 and Rong Xiong1

6
1
0
2

 
r
a

 

M
4
1

 
 
]

O
R
.
s
c
[
 
 

1
v
4
3
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstractâ€” In this paper, we present a novel RGB-D feature,
RISAS, which is robust to Rotation, Illumination and Scale
variations through fusing Appearance and Shape information.
We propose a keypoint detector which is able to extract
information rich regions in both appearance and shape using a
novel 3D information representation method in combination
with grayscale information. We extend our recent work on
Local Ordinal Intensity and Normal Descriptor(LOIND)[1], to
further signiï¬cantly improve its illumination, scale and rotation
invariance using 1) a precise neighbourhood region selection
method and 2) a more robust dominant orientation estimation.
We also present a dataset for evaluation of RGB-D features,
together with comprehensive experiments to illustrate the
effectiveness of the proposed RGB-D feature when compared
to SIFT[2], C-SHOT[3] and LOIND. We also show the use
of RISAS for point cloud alignment associated with many
robotics applications and demonstrate its effectiveness in a
poorly illuminated environment when compared with SIFT and
ORB[4].

I. INTRODUCTION

In both ï¬elds of computer vision and robotics, feature
matching is a fundamental problem in various applications
such as object detection, image retrieval and robotic tasks
such as vision based Simultaneous Localisation and Map-
ping (SLAM). Two essential steps critical to ï¬nding robust
and reliable correspondences are 1) extracting discrimina-
tive keypoints and 2) building invariant descriptors. In the
past decade, there has been enormous progress in devel-
oping robust features in 2-dimension image space such as
SIFT(Scale Invariant Feature Descriptor)[2], SURF(Speed-
Up Robust Descriptor)[5] and ORB(Oriented FAST and
Rotated BRIEF)[6]. When rich texture information is avail-
able, these methods achieve excellent performance under
signiï¬cant variation to both scale and rotation. However,
their performance can degrade drastically under illumination
variations and environments with poor texture information.
With the development of low-cost, real-time depth sensors
such as Kinect and Xtion, depth information of the envi-
ronment can be easily obtained, thus it is now prudent to
consider geometric information in building local descriptors.
Spin Image[7] is one of the well-known 3D descriptors which

*This indicates equal contribution of the ï¬rst two authors.
1Xiaoyang Li, Yong Liu and Rong Xiong are with Institute
and Control, Zhejiang University, Hangzhou,
yongliu,

lixiaoyang@163.com,

of Cyber-Systems
310027,
China.
rxiong@iipc.zju.edu.cn

2Kanzhi Wu, Ravindra Ranasinghe and Gamini Dissanayake are with
Centre for Autonomous Systems, University of Technology Sydney, Ul-
timo, 2007, Australia. kanzhi.wu, ravindra.ranasinghe,
gamini.dissanayake@uts.edu.au

â€ Yong Liu is the corresponding author of this paper.

is widely used in 3D surface registration tasks. Rusu and
et al. also made signiï¬cant contributions to the literature
in depth descriptors and proposed various methods such
as PFH(Persistent Feature Histogram) and NARF(Normal
Aligned Radial Feature). However, relying depth information
alone makes the correspondence problem more challenging
due to two reasons: 1) sensor information from Kinect
and Xtion is general
incomplete and 2) depth image is
much less informative when compared with RGB/grayscale
image, particularly in textured regions. Due to the comple-
mentarity nature of RGB/grayscale information and depth
information, combining appearance and geometric informa-
tion is a promising approach to building descriptors and
improving matching performance. C-SHOT(Color Signature
of Histogram and OrienTation) and BRAND(Binary Robust
Appearance and Normal Descriptor) are typical examples
RGB-D descriptors. Compared with existing prominent 2D
image features such as SIFT and SURF, a major limitation
of current RGB-D detectors and descriptions is that they are
designed without simultaneously considering effectiveness
and both detection and description, which the present paper
aims to address.

Fig. 1. Feature matching example using RISAS under severe rotation, scale,
viewpoint and illumination variations. The correct matches are shown using
green lines and incorrect matches are denoted by red lines.

In this paper, we present a novel Rotation, Illumination
and Scale invariant Appearance and Shape feature (RISAS)
which combines a discriminative RGB-D keypoint detector
and an invariant feature descriptor. Fig.1 illustrates the abil-
ity of RISAS for obtaining correspondences under severe
rotation, scale and illumination changes. Section. II reviews
the related work in traditional RGB/grayscale local feature
and descriptors, depth and RGB-D descriptors. In section
III, We introduce the proposed keypoint detector computed
using both grayscale image and a novel representation of
depth image. Based on our previous work LOIND[1], a
novel RGB-D descriptor with signiï¬cant improvements is
also explained in section III. In section IV, we present a

dataset speciï¬cally designed for RGB-D feature evaluation.
RISAS is evaluated using four different criteria: illumination,
scale, rotation and viewpoint and comparative experiments
are conducted against SIFT, C-SHOT and the earlier version
of LOIND. Comprehensive experiment results obtained using
the proposed dataset as well as a public RGB-D dataset
substantiate the superior performance of RISAS. We also
adopted RISAS into point cloud alignment task. Under poor
illuminated environments, compared with SIFT and ORB,
our approach can achieve more consistent alignment results.
Conclusion and future work are discussed in section V.

II. RELATED WORK

There have been extensive research on feature detectors
and descriptors for 2D images. The well-known SIFT fea-
ture, proposed by Lowe[2] with superior scale invariance,
has been widely used in academic and industrial ï¬elds.
Bay and et al.[5] presented SURF which follows a similar
strategy to SIFT but requires less computation time and
even outperforms SIFT with respect
to repeatability and
robustness. Because of the computational cost of using SIFT
and SURF in real-time applications such as SLAM and
tracking, binary descriptors such as BRIEF (Binary Robust
Independent Elementary Features), BRISK (Binary Robust
Invariant Scalable Keypoints) and ORB became popular
since 2010. With less storage consumption and computation
time, the most recent binary descriptor, ORB, is able to
achieve comparable performance even compared with SIFT
and SURF. Other types of descriptors are also proposed such
as OSID (Ordinal-Spatial Intensity Distribution) [8] which
computes the histogram to combine the relative ordering of
the pixel intensities and spatial information.

Even though building descriptor from 2D images has been
the focus in the past decade, it is not surprising that descrip-
tors can also be constructed using 3D geometric information.
In 1997, Johnson and Hebert [7] proposed Spin Image to
match surfaces. Recently, with the development of low-cost
RGB-D sensors, geometry information of the environment
can be easily captured thus the importance of 3D shape de-
scriptors have again been recognised. Various 3D descriptors
have been proposed in the literature and currently available
as implementations in the main public domain environment
for dealing with point clounds,Point Cloud Library. Typi-
cal examples are PFH[9], NARF[10], VFH[11] and SHOT
(Signatures of Histograms of Orientations)[12]. Guo and et
al. presented a comprehensive performance evaluations and
comparisons of the popular 3D local feature descriptors in
[13]. Despite the progress in 3D shape descriptors, however,
because of the fact
that 3D shape(depth) information is
not rich compared with RGB or grayscale image, a shape
descriptor alone is able to provide reliable and robust feature
matching results.

Lai and et al. [14], proved that by joining RGB and
depth channels together, better recognition performance can
be achieved for object recognitions? when compared with
using one channel only. Tombari et al. introduced colour
information into SHOT descriptor and proposed C-SHOT.

C-SHOT improves the feature matching accuracy and shows
better performances for object recognition under cluttered
environment. Nascimento and et al. proposed a binary RGB-
D descriptor named BRAND (Binary Robust Appearance
and Normals Descriptor)[15] which is invariant to rotation
and scale transform. More recently, Feng and et al. proposed
LOIND which encodes the texture and depth information into
one descriptor using histogram representation which shows
better performance compared with BRAND and C-SHOT on
public datasets.

III. METHOD

In this section, we describe the proposed Illumination,
Scale and Rotation invariant Appearance and Shape feature,
RISAS, in detailed. Compared with previous work[1], except
for the further improvement on constructing a better descrip-
tor, we proposed an RGB-D keypoint detector speciï¬cally
designed for the new descriptor called LOIND+. The detector
and LOIND+ are explained in detail in Section.III-A and
Section.III-B.
A. Keypoint Detector
In Fengâ€™s work[1],

the traditional keypoint detectors,
Harris-Afï¬ne detector and Hessian-Afï¬ne detector[16], are
applied in extracting regions of interest from gray-scale
images even though both RGB and depth information are
available. Because of the fact that depth information is not
considered in detection phase, regions which are information-
rich in depth channel are ignored due to the lack of texture
information.

Considering depth information in keypoint detection helps
in at least two aspects: 1) more number of keypoints are
expected to be extracted and 2) more â€œdiscriminantâ€ depth
information will be used in building the descriptor and
further improve the feature matching performance. In this
paper, we propose an RGB-D keypoint detector method
which is highly coupled with LOIND+ and the performances
of LOIND+ is maximised when using the proposed detector.

Fig. 2.
Flowchart of the proposed RGB-D detector. Irgb is the original
RGB image and Igrayscale is the converted grayscale image. Inormal is
the 3 channel normal vector image and Idp is the computed dot product
image.

The ï¬‚owchart of the proposed detector is shown in Fig. 2

and the key steps are listed below:

1) For each point in the depth image Idepth, we calculate
the surface normal vector. From the 3 components of
the normal vector, we create the corresponding normal
image Inormal with 3 channels.

Detected Keypointsğ¼ğ‘Ÿğ‘”ğ‘ğ¼ğ‘‘ğ‘’ğ‘ğ‘¡â„ğ¼ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ¼ğ‘‘ğ‘Weighted sumSelf-correlationSelf-correlationSensor Input2) Using Inormal, compute the three angles [Î±, Î², Î³]
between each normal vector and the [x, y, z] axis of
the camera coordinate system respectively. Note that
angles are in the continuous space in range [0, Ï€].
The angle range [0, Ï€]
is segmented into 4 sector
labelled with [1, 2, 3, 4] as shown in Fig.3 and each
computed angle is mapped into one of these sectors.
has
For example, normal vector n =
the [Î±, Î², Î³] = [54.7â—¦, 54.7â—¦, 54.7â—¦] will be labelled as
[2, 2, 2];

(cid:104)âˆš

âˆš
3
3 ,

(cid:105)

3
3 ,

âˆš

3
3

3) Using this labelled image, build a statistical histogram
to capture the distribution of labels along each channel.
From this histogram, we choose the highest entry
and use the corresponding label [nX , nY , nZ] to rep-
resent the most frequent label where nX , nY , nZ âˆˆ
[1, 2, 3, 4]. Using these 3 values, we deï¬ne what we
call the main normal vector nmain of the depth image
Idepth. This vector represents the relationship between
the sensor and the scene;

4) Calculate the dot-product between the nmain and each
normal vector in Inormal. This dot product value is
an indicator of how similar a given normal vector
to the main normal vector which is approximately
consistent when the sensor is pointing at the same
scene from different angle. We then normalise the dot
product value into range [0, 255]. Using this value,
we create the novel dot-product image Idp which can
describe the variation of information in depth channel.
This is highly coupled with the following descriptor
construction step since the geometrical information is
encoded using the dot-product value;

5) We adopt the similar principle as in Harris detector to
compute the response value E(u, v). We use both the
grayscale image Igrayscale and the dot product image
Idp to compute the response value and set a threshold
to select the point which shows an extreme value in the
weighted sum of response values, as shown in Eq.1:

(cid:88)

x,y

E (u, v) =

Ï‰(x, y)[Î± (I(x + u, y + v) âˆ’ I(x, y))2

+ (1 âˆ’ Î±) (P (x + u, y + v) âˆ’ P (x, y))2]

(1)
where Ï‰(x, y) is the window function centred at (x, y)
which is a Gaussian function in this paper. I(u, v)
is the intensity value at (u, v) and P (u, v) is the
normalised dot product value at (u, v). Since the RGB
information is more accuracy compared with noisy
depth information, in this paper, Î± is set to be 0.8.

Fig. 3. Flowchart of calculating the main normal.

Section.IV.

B. LOIND+

1) Scale Estimation and Neighbourhood Region Selection:
In grayscale image, the scale of the keypoint is estimated
by ï¬nding the extreme value in scale space using image
pyramid, such as SIFT[2] and SURF[5]. However, the scale
invariance will be degraded once the texture information
in the neighbourhood is poor or under large illumination
variations. With the development of modern RGB-D sensors
such as Kinect and Xtion, the scale can be easily measured
using the depth information captured from the sensor. In both
LOIND[1] and BRAND[15], the following empirical equa-
tion is applied by assuming the linear relationship between
depth measurement d and scale s in range [2, 8] meters.

s = max(0.2,

3.8 âˆ’ 0.4 max(2, d)

3

)

(2)

One obvious problem in Eq. 2 is that it is not applicable to
closer range within 2 meters where the sensor is frequently
used. After s is estimated, the neighbourhood region that
is used to build the descriptor is selected with radius R
which varies w.r.t. s in a linear relationship, as shown in
[1][15]. However, there are two facts ignored in selecting
the neighbourhood region: 1) the depth resolution is not
constant and decreases with increasing distance to the sensor
[17], thus the linear assumption no longer holds; 2) select
neighbourhood region using radius R without considering the
geometric information in the neighbourhood is likely to in-
clude irrelevant background points, shown in Fig.4. In order
to address this, we present a more accurate neighbourhood
region selection method for building descriptor:

Using the proposed approach, the regions which are lack
of depth information are ï¬rst eliminated. Also by introducing
the novel representation of depth image, Idp, we can extract
keypoints from not only appearance information rich region
but also geometry information rich region. The designed
detector is highly coupled with the principle of LOIND+
and drastically improves the performance of LOIND+ under
different scenarios and the detailed results will be shown in

Fig. 4. Neighbourhood selection: The default strategy (left) selects the
whole region (shown in red) which covers both foreground and background
area. However, the introduced background points are not necessary for
building the local descriptor. Our approach (right) eliminates the background
points (shown in blue) and constructs the descriptor using the foreground
(shown in red) only which achieves more robust descriptor matching
performances.

ğ‘¥ğ‘¦ğ‘§ğ›¼ğ›½ğ›¾ğ›¼ğ›½ğ›¾main normal123412341234Discretizedlabeling3D Histogram Computation(for all normal vectors)Select the dominant value as the main normal vector1) Based on Eq.2, initial value of the scale s is estimated
and the radius R of the patch is computed using Eq.3.

orientation d3D of the patch in below:

(cid:18)

(cid:18)

(cid:19)(cid:19)

R =

âˆ’5 + 25 âˆ— min

3,

max(0.2.smax)
max(0.2.smin)

âˆ—s (3)

d3D =

Where smax and smin are the maximum and minimum
scale values in the image. Here we denote the patch
centred at keypoint ki in 2D image space as Puv(ki)
and the corresponding patch in 3D point cloud space
is represented as Pxyz(ki);
2) For each point p âˆˆ Pxyz(ki), we remove the outlier
neighbouring points which are relatively far away
from the keypoint ki according to Eq.4 . This simple
elimination step enables signiï¬cant improvements with
LOIND+ descriptor.

(cid:40)

1

0

(cid:13)(cid:13)(cid:13)p âˆ’ ki

if
otherwise

(cid:13)(cid:13)(cid:13) < t

f (p) =

(4)

where t is the threshold and set to be 0.1 meter in
this work. We only keep the neighbouring points with
f (p) = 1;

3) We conduct ellipsoid ï¬tting for the processed 3D
neighbouring points Â¯Pxyz(ki) based on the following
equation.

(x âˆ’ kix)2

a2

+

(y âˆ’ kiy)2

b2

+

(x âˆ’ kiz)2

c2

= 1

(5)

where a, b and c are the length of the axes. We project
the 3D ellipsoid into the image space for the new
accurate patch P with radius Â¯R for further descriptor
construction.

2) Orientation Estimation:

In order to achieve robust
rotation invariance, the traditional methods use the texture in-
formation which can fail under severe illumination variations.
In our previous work, LOIND[1], the dominant orientation
Î¸ is computed from the depth information rather than the
appearance information and works reasonably well under
different variations. However, the proposed approach is not
computation efï¬cient and more importantly, Î¸ is sensitive to
the noise in neighbourhood region where the normal vectors
are similar to each other.

In this work, still using depth information, we propose an
alternative novel dominant orientation estimation algorithm
which is more robust and efï¬cient compared with LOIND[1],
as shown in detail below:

1) Given the processed 2D patch Â¯Puv and 3D patch Â¯Pxyz,
we adopt PCA to compute the eigenvalues [e1, e2, e3]
(in descending order) and corresponding eigenvectors
[v1, v2, v3].

2) Given the eigenvectors [v1, v2, v3], here we provide a
approximation method for computing the 3D dominant

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£³

v1+v2
|v1+v2|

v1+v2+v3
|v1+v2+v3|

v1

if (e2 > Ï„ e1)âˆ§
(e3 â‰¤ Ï„ e1)
if (e2 > Ï„ e1)âˆ§
(e3 > Ï„ e1)

others

(6)

where Ï„ is set within [0, 1]. If the e1 is signiï¬cantly
larger than other two, the 3D dominant orientation is
set to be the corresponding eigenvector v1. If e2 is
closer to e1, both eigenvector v1 and v2 are considered
in computing the dominant orientation. If further both
e1 and e2 are closer to e1 which means no clear
differences between 3 eigenvalues, all 3 eigenvector are
used in computing d3D. Threshold Ï„ determines when
the other two eigenvalues e2 and e3 can be regarded
as â€œcloserâ€ enough to the largest eigenvalue e1 which
is set to be 0.8 in this paper.

3) Project the 3D dominant direction d3D into the image
plane and get the 2D dominant direction d2D. The
angle Î¸ between d2D and u axis in image space is
set to be the dominant orientation.

3) Descriptor Construction: Based on the results from the
above steps, we can construct the descriptor of keypoint ki =
[u, v] using the neighbourhood region with radius R and the
orientation Î¸. We still follow on the main idea of LOIND[1].
The descriptor is based on the relative order information
in both grayscale and depth channels. The descriptor is
constructed in a three-dimensional space, as show in Fig.
5 below where [x, y, z] axes denote the spatial labelling, the
intensity labelling and the angles labelling respectively.

Fig. 5. Flowchart of the RGB-D descriptor.

- Spatial Distribution Encoding

For spatial distribution,
the pixels in the region
(u, v, R, Î¸) are labeled based on npie equal-size spatial
sectors. With larger number of pies, the descriptor is
supposed to be more discriminative and also more
time-consuming for both construction and matching.
In previous work, LOIND[1], npie is set to be 8 which
shows a good trade-off between its pros and cons and
we use the same value in this work.
- Grayscale Information Encoding

Instead of constructing the descriptor in the absolute
intensity space, we build the statistical histogram using
the relative intensity with respect to the intensity value

Spatial DistributionIntensity ordinal labelingNormal vector ordinal labelingRasterization3-D histogramğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘£ğ‘’ğ‘ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘›ğ‘›ğ‘ğ‘–ğ‘’=1ğ‘›ğ‘ğ‘–ğ‘’=2ğ‘›ğ‘ğ‘–ğ‘’=3ğ‘›ğ‘ğ‘–ğ‘’=4ğ‘›ğ‘ğ‘–ğ‘’=8ğ‘›ğ‘ğ‘–ğ‘’=7ğ‘›ğ‘ğ‘–ğ‘’=5ğ‘›ğ‘ğ‘–ğ‘’=6of the keypoint. According to the rank of all the pixels
in the patch,we group the intensity values into nbin
equally sized bins. For example, given 100 intensity
levels and 10 bins, each bin has 10 intensity levels
(i.e., orderings of [1, 10], [11, 20], . . . , [91, 100] ).

- Geometrical Information Encoding

Given the normal vector of each point, we ï¬rst com-
pute the dot product between the normal vector of the
selected keypoint npk and the normal vector each point
in the neighbourhood patch npi.

Ïi = |(cid:104)npk , npi(cid:105)|

(7)

Due to the fact that normal vectors from small patches
are similar to each other, thus the distribution of Ïi
is highly unbalanced where the majority of Ïi will
fall into the range close to 1. In this work, we set a
threshold Â¯Ï = 0.9 and any Ïi â‰¥ Â¯Ï will be grouped into
a special case. Other remaining dot product results will
be ranked and grouped into nvec bins and the points
will be labelled based on the group they belong to re-
spectively. Therefore, in normal vector space encoding,
there are overall nvec + 1 labels and in our work nvec
is set to be 2.

In the end, we can encode every neighbouring pixel in the
patch as (i, j, k),and then build a three-dimensional statistical
histogram to get a 192-dimensional normalised descriptor for
the patch.

IV. EXPERIMENTS AND RESULTS

In this section, in order to evaluate the proposed RGB-D
feature on different criterion independently, we constructed
an RGB-D descriptor evaluation dataset1. We present the
comparative results of RISAS against SIFT, CSHOT and
LOIND on the constructed dataset. We also compared these
methods on the public RGB-D dataset which is originally
designed for object detection2. Furthermore, we demonstrate
the result for point cloud alignment using RISAS as an appli-
cation. Under poor illuminated environment, our algorithm
achieves more consistent results compared with using SIFT
and ORB.

A. Performance Evaluation

1) Evaluation Metric: We adopt

the same evaluation
metric proposed by Mikolajczyk and Schmid[18]. We ï¬rst
extract interesting points from the two frames and compute
the descriptors for all keypoints. Nearest neighbour distance
ratio (NNDR) is used to establish the correspondences
between keypoints between different images. In the end,
we can use the formula below to determinate whether the
correspondence is correct:

||pi âˆ’ (Rpj + t)|| â‰¤ Ï„

(8)

where pi and pj are 3d points from two frame i and j. R
and t denote relative rotation and translation between frame
i and j. If the projected error is less than Ï„, the match is
regarded as a correct one and Ï„ is set to be 0.05 m in our
work.

2) Dataset and Results in Detail:

In this section, we
demonstrate the comparison results again SIFT, CSHOT and
LOIND on the public dataset and our dataset to validate the
effectiveness of RISAS3.
- Scale Invariance

In this experiment, we collect 10 frames of images
with the variations in z axis of the sensor coordinate
system. The ï¬rst frame which captured at 1.1 m is
selected as the reference image and all other images
are captured backwards every 0.1 m. We select 500
matches with highest matching scores as the total
matches. We decide whether the match is correct or not
according to Eq.8. The matching accuracy is shown in
Fig.6. The proposed RISAS shows signiï¬cant higher
percentage of inliers compared with SIFT and LOIND
and slightly improvement compared with CSHOT.

Fig. 6. Matching performance of
SIFT, LOIND, CSHOT and RISAS
under scale variations.

Fig. 7. Matching performance of
SIFT, LOIND, CSHOT and RISAS
under synthetic rotation variations.

- Rotation Invariance

We evaluate the performance of RISAS under 2 types
of rotation transformations: synthetic in-plane rotation
and actual 3D rotation. We generated synthetic data
by rotating the original image every 10â—¦ and added
additional Gaussian noise to the rotated image. Fig.7
shows that RISAS achieves comparable performance
compared with CSHOT and outperforms SIFT and
LOIND signiï¬cantly.
We also evaluate RISAS under actual 3D rotation
shown in Fig. 8 and compared against SIFT, LOIND
and CSHOT and the precision-recall curves are pre-
sented in Fig.9. RISAS shows similar performance
compared with CSHOT while achieve signiï¬cant im-
provements compared with SIFT and LOIND.

- Illumination Invariance

In order to validate the performance of RISAS under
illumination variations, we constructed a dataset which
consists 4 different
levels of synthetic illumination
variations: 1) square 2) square root 3) cube and 4)
cube root, shown in the left column in Fig.10. We

1The dataset

can be downloaded from http://kanzhi.me/

rgbd-descriptor-dataset/

2http://rgbd-dataset.cs.washington.edu/

3SIFT has its own keypoint detector. For CSHOT and LOIND, we apply

the proposed detector in this work to both of them.

1.11.21.31.41.51.61.71.81.901020304050607080SIFTLOINDCSHOTRISAS0501001502002503003500102030405060708090100SIFTLOINDCSHOTRISASFig. 8. Example images of 3D rotations.

1

(a)

(b)

Fig. 9. Precision-Recall curves corresponding to images shown in Fig. 8.

also utilise neutral density ï¬lter to simulate the natu-
ral illumination variation. The Precision-Recall curves
corresponding to the left column images are also
shown in Fig.10 where the proposed RISAS shows
signiï¬cant better performance compared with SIFT,
LOIND and especially CSHOT.

- Viewpoint Invariance

In this section, we collected overall 24 images by
moving the sensor around the objects in 60â—¦ at 0.7
meters away from the objects. In order to compute the
ground-truth transformation across each pair of frames
and further evaluate the performance of descriptors,
we adopt RGBD-SLAM[19] to compute the optimised
poses and regard the optimised poses as the ground-
truth. The Precision-Recall curves of 4 pairs of images
are shown in Fig. 11. Again, the proposed RISAS
shows signiï¬cantly better performance compared with
C-SHOT, SIFT and LOIND.

3) Public Dataset Evaluation: Except for the proposed
dataset, we also validate RISAS on public RGB-D scene
dataset. We select the table 1 dataset from RGB-D scene
dataset. Parts of the images and results are presented in
Fig.12. RISAS also achieves better results compared with
SIFT, C-SHOT and LOIND.

B. Application: Point Cloud Alignment

thus creates signiï¬cant

In order to validate the performance of the proposed RGB-
D feature, we apply RISAS into the point cloud alignment
task. During the data collection, we turned off the indoor
light
illumination changes in the
environment. We match features across each observations
using SIFT, ORB and RISAS and compute the relative poses.
The point clouds from each steps are aligned and Iterative
Closest Point is used to obtain the ï¬nal alignment results. The
aligned clouds are visualised in Fig.13. Using SIFT feature
shows obvious error in the top right corner of the map. Result

(a) Square root illumination change

(b) Precision-Recall curve

(c) Square illumination change

(d) Precision-Recall curve

(e) Cube root illumination change

(f) Precision-Recall curve

(g) Cube illumination change

(h) Precision-Recall curve

(i) Illumination change using ND
mirror

(j) Precision-Recall curve

Fig. 10. RISAS evaluation under illumination variations.

00.10.20.30.40.50.60.70.8recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall0.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall0.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall0.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall0.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.60.70.8recall0.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS(a) Between image 12 and 1

(b) Between image 12 and 6

(c) Between image 12 and 18

(d) Between image 12 and 24

Fig. 11. Precision-Recall curves under viewpoint variations.

(a) Image 10 and 20.

(c) Image 70 and 80.

(e) Image 110 and 120.

(b)

(d)

(f)

(a) Aligned point cloud using SIFT

(b) Aligned point cloud using ORB

(c) Aligned point cloud using RISAS

Fig. 13. Point cloud alignment under illumination variations.

Fig. 12.
Evaluation results on RGB-D scene dataset table 1. The left
column shows the input images and the right column shows in the corre-
sponding Precision-Recall curve.

from ORB feature shows slightly offset in both corners.
RISAS provides the best aligned point cloud as shown in
Fig.13(c).

V. CONCLUSION

This paper presents, as far as we know, the ï¬rst RGB-
D feature which consists a highly coupled a RGB-D key-
point detector and descriptor. RISAS is invariant to multiple
variations include illumination, scale, rotation and viewpoint.
A novel 3D representation, dot-product image, is proposed

based on original depth image and it is further combined
with grayscale image to extract the interest keypoints using
the similar principle as Harris detector. We also propose
a novel RGB-D descriptor, LOIND+, which signiï¬cantly
improves the previous LOIND descriptor on every aspect. We
construct an RGB-D dataset speciï¬cally designed for RGB-
D feature evaluation. Through comprehensive comparisons,
RISAS shows exceptional performance not only on the
proposed dataset but also on publicly available datasets such
as RGB-D scene dataset. Through adopting RISAS into point
cloud alignment task, our approach shows more consistent
result compared with using SIFT and ORB.

00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISAS00.10.20.30.40.50.6recall00.10.20.30.40.50.60.70.80.91precisionSIFTLOINDCSHOTRISASACKNOWLEDGEMENT

This work is supported by the Joint Research Centre
for Robotics Research between Zhejiang University (ZJU)
and University of Technology Sydney (UTS). Kanzhi Wu
is also supported by University of Technology Sydney â€“
China Scholarship Council (UTSâ€“CSC) International Re-
search Scholarship.

REFERENCES

[19] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard, â€œ3-d
mapping with an rgb-d camera,â€ Robotics, IEEE Transactions on,
vol. 30, no. 1, pp. 177â€“187, Feb 2014.

[1] G. Feng, Y. Liu, and Y. Liao, â€œLoind: An illumination and scale
invariant rgb-d descriptor,â€ in Proc. IEEE International Conference on
Robotics and Automation (ICRAâ€™ 2015), May 2015, pp. 1893â€“1898.
[2] D. Lowe, â€œDistinctive image features from scale-invariant keypoints,â€
International Journal of Computer Vision, vol. 60, no. 2, pp. 91â€“110,
2004.

[3] F. Tombari, S. Salti, and L. Di Stefano, â€œA combined texture-shape
descriptor for enhanced 3d feature matching,â€ in Proc. IEEE Interna-
tional Conference on Image Processing (ICIPâ€™2011), Sep 2011, pp.
809â€“812.

[4] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, â€œOrb: an efï¬cient
alternative to sift or surf,â€ in IEEE International Conference on
Computer Vision (ICCVâ€™2011).

IEEE, 2011, pp. 2564â€“2571.

[5] H. Bay, T. Tuytelaars, and L. Van Gool, â€œSurf: Speeded up robust
features,â€ in Proc. European Conference on Computer Vision (ECCVâ€™
2006), 2006, vol. 3951, pp. 404â€“417.

[6] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, â€œOrb: An efï¬cient
alternative to sift or surf,â€ in Proc. IEEE International Conference on
Computer Vision (ICCVâ€™ 2011), Nov 2011, pp. 2564â€“2571.

[7] A. Johnson and M. Hebert, â€œUsing spin images for efï¬cient object
recognition in cluttered 3d scenes,â€ IEEE Transaction on Pattern
Analysis and Machine Intelligence (PAMI), vol. 21, no. 5, pp. 433â€“
449, May 1999.

[8] F. Tang, S. H. Lim, N. L. Chang, and H. Tao, â€œA novel feature
descriptor invariant to complex brightness changes,â€ in IEEE Inter-
national Conference on Computer Vision and Pattern Recognition
(CVPRâ€™2009).

IEEE, 2009, pp. 2631â€“2638.

[9] R. B. Rusu, Z. C. Marton, N. Blodow, and M. Beetz, â€œPersistent
point feature histograms for 3d point clouds,â€ in Proc. International
Conference on Intelligent Autonomous Systems (IASâ€™2008), 2008.

[10] K. K. Bastian Steder, Radu Bogdan Rusu and W. Burgard, â€œNarf:
3d range image features for object recognition,â€ in Proc. Work-
shop on Deï¬ning and SOlving Realistic Perception Problems at
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROSâ€™2010), 2010.

[11] R. Rusu, G. Bradski, R. Thibaux, and J. Hsu, â€œFast 3d recognition and
pose using the viewpoint feature histogram,â€ in Proc. IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROSâ€™2010),
Oct 2010, pp. 2155â€“2162.

[12] F. Tombari, S. Salti, and L. Di Stefano, â€œUnique signatures of his-
tograms for local surface description,â€ in Proc. European Conference
on Computer Vision (ECCVâ€™2010), 2010, pp. 356â€“369.

[13] Y. Guo, M. Bennamoun, F. Sohel, M. Lu, J. Wan, and N. Kwok, â€œA
comprehensive performance evaluation of 3d local feature descriptors,â€
International Journal of Computer Vision, pp. 1â€“24, 2015.

[14] K. Lai, L. Bo, X. Ren, and D. Fox, â€œSparse distance learning for object
recognition combining rgb and depth information,â€ in Proc. IEEE
International Conference on Robotics and Automation (ICRAâ€™2011),
May 2011, pp. 4007â€“4013.

[15] E. Nascimento, G. Oliveira, M. Campos, A. Vieira, and W. Schwartz,
â€œBrand: A robust appearance and depth descriptor for rgb-d images,â€
in Proc. IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROSâ€™ 2012), Oct 2012, pp. 1720â€“1726.

[16] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas,
F. Schaffalitzky, T. Kadir, and L. V. Gool, â€œA comparison of afï¬ne
region detectors,â€ International Journal of Computer Vision, vol. 65,
no. 1, pp. 43â€“72, 2005.

[17] K. Khoshelham, â€œAccuracy analysis of kinect depth data,â€ in Interna-
tional Archives of the Photogrammetry, Remote Sensing and Spatial
Information Sciences, vol. 38, 2011, p. 1.

[18] K. Mikolajczyk and C. Schmid, â€œA performance evaluation of local
descriptors,â€ IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 27, no. 10, pp. 1615â€“1630, 2005.

