Neural Architectures for Named Entity Recognition

Guillaume Lample♠ Miguel Ballesteros♣♠

Sandeep Subramanian♠ Kazuya Kawakami♠ Chris Dyer♠
♠Carnegie Mellon University ♣NLP Group, Pompeu Fabra University
{glample,sandeeps,kkawakam,cdyer}@cs.cmu.edu,

miguel.ballesteros@upf.edu

6
1
0
2

 
r
p
A
6

 

 
 
]
L
C
.
s
c
[
 
 

2
v
0
6
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

State-of-the-art named entity recognition sys-
tems rely heavily on hand-crafted features and
domain-speciﬁc knowledge in order to learn
effectively from the small, supervised training
corpora that are available.
In this paper, we
introduce two new neural architectures—one
based on bidirectional LSTMs and conditional
random ﬁelds, and the other that constructs
and labels segments using a transition-based
approach inspired by shift-reduce parsers.
Our models rely on two sources of infor-
mation about words: character-based word
representations learned from the supervised
corpus and unsupervised word representa-
tions learned from unannotated corpora. Our
models obtain state-of-the-art performance in
NER in four languages without resorting to
any language-speciﬁc knowledge or resources
such as gazetteers. 1

1

Introduction

Named entity recognition (NER) is a challenging
learning problem. One the one hand, in most lan-
guages and domains,
there is only a very small
amount of supervised training data available. On the
other, there are few constraints on the kinds of words
that can be names, so generalizing from this small
sample of data is difﬁcult. As a result, carefully con-
structed orthographic features and language-speciﬁc
knowledge resources, such as gazetteers, are widely
used for solving this task. Unfortunately, language-
speciﬁc resources and features are costly to de-
velop in new languages and new domains, making
NER a challenge to adapt. Unsupervised learning

1The code of the LSTM-CRF and Stack-LSTM NER
systems
https://github.com/
glample/tagger and https://github.com/clab/
stack-lstm-ner

available

are

at

from unannotated corpora offers an alternative strat-
egy for obtaining better generalization from small
amounts of supervision. However, even systems
that have relied extensively on unsupervised fea-
tures (Collobert et al., 2011; Turian et al., 2010;
Lin and Wu, 2009; Ando and Zhang, 2005b, in-
ter alia) have used these to augment, rather than
replace, hand-engineered features (e.g., knowledge
about capitalization patterns and character classes in
a particular language) and specialized knowledge re-
sources (e.g., gazetteers).

In this paper, we present neural architectures
for NER that use no language-speciﬁc resources
or features beyond a small amount of supervised
training data and unlabeled corpora. Our mod-
els are designed to capture two intuitions. First,
since names often consist of multiple tokens, rea-
soning jointly over tagging decisions for each to-
ken is important. We compare two models here,
(i) a bidirectional LSTM with a sequential condi-
tional random layer above it (LSTM-CRF; §2), and
(ii) a new model that constructs and labels chunks
of input sentences using an algorithm inspired by
transition-based parsing with states represented by
stack LSTMs (S-LSTM; §3). Second, token-level
evidence for “being a name” includes both ortho-
graphic evidence (what does the word being tagged
as a name look like?) and distributional evidence
(where does the word being tagged tend to oc-
cur in a corpus?). To capture orthographic sen-
sitivity, we use character-based word representa-
tion model (Ling et al., 2015b) to capture distribu-
tional sensitivity, we combine these representations
with distributional representations (Mikolov et al.,
2013b). Our word representations combine both of
these, and dropout training is used to encourage the
model to learn to trust both sources of evidence (§4).
Experiments in English, Dutch, German, and
Spanish show that we are able to obtain state-

of-the-art NER performance with the LSTM-CRF
model in Dutch, German, and Spanish, and very
near the state-of-the-art
in English without any
hand-engineered features or gazetteers (§5). The
transition-based algorithm likewise surpasses the
best previously published results in several
lan-
guages, although it performs less well than the
LSTM-CRF model.

2 LSTM-CRF Model

We provide a brief description of LSTMs and CRFs,
and present a hybrid tagging architecture. This ar-
chitecture is similar to the ones presented by Col-
lobert et al. (2011) and Huang et al. (2015).

and return another

2.1 LSTM
Recurrent neural networks (RNNs) are a family
of neural networks that operate on sequential
data. They take as input a sequence of vectors
sequence
(x1, x2, . . . , xn)
(h1, h2, . . . , hn) that represents some information
about
the sequence at every step in the input.
Although RNNs can, in theory, learn long depen-
dencies, in practice they fail to do so and tend to
be biased towards their most recent inputs in the
sequence (Bengio et al., 1994). Long Short-term
Memory Networks (LSTMs) have been designed to
combat this issue by incorporating a memory-cell
and have been shown to capture long-range depen-
dencies. They do so using several gates that control
the proportion of the input to give to the memory
cell, and the proportion from the previous state to
forget (Hochreiter and Schmidhuber, 1997). We use
the following implementation:

it = σ(Wxixt + Whiht−1 + Wcict−1 + bi)
ct = (1 − it) (cid:12) ct−1+

it (cid:12) tanh(Wxcxt + Whcht−1 + bc)
ot = σ(Wxoxt + Whoht−1 + Wcoct + bo)
ht = ot (cid:12) tanh(ct),

where σ is the element-wise sigmoid function, and
(cid:12) is the element-wise product.

For a given sentence (x1, x2, . . . , xn) containing
n words, each represented as a d-dimensional vector,
−→
ht of the left
an LSTM computes a representation

context of the sentence at every word t. Naturally,
←−
generating a representation of the right context
ht
as well should add useful information. This can be
achieved using a second LSTM that reads the same
sequence in reverse. We will refer to the former as
the forward LSTM and the latter as the backward
LSTM. These are two distinct networks with differ-
ent parameters. This forward and backward LSTM
pair is referred to as a bidirectional LSTM (Graves
and Schmidhuber, 2005).

The representation of a word using this model is
obtained by concatenating its left and right context
←−
representations, ht = [
ht]. These representa-
tions effectively include a representation of a word
in context, which is useful for numerous tagging ap-
plications.

−→
ht;

2.2 CRF Tagging Models
A very simple—but surprisingly effective—tagging
model is to use the ht’s as features to make indepen-
dent tagging decisions for each output yt (Ling et
al., 2015b). Despite this model’s success in simple
problems like POS tagging, its independent classiﬁ-
cation decisions are limiting when there are strong
dependencies across output labels. NER is one such
task, since the “grammar” that characterizes inter-
pretable sequences of tags imposes several hard con-
straints (e.g., I-PER cannot follow B-LOC; see §2.4
for details) that would be impossible to model with
independence assumptions.

Therefore, instead of modeling tagging decisions
independently, we model them jointly using a con-
ditional random ﬁeld (Lafferty et al., 2001). For an
input sentence

X = (x1, x2, . . . , xn),

we consider P to be the matrix of scores output by
the bidirectional LSTM network. P is of size n × k,
where k is the number of distinct tags, and Pi,j cor-
responds to the score of the jth tag of the ith word
in a sentence. For a sequence of predictions

y = (y1, y2, . . . , yn),

we deﬁne its score to be

s(X, y) =

Ayi,yi+1 +

n(cid:88)

i=0

n(cid:88)

i=1

Pi,yi

where A is a matrix of transition scores such that
Ai,j represents the score of a transition from the
tag i to tag j. y0 and yn are the start and end
tags of a sentence, that we add to the set of possi-
ble tags. A is therefore a square matrix of size k +2.

A softmax over all possible tag sequences yields a
probability for the sequence y:

p(y|X) =

(cid:80)(cid:101)y∈YX

es(X,y)

.

es(X,(cid:101)y)
 (cid:88)
es(X,(cid:101)y)
(cid:101)y∈YX
s(X,(cid:101)y),



(1)

During training, we maximize the log-probability of
the correct tag sequence:

log(p(y|X)) = s(X, y) − log
= s(X, y) − logadd
(cid:101)y∈YX

where YX represents all possible tag sequences
(even those that do not verify the IOB format) for
a sentence X. From the formulation above, it is ev-
ident that we encourage our network to produce a
valid sequence of output labels. While decoding, we
predict the output sequence that obtains the maxi-
mum score given by:

s(X,(cid:101)y).

y∗ = argmax
(cid:101)y∈YX

(2)

Since we are only modeling bigram interactions
between outputs, both the summation in Eq. 1 and
the maximum a posteriori sequence y∗ in Eq. 2 can
be computed using dynamic programming.

2.3 Parameterization and Training
The scores associated with each tagging decision
for each token (i.e., the Pi,y’s) are deﬁned to be
the dot product between the embedding of a word-
in-context computed with a bidirectional LSTM—
exactly the same as the POS tagging model of Ling
et al. (2015b) and these are combined with bigram
compatibility scores (i.e., the Ay,y(cid:48)’s). This archi-
tecture is shown in ﬁgure 1. Circles represent ob-
served variables, diamonds are deterministic func-
tions of their parents, and double circles are random
variables.

Figure 1: Main architecture of the network. Word embeddings
are given to a bidirectional LSTM. li represents the word i and
its left context, ri represents the word i and its right context.
Concatenating these two vectors yields a representation of the
word i in its context, ci.

The parameters of this model are thus the matrix
of bigram compatibility scores A, and the parame-
ters that give rise to the matrix P, namely the param-
eters of the bidirectional LSTM, the linear feature
weights, and the word embeddings. As in part 2.2,
let xi denote the sequence of word embeddings for
every word in a sentence, and yi be their associated
tags. We return to a discussion of how the embed-
dings xi are modeled in Section 4. The sequence of
word embeddings is given as input to a bidirectional
LSTM, which returns a representation of the left and
right context for each word as explained in 2.1.

These representations are concatenated (ci) and
linearly projected onto a layer whose size is equal
to the number of distinct tags. Instead of using the
softmax output from this layer, we use a CRF as pre-
viously described to take into account neighboring
tags, yielding the ﬁnal predictions for every word
yi. Additionally, we observed that adding a hidden
layer between ci and the CRF layer marginally im-
proved our results. All results reported with this
model incorporate this extra-layer. The parameters
are trained to maximize Eq. 1 of observed sequences
of NER tags in an annotated corpus, given the ob-
served words.

2.4 Tagging Schemes
The task of named entity recognition is to assign a
named entity label to every word in a sentence. A
single named entity could span several tokens within
a sentence. Sentences are usually represented in the
IOB format (Inside, Outside, Beginning) where ev-
ery token is labeled as B-label if the token is the
beginning of a named entity, I-label if it is inside
a named entity but not the ﬁrst token within the
named entity, or O otherwise. However, we de-
cided to use the IOBES tagging scheme, a variant of
IOB commonly used for named entity recognition,
which encodes information about singleton entities
(S) and explicitly marks the end of named entities
(E). Using this scheme, tagging a word as I-label
with high-conﬁdence narrows down the choices for
the subsequent word to I-label or E-label, however,
the IOB scheme is only capable of determining that
the subsequent word cannot be the interior of an-
other label. Ratinov and Roth (2009) and Dai et al.
(2015) showed that using a more expressive tagging
scheme like IOBES improves model performance
marginally. However, we did not observe a signif-
icant improvement over the IOB tagging scheme.

3 Transition-Based Chunking Model

As an alternative to the LSTM-CRF discussed in
the previous section, we explore a new architecture
that chunks and labels a sequence of inputs using
an algorithm similar to transition-based dependency
parsing. This model directly constructs representa-
tions of the multi-token names (e.g., the name Mark
Watney is composed into a single representation).

This model relies on a stack data structure to in-
crementally construct chunks of the input. To ob-
tain representations of this stack used for predict-
ing subsequent actions, we use the Stack-LSTM pre-
sented by Dyer et al. (2015), in which the LSTM
is augmented with a “stack pointer.” While sequen-
tial LSTMs model sequences from left to right, stack
LSTMs permit embedding of a stack of objects that
are both added to (using a push operation) and re-
moved from (using a pop operation). This allows
the Stack-LSTM to work like a stack that maintains
a “summary embedding” of its contents. We refer
to this model as Stack-LSTM or S-LSTM model for
simplicity.

Finally, we refer interested readers to the original
paper (Dyer et al., 2015) for details about the Stack-
LSTM model since in this paper we merely use the
same architecture through a new transition-based al-
gorithm presented in the following Section.

3.1 Chunking Algorithm
We designed a transition inventory which is given in
Figure 2 that is inspired by transition-based parsers,
in particular the arc-standard parser of Nivre (2004).
In this algorithm, we make use of two stacks (des-
ignated output and stack representing, respectively,
completed chunks and scratch space) and a buffer
that contains the words that have yet to be processed.
The transition inventory contains the following tran-
sitions: The SHIFT transition moves a word from
the buffer to the stack, the OUT transition moves a
word from the buffer directly into the output stack
while the REDUCE(y) transition pops all items from
the top of the stack creating a “chunk,” labels this
with label y, and pushes a representation of this
chunk onto the output stack. The algorithm com-
pletes when the stack and buffer are both empty. The
algorithm is depicted in Figure 2, which shows the
sequence of operations required to process the sen-
tence Mark Watney visited Mars.

The model is parameterized by deﬁning a prob-
ability distribution over actions at each time step,
given the current contents of the stack, buffer, and
output, as well as the history of actions taken. Fol-
lowing Dyer et al. (2015), we use stack LSTMs
to compute a ﬁxed dimensional embedding of each
of these, and take a concatenation of these to ob-
tain the full algorithm state. This representation is
used to deﬁne a distribution over the possible ac-
tions that can be taken at each time step. The model
is trained to maximize the conditional probability of
sequences of reference actions (extracted from a la-
beled training corpus) given the input sentences. To
label a new input sequence at test time, the maxi-
mum probability action is chosen greedily until the
algorithm reaches a termination state. Although this
is not guaranteed to ﬁnd a global optimum, it is ef-
fective in practice. Since each token is either moved
directly to the output (1 action) or ﬁrst to the stack
and then the output (2 actions), the total number of
actions for a sequence of length n is maximally 2n.
It is worth noting that the nature of this algorithm

Outt
O
O
O

Stackt
S
(u, u), . . . , (v, v), S B
S

Action
Buffert
(u, u), B SHIFT

REDUCE(y)

(u, u), B OUT

Outt+1
O
g(u, . . . , v, ry), O S
g(u, r∅), O
S

Stackt+1 Buffert+1
(u, u), S B
B
B

Segments

—

(u . . . v, y)

—

Figure 2: Transitions of the Stack-LSTM model indicating the action applied and the resulting state. Bold symbols indicate
(learned) embeddings of words and relations, script symbols indicate the corresponding words and relations.

Transition

SHIFT
SHIFT
REDUCE(PER)
OUT
SHIFT
REDUCE(LOC)

Output
[]
[]
[]
[(Mark Watney)-PER]
[(Mark Watney)-PER, visited]
[(Mark Watney)-PER, visited]
[(Mark Watney)-PER, visited, (Mars)-LOC]

Stack
[]
[Mark]
[Mark, Watney]
[]
[]
[Mars]
[]

Buffer
[Mark, Watney, visited, Mars]
[Watney, visited, Mars]
[visited, Mars]
[visited, Mars]
[Mars]
[]
[]

Segment

(Mark Watney)-PER

(Mars)-LOC

Figure 3: Transition sequence for Mark Watney visited Mars with the Stack-LSTM model.

model makes it agnostic to the tagging scheme used
since it directly predicts labeled chunks.

3.2 Representing Labeled Chunks
When the REDUCE(y) operation is executed, the al-
gorithm shifts a sequence of tokens (together with
their vector embeddings) from the stack to the out-
put buffer as a single completed chunk. To compute
an embedding of this sequence, we run a bidirec-
tional LSTM over the embeddings of its constituent
tokens together with a token representing the type of
the chunk being identiﬁed (i.e., y). This function is
given as g(u, . . . , v, ry), where ry is a learned em-
bedding of a label type. Thus, the output buffer con-
tains a single vector representation for each labeled
chunk that is generated, regardless of its length.

Input Word Embeddings

4
The input layers to both of our models are vector
representations of individual words. Learning inde-
pendent representations for word types from the lim-
ited NER training data is a difﬁcult problem: there
are simply too many parameters to reliably estimate.
Since many languages have orthographic or mor-
phological evidence that something is a name (or
not a name), we want representations that are sen-
sitive to the spelling of words. We therefore use a
model that constructs representations of words from
representations of the characters they are composed
of (4.1). Our second intuition is that names, which
may individually be quite varied, appear in regular
contexts in large corpora. Therefore we use embed-

Figure 4: The character embeddings of the word “Mars” are
given to a bidirectional LSTMs. We concatenate their last out-
puts to an embedding from a lookup table to obtain a represen-
tation for this word.

dings learned from a large corpus that are sensitive
to word order (4.2). Finally, to prevent the models
from depending on one representation or the other
too strongly, we use dropout training and ﬁnd this is
crucial for good generalization performance (4.3).

4.1 Character-based models of words
An important distinction of our work from most
previous approaches is that we learn character-level

features while training instead of hand-engineering
preﬁx and sufﬁx information about words. Learn-
ing character-level embeddings has the advantage of
learning representations speciﬁc to the task and do-
main at hand. They have been found useful for mor-
phologically rich languages and to handle the out-
of-vocabulary problem for tasks like part-of-speech
tagging and language modeling (Ling et al., 2015b)
or dependency parsing (Ballesteros et al., 2015).

Figure 4 describes our architecture to generate a
word embedding for a word from its characters. A
character lookup table initialized at random contains
an embedding for every character. The character
embeddings corresponding to every character in a
word are given in direct and reverse order to a for-
ward and a backward LSTM. The embedding for a
word derived from its characters is the concatenation
of its forward and backward representations from
the bidirectional LSTM. This character-level repre-
sentation is then concatenated with a word-level rep-
resentation from a word lookup-table. During test-
ing, words that do not have an embedding in the
lookup table are mapped to a UNK embedding. To
train the UNK embedding, we replace singletons
with the UNK embedding with a probability 0.5. In
all our experiments, the hidden dimension of the for-
ward and backward character LSTMs are 25 each,
which results in our character-based representation
of words being of dimension 50.

Recurrent models like RNNs and LSTMs are ca-
pable of encoding very long sequences, however,
they have a representation biased towards their most
recent inputs. As a result, we expect the ﬁnal rep-
resentation of the forward LSTM to be an accurate
representation of the sufﬁx of the word, and the ﬁ-
nal state of the backward LSTM to be a better rep-
resentation of its preﬁx. Alternative approaches—
most notably like convolutional networks—have
been proposed to learn representations of words
from their characters (Zhang et al., 2015; Kim et al.,
2015). However, convnets are designed to discover
position-invariant features of their inputs. While this
is appropriate for many problems, e.g., image recog-
nition (a cat can appear anywhere in a picture), we
argue that important information is position depen-
dent (e.g., preﬁxes and sufﬁxes encode different in-
formation than stems), making LSTMs an a priori
better function class for modeling the relationship

between words and their characters.

4.2 Pretrained embeddings
As in Collobert et al. (2011), we use pretrained
word embeddings to initialize our lookup table. We
observe signiﬁcant improvements using pretrained
word embeddings over randomly initialized ones.
Embeddings are pretrained using skip-n-gram (Ling
et al., 2015a), a variation of word2vec (Mikolov et
al., 2013a) that accounts for word order. These em-
beddings are ﬁne-tuned during training.

Word embeddings for Spanish, Dutch, German
and English are trained using the Spanish Gigaword
version 3, the Leipzig corpora collection, the Ger-
man monolingual training data from the 2010 Ma-
chine Translation Workshop and the English Giga-
word version 4 (with the LA Times and NY Times
portions removed) respectively.2 We use an embed-
ding dimension of 100 for English, 64 for other lan-
guages, a minimum word frequency cutoff of 4, and
a window size of 8.

4.3 Dropout training
Initial experiments showed that character-level em-
beddings did not improve our overall performance
when used in conjunction with pretrained word rep-
resentations. To encourage the model to depend on
both representations, we use dropout training (Hin-
ton et al., 2012), applying a dropout mask to the ﬁnal
embedding layer just before the input to the bidirec-
tional LSTM in Figure 1. We observe a signiﬁcant
improvement in our model’s performance after us-
ing dropout (see table 5).

5 Experiments

This section presents the methods we use to train our
models, the results we obtained on various tasks and
the impact of our networks’ conﬁguration on model
performance.

5.1 Training
For both models presented, we train our networks
using the back-propagation algorithm updating our
parameters on every training example, one at a
time, using stochastic gradient descent (SGD) with

2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al.,

2010; Parker et al., 2009)

a learning rate of 0.01 and a gradient clipping of
5.0. Several methods have been proposed to enhance
the performance of SGD, such as Adadelta (Zeiler,
2012) or Adam (Kingma and Ba, 2014). Although
we observe faster convergence using these methods,
none of them perform as well as SGD with gradient
clipping.

Our LSTM-CRF model uses a single layer for
the forward and backward LSTMs whose dimen-
sions are set to 100. Tuning this dimension did
not signiﬁcantly impact model performance. We set
the dropout rate to 0.5. Using higher rates nega-
tively impacted our results, while smaller rates led
to longer training time.

The stack-LSTM model uses two layers each of
dimension 100 for each stack. The embeddings of
the actions used in the composition functions have
16 dimensions each, and the output embedding is
of dimension 20. We experimented with different
dropout rates and reported the scores using the best
dropout rate for each language.3 It is a greedy model
that apply locally optimal actions until the entire
sentence is processed, further improvements might
be obtained with beam search (?) or training with
exploration (?).

5.2 Data Sets

We test our model on different datasets for named
entity recognition. To demonstrate our model’s
ability to generalize to different
languages, we
present results on the CoNLL-2002 and CoNLL-
2003 datasets (Tjong Kim Sang, 2002; Tjong
Kim Sang and De Meulder, 2003) that contain in-
dependent named entity labels for English, Span-
ish, German and Dutch. All datasets contain four
different types of named entities:
locations, per-
sons, organizations, and miscellaneous entities that
do not belong in any of the three previous cate-
gories. Although POS tags were made available for
all datasets, we did not include them in our models.
We did not perform any dataset preprocessing, apart
from replacing every digit with a zero in the English
NER dataset.

3English (D=0.2), German, Spanish and Dutch (D=0.3)

5.3 Results
Table 1 presents our comparisons with other mod-
els for named entity recognition in English. To
make the comparison between our model and oth-
ers fair, we report the scores of other models with
and without the use of external labeled data such
as gazetteers and knowledge bases. Our models do
not use gazetteers or any external labeled resources.
The best score reported on this task is by Luo et al.
(2015). They obtained a F1 of 91.2 by jointly model-
ing the NER and entity linking tasks (Hoffart et al.,
2011). Their model uses a lot of hand-engineered
features including spelling features, WordNet clus-
ters, Brown clusters, POS tags, chunks tags, as
well as stemming and external knowledge bases like
Freebase and Wikipedia. Our LSTM-CRF model
outperforms all other systems, including the ones us-
ing external labeled data like gazetteers. Our Stack-
LSTM model also outperforms all previous models
that do not incorporate external features, apart from
the one presented by Chiu and Nichols (2015).

Tables 2, 3 and 4 present our results on NER for
German, Dutch and Spanish respectively in compar-
ison to other models. On these three languages, the
LSTM-CRF model signiﬁcantly outperforms all pre-
vious methods, including the ones using external la-
beled data. The only exception is Dutch, where the
model of Gillick et al. (2015) can perform better by
leveraging the information from other NER datasets.
The Stack-LSTM also consistently presents state-
the-art (or close to) results compared to systems that
do not use external data.

As we can see in the tables, the Stack-LSTM
model is more dependent on character-based repre-
sentations to achieve competitive performance; we
hypothesize that the LSTM-CRF model requires less
orthographic information since it gets more contex-
tual information out of the bidirectional LSTMs;
however,
the Stack-LSTM model consumes the
words one by one and it just relies on the word rep-
resentations when it chunks words.

5.4 Network architectures
Our models had several components that we could
tweak to understand their impact on the overall per-
formance. We explored the impact that the CRF, the
character-level representations, pretraining of our

Model
Collobert et al. (2011)*
Lin and Wu (2009)
Lin and Wu (2009)*
Huang et al. (2015)*
Passos et al. (2014)
Passos et al. (2014)*
Luo et al. (2015)* + gaz
Luo et al. (2015)* + gaz + linking
Chiu and Nichols (2015)
Chiu and Nichols (2015)*
LSTM-CRF (no char)
LSTM-CRF
S-LSTM (no char)
S-LSTM

F1
89.59
83.78
90.90
90.10
90.05
90.90
89.9
91.2
90.69
90.77
90.20
90.94
87.96
90.33

Table 1: English NER results (CoNLL-2003 test set). * indi-
cates models trained with the use of external labeled data

Model
Florian et al. (2003)*
Ando and Zhang (2005a)
Qi et al. (2009)
Gillick et al. (2015)
Gillick et al. (2015)*
LSTM-CRF – no char
LSTM-CRF
S-LSTM – no char
S-LSTM

F1
72.41
75.27
75.72
72.08
76.22
75.06
78.76
65.87
75.66

Table 2: German NER results (CoNLL-2003 test set). * indi-
cates models trained with the use of external labeled data

Model
Carreras et al. (2002)
Nothman et al. (2013)
Gillick et al. (2015)
Gillick et al. (2015)*
LSTM-CRF – no char
LSTM-CRF
S-LSTM – no char
S-LSTM

F1
77.05
78.6
78.08
82.84
73.14
81.74
69.90
79.88

Table 3: Dutch NER (CoNLL-2002 test set). * indicates mod-
els trained with the use of external labeled data

Model
Carreras et al. (2002)*
Santos and Guimar˜aes (2015)
Gillick et al. (2015)
Gillick et al. (2015)*
LSTM-CRF – no char
LSTM-CRF
S-LSTM – no char
S-LSTM

F1
81.39
82.21
81.83
82.95
83.44
85.75
79.46
83.93

Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
els trained with the use of external labeled data

word embeddings and dropout had on our LSTM-
CRF model. We observed that pretraining our word
embeddings gave us the biggest improvement in
overall performance of +7.31 in F1. The CRF layer
gave us an increase of +1.79, while using dropout
resulted in a difference of +1.17 and ﬁnally learn-

ing character-level word embeddings resulted in an
increase of about +0.74. For the Stack-LSTM we
performed a similar set of experiments. Results with
different architectures are given in table 5.

Model
LSTM
LSTM-CRF
LSTM-CRF
LSTM-CRF
LSTM-CRF
LSTM-CRF
S-LSTM
S-LSTM
S-LSTM
S-LSTM
S-LSTM

Variant
char + dropout + pretrain
char + dropout
pretrain
pretrain + char
pretrain + dropout
pretrain + dropout + char
char + dropout
pretrain
pretrain + char
pretrain + dropout
pretrain + dropout + char

F1
89.15
83.63
88.39
89.77
90.20
90.94
80.88
86.67
89.32
87.96
90.33

Table 5: English NER results with our models, using differ-
ent conﬁgurations. “pretrain” refers to models that include pre-
trained word embeddings, “char” refers to models that include
character-based modeling of words, “dropout” refers to models
that include dropout rate.

6 Related Work

In the CoNLL-2002 shared task, Carreras et al.
(2002) obtained among the best results on both
Dutch and Spanish by combining several small
ﬁxed-depth decision trees. Next year, in the CoNLL-
2003 Shared Task, Florian et al. (2003) obtained the
best score on German by combining the output of
four diverse classiﬁers. Qi et al. (2009) later im-
proved on this with a neural network by doing unsu-
pervised learning on a massive unlabeled corpus.

Several other neural architectures have previously
been proposed for NER. For instance, Collobert et
al. (2011) uses a CNN over a sequence of word em-
beddings with a CRF layer on top. This can be
thought of as our ﬁrst model without character-level
embeddings and with the bidirectional LSTM be-
ing replaced by a CNN. More recently, Huang et al.
(2015) presented a model similar to our LSTM-CRF,
but using hand-crafted spelling features. ?) also
used a similar model and adapted it to the semantic
role labeling task. Lin and Wu (2009) used a linear
chain CRF with L2 regularization, they added phrase
cluster features extracted from the web data and
spelling features. Passos et al. (2014) also used a lin-
ear chain CRF with spelling features and gazetteers.
Language independent NER models like ours
have also been proposed in the past. Cucerzan

and Yarowsky (1999; 2002) present semi-supervised
bootstrapping algorithms for named entity recogni-
tion by co-training character-level (word-internal)
and token-level (context) features. Eisenstein et
al. (2011) use Bayesian nonparametrics to construct
a database of named entities in an almost unsu-
pervised setting. Ratinov and Roth (2009) quanti-
tatively compare several approaches for NER and
build their own supervised model using a regular-
ized average perceptron and aggregating context in-
formation.

Finally, there is currently a lot of interest in mod-
els for NER that use letter-based representations.
Gillick et al. (2015) model the task of sequence-
labeling as a sequence to sequence learning prob-
lem and incorporate character-based representations
into their encoder model. Chiu and Nichols (2015)
employ an architecture similar to ours, but instead
use CNNs to learn character-level features, in a way
similar to the work by Santos and Guimar˜aes (2015).

7 Conclusion

This paper presents two neural architectures for se-
quence labeling that provide the best NER results
ever reported in standard evaluation settings, even
compared with models that use external resources,
such as gazetteers.

A key aspect of our models are that they model
output label dependencies, either via a simple CRF
architecture, or using a transition-based algorithm
to explicitly construct and label chunks of the in-
put. Word representations are also crucially impor-
tant for success: we use both pre-trained word rep-
resentations and “character-based” representations
that capture morphological and orthographic infor-
mation. To prevent the learner from depending too
heavily on one representation class, dropout is used.

Acknowledgments

This work was sponsored in part by the Defense
Advanced Research Projects Agency (DARPA)
Information Innovation Ofﬁce (I2O) under
the
Low Resource Languages for Emergent Incidents
(LORELEI) program issued by DARPA/I2O under
Contract No. HR0011-15-C-0114. Miguel Balles-
teros is supported by the European Commission un-
der the contract numbers FP7-ICT-610411 (project

MULTISENSOR) and H2020-RIA-645012 (project
KRISTINA).

References
[Ando and Zhang2005a] Rie Kubota Ando and Tong
Zhang. 2005a. A framework for learning predictive
structures from multiple tasks and unlabeled data. The
Journal of Machine Learning Research, 6:1817–1853.
[Ando and Zhang2005b] Rie Kubota Ando and Tong
Zhang. 2005b. Learning predictive structures. JMLR,
6:1817–1853.

[Ballesteros et al.2015] Miguel Ballesteros, Chris Dyer,
and Noah A. Smith. 2015. Improved transition-based
dependency parsing by modeling characters instead of
words with LSTMs. In Proceedings of EMNLP.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and
Paolo Frasconi. 1994. Learning long-term depen-
dencies with gradient descent is difﬁcult. Neural Net-
works, IEEE Transactions on, 5(2):157–166.

[Biemann et al.2007] Chris Biemann, Gerhard Heyer,
Uwe Quasthoff, and Matthias Richter. 2007. The
leipzig corpora collection-monolingual corpora of
standard size. Proceedings of Corpus Linguistic.

[Callison-Burch et al.2010] Chris

Callison-Burch,
Philipp Koehn, Christof Monz, Kay Peterson, Mark
Przybocki, and Omar F Zaidan.
Findings
of the 2010 joint workshop on statistical machine
translation and metrics for machine translation.
In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 17–53.
Association for Computational Linguistics.

2010.

[Carreras et al.2002] Xavier Carreras, Llu´ıs M`arquez, and
Llu´ıs Padr´o. 2002. Named entity extraction using ad-
aboost, proceedings of the 6th conference on natural
language learning. August, 31:1–4.

[Chiu and Nichols2015] Jason PC Chiu and Eric Nichols.
2015. Named entity recognition with bidirectional
lstm-cnns. arXiv preprint arXiv:1511.08308.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language process-
ing (almost) from scratch. The Journal of Machine
Learning Research, 12:2493–2537.
[Cucerzan and Yarowsky1999] Silviu

and
David Yarowsky.
Language independent
named entity recognition combining morphological
and contextual evidence. In Proceedings of the 1999
Joint SIGDAT Conference on EMNLP and VLC, pages
90–99.

Cucerzan

1999.

[Cucerzan and Yarowsky2002] Silviu

and
David Yarowsky. 2002. Language independent ner
using a uniﬁed model of internal and contextual

Cucerzan

evidence.
Natural
Association for Computational Linguistics.

In proceedings of the 6th conference on
language learning-Volume 20, pages 1–4.

[Dai et al.2015] Hong-Jie Dai, Po-Ting Lai, Yung-Chun
Chang, and Richard Tzong-Han Tsai. 2015. Enhanc-
ing of chemical compound and drug name recogni-
tion using representative tag scheme and ﬁne-grained
Journal of cheminformatics, 7(Suppl
tokenization.
1):S14.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
short-term memory. In Proc. ACL.

[Eisenstein et al.2011] Jacob Eisenstein,

Tae Yano,
William W Cohen, Noah A Smith, and Eric P Xing.
2011. Structured databases of named entities from
bayesian nonparametrics. In Proceedings of the First
Workshop on Unsupervised Learning in NLP, pages
2–12. Association for Computational Linguistics.

[Florian et al.2003] Radu Florian, Abe

Ittycheriah,
2003. Named
Hongyan Jing, and Tong Zhang.
entity recognition through classiﬁer combination.
In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume
4, pages 168–171. Association for Computational
Linguistics.

[Gillick et al.2015] Dan Gillick, Cliff Brunk, Oriol
Vinyals, and Amarnag Subramanya. 2015. Multilin-
gual language processing from bytes. arXiv preprint
arXiv:1512.00103.

[Graff2011] David Graff. 2011. Spanish gigaword third
edition (ldc2011t12). Linguistic Data Consortium,
Univer-sity of Pennsylvania, Philadelphia, PA.

[Graves and Schmidhuber2005] Alex Graves and J¨urgen
Schmidhuber. 2005. Framewise phoneme classiﬁ-
In Proc.
cation with bidirectional LSTM networks.
IJCNN.

[Hinton et al.2012] Geoffrey E Hinton, Nitish Srivas-
tava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R
Salakhutdinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Hoffart et al.2011] Johannes Hoffart, Mohamed Amir
Ilaria Bordino, Hagen F¨urstenau, Manfred
Yosef,
Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater,
and Gerhard Weikum. 2011. Robust disambiguation
of named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782–792. Association for Compu-
tational Linguistics.

[Huang et al.2015] Zhiheng Huang, Wei Xu, and Kai Yu.
2015. Bidirectional LSTM-CRF models for sequence
tagging. CoRR, abs/1508.01991.

[Kim et al.2015] Yoon Kim, Yacine Jernite, David Son-
tag, and Alexander M. Rush. 2015. Character-aware
neural language models. CoRR, abs/1508.06615.

[Kingma and Ba2014] Diederik Kingma and Jimmy Ba.
2014. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando CN Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and label-
ing sequence data. In Proc. ICML.

[Lin and Wu2009] Dekang Lin and Xiaoyun Wu. 2009.
Phrase clustering for discriminative learning. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 1030–1038. As-
sociation for Computational Linguistics.

[Ling et al.2015a] Wang Ling, Lin Chu-Cheng, Yulia
Tsvetkov, Silvio Amir, R´amon Fernandez Astudillo,
Chris Dyer, Alan W Black, and Isabel Trancoso.
2015a. Not all contexts are created equal: Better
word representations with variable attention. In Proc.
EMNLP.

[Ling et al.2015b] Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo,
Ram´on Fernandez Astudillo, Silvio Amir, Chris Dyer,
Alan W Black, and Isabel Trancoso. 2015b. Finding
function in form: Compositional character models for
open vocabulary word representation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).

[Luo et al.2015] Gang Luo, Xiaojiang Huang, Chin-Yew
Lin, and Zaiqing Nie. 2015. Joint named entity recog-
nition and disambiguation. In Proc. EMNLP.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
their compositionality. In Proc. NIPS.

[Nivre2004] Joakim Nivre. 2004. Incrementality in de-
In Proceedings of
terministic dependency parsing.
the Workshop on Incremental Parsing: Bringing En-
gineering and Cognition Together.

[Nothman et al.2013] Joel Nothman, Nicky Ringland,
Will Radford, Tara Murphy, and James R Curran.
2013. Learning multilingual named entity recognition
from wikipedia. Artiﬁcial Intelligence, 194:151–175.
[Parker et al.2009] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2009. English

gigaword fourth edition (ldc2009t13). Linguistic Data
Consortium, Univer-sity of Pennsylvania, Philadel-
phia, PA.

[Passos et al.2014] Alexandre Passos, Vineet Kumar, and
Andrew McCallum. 2014. Lexicon infused phrase
arXiv
embeddings for named entity resolution.
preprint arXiv:1404.5367.

[Qi et al.2009] Yanjun Qi, Ronan Collobert, Pavel Kuksa,
Koray Kavukcuoglu, and Jason Weston. 2009. Com-
bining labeled and unlabeled data with word-class dis-
In Proceedings of the 18th ACM
tribution learning.
conference on Information and knowledge manage-
ment, pages 1737–1740. ACM.

[Ratinov and Roth2009] Lev Ratinov and Dan Roth.
2009.
Design challenges and misconceptions in
named entity recognition. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 147–155. Association for
Computational Linguistics.

[Santos and Guimar˜aes2015] Cicero Nogueira dos Santos
and Victor Guimar˜aes. 2015. Boosting named entity
recognition with neural character embeddings. arXiv
preprint arXiv:1505.05008.

[Tjong Kim Sang and De Meulder2003] Erik F. Tjong
Kim Sang and Fien De Meulder. 2003. Introduction
to the conll-2003 shared task: Language-independent
named entity recognition. In Proc. CoNLL.

[Tjong Kim Sang2002] Erik F. Tjong Kim Sang. 2002.
Introduction to the conll-2002 shared task: Language-
In Proc.
independent named entity recognition.
CoNLL.

[Turian et al.2010] Joseph Turian, Lev Ratinov,

and
Yoshua Bengio. 2010. Word representations: A sim-
ple and general method for semi-supervised learning.
In Proc. ACL.

[Zeiler2012] Matthew D Zeiler.

An adaptive learning rate method.
arXiv:1212.5701.

2012.

Adadelta:
arXiv preprint

[Zhang et al.2015] Xiang Zhang, Junbo Zhao, and Yann
LeCun. 2015. Character-level convolutional networks
for text classiﬁcation. In Advances in Neural Informa-
tion Processing Systems, pages 649–657.

