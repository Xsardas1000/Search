6
1
0
2

 
r
a

 

M
0
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
4
7
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Submitted to the Annals of Statistics
arXiv: arXiv:0000.0000

ACCURACY ASSESSMENT FOR HIGH-DIMENSIONAL

LINEAR REGRESSIONâˆ—

By T. Tony Cai, and Zijian Guo

University of Pennsylvania

This paper considers point and interval estimation of the (cid:96)q loss
of an estimator in high-dimensional linear regression with random
design. Both the setting of known identity design covariance matrix
and known noise level and the setting of unknown design covariance
matrix and noise level are studied. We establish the minimax conver-
gence rate for estimating the (cid:96)q loss and the minimax expected length
of conï¬dence intervals for the (cid:96)q loss of a broad collection of estima-
tors of the regression vector. We also investigate the adaptivity of the
conï¬dence intervals for the (cid:96)q loss. The results reveal interesting and
signiï¬cant diï¬€erences between estimating the (cid:96)2 loss and (cid:96)q loss with
1 â‰¤ q < 2 as well as the diï¬€erences between the two settings.

A major step in our analysis is to establish rate sharp lower bounds
for the minimax estimation error and the expected length of minimax
and adaptive conï¬dence intervals for the (cid:96)q loss, which requires the
development of new technical tools. A signiï¬cant diï¬€erence between
loss estimation and the traditional parameter estimation is that for
loss estimation the constraint is on the performance of the estimator
of the regression vector, but the lower bounds are on the diï¬ƒculty
of estimating its (cid:96)q loss. The technical tools developed in this paper
can also be of independent interest.

1. Introduction.

In many applications, the goal of statistical inference
is not only to construct a good estimator, but also to provide a measure of
accuracy for this estimator. In classical statistics, when the parameter of
interest is one-dimensional, this is achieved in the form of a standard er-
ror or a conï¬dence interval. A prototypical example is the inference for a
binomial proportion, where often not only an estimate of the proportion
but also its margin of error are given. Accuracy measures of an estimation
procedure have also been used as a tool for the empirical selection of tun-
ing parameters. A well known example is Steinâ€™s Unbiased Risk Estimate
(SURE), which has been an eï¬€ective tool for the construction of data-driven
âˆ—The research was supported in part by NSF Grants DMS-1208982 and DMS-1403708,

and NIH Grant R01 CA127334.

MSC 2010 subject classiï¬cations: Primary 62G15; secondary 62C20, 62H35
Keywords and phrases: Accuracy assessment, adaptivity, conï¬dence interval, high-
dimensional linear regression, loss estimation, minimax lower bound, minimaxity, rate
of convergence, sparsity.

1

2

T. T. CAI AND Z. GUO

adaptive estimators in normal means estimation, nonparametric signal re-
covery, covariance matrix estimation, and other problems. See, for instance,
[15, 12, 9, 7, 21]. The commonly used cross-validation methods for choosing
tuning parameters can also be viewed as a useful tool based on the idea of
empirical assessment of accuracy.

In this paper, we consider the problem of estimating the loss of a given
estimator in the setting of high-dimensional linear regression, where one
observes (X, y) with X âˆˆ RnÃ—p and y âˆˆ Rn, and for 1 â‰¤ i â‰¤ n,

yi = XiÂ·Î² + i.

Here Î² âˆˆ Rp is the regression vector, XiÂ· iidâˆ¼ Np(0, Î£) are the rows of X,
iidâˆ¼ N (0, Ïƒ2) are independent of X. This high dimensional
and the errors i
linear model has been well studied in the literature, with the main focus
on estimation of Î². Several penalized/constrained (cid:96)1 minimization methods,
including Lasso [18], Dantzig selector [8], scaled Lasso [16] and square-root
Lasso [2], have been proposed. These methods have been shown to work well
in applications and produce interpretable estimates of Î² when Î² is assumed
to be sparse. Theoretically, with a properly chosen tuning parameter, these
For a given estimator (cid:98)Î², the (cid:96)q loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
estimators achieve the optimal rate of convergence over collections of sparse
parameter spaces. See, for example, [8, 16, 2, 14, 3, 4, 19].
used as a metric of accuracy for (cid:98)Î². We consider in the present paper both
q with 1 â‰¤ q â‰¤ 2 is commonly
point and interval estimation of the (cid:96)q loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
q for a given (cid:98)Î². Note
that the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
mator (cid:98)Î² and the parameter Î². Usually, prediction and prediction interval
loss (cid:107)(cid:98)Î²âˆ’ Î²(cid:107)2
q. Since the (cid:96)q loss depends on the estimator (cid:98)Î², it is necessary to
paper, we restrict our attention to a broad collection of estimators (cid:98)Î² that

are used for point and interval estimation of a random quantity. However,
we slightly abuse the terminologies in the present paper by using estimation
and conï¬dence interval to represent the point and interval estimator of the

q is a random quantity, depending on both the esti-

specify the estimator in the discussion of loss estimation. Throughout this

perform well at least at one interior point or a small subset of the parame-
ter space. This collection of estimators includes most state-of-art estimators
such as Lasso, Dantzig selector, scaled Lasso and square-root Lasso.

High-dimensional linear regression has been well studied in two settings.
One is the setting with known design covariance matrix Î£ = I and known
noise level Ïƒ = Ïƒ0 and sparse Î². See for example, [10, 1, 13, 19, 17, 11, 6].
Another commonly considered setting is sparse Î² with unknown Î£ and Ïƒ. In
q in

this paper, we consider point and interval estimation of the (cid:96)q loss (cid:107)(cid:98)Î²âˆ’Î²(cid:107)2

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

3

both settings. Speciï¬cally, we consider the parameter space Î˜0(k) introduced
in (2.3) consisting of k-sparse signal Î² with known design covariance matrix
Î£ = I and known noise level Ïƒ = Ïƒ0, and the parameter space Î˜(k) deï¬ned
in (2.4) which consists of k-sparse signals with unknown Î£ and Ïƒ.

minimax and adaptive estimation of the loss (cid:107)(cid:98)Î²âˆ’Î²(cid:107)2

q for a given estimator (cid:98)Î²

1.1. Our contributions. The goals of the present paper are to study the

and the minimax expected length and the adaptivity of conï¬dence intervals
for the loss. A major step in our analysis is to establish rate sharp lower
bounds for the minimax estimation error and minimax expected length of
conï¬dence intervals for the (cid:96)q loss over Î˜0(k) and Î˜(k) for a broad collec-
tion of estimators of Î². We then take the Lasso and scaled Lasso estimators
as speciï¬c examples and propose procedures for point estimation as well as
conï¬dence intervals for their (cid:96)q losses. It is shown that the proposed pro-
cedures achieve their corresponding lower bounds up to a constant factor.
These results together establish the minimax rates of estimating the (cid:96)q loss
over Î˜0(k) and Î˜(k). The analysis shows interesting and signiï¬cant diï¬€er-
ences between estimating the (cid:96)2 loss and (cid:96)q loss with 1 â‰¤ q < 2 as well as
the diï¬€erences between the two parameter spaces Î˜(k) and Î˜0(k).

â€¢ The minimax rates of estimating the (cid:96)2 loss, (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

and over Î˜(k) is k log p

2, over Î˜0(k) is
n . So loss estimation is much
âˆš
log p (cid:28) k (cid:46)
â€¢ The minimax rate of estimating the (cid:96)q loss with 1 â‰¤ q < 2 over both

easier with the prior information Î£ = I and Ïƒ = Ïƒ0 when
n
log p .

(cid:110) 1âˆš

n , k log p

n

(cid:111)

min

n

Î˜0(k) and Î˜(k) is k

2
q log p
n .
âˆš
log p (cid:28) k (cid:46) n

n

In the regime

log p , a practical loss estimator is proposed for
estimating the (cid:96)2 loss and shown to achieve the optimal convergence rate 1âˆš
n
adaptively over Î˜0(k). We say estimation of loss is impossible if the minimax
rate can be achieved by a trivial estimator, say, 0. In all other considered
cases, estimation of loss is shown to be impossible. These results indicate
that loss estimation is diï¬ƒcult.

We then turn to construction of conï¬dence intervals for the (cid:96)q loss. A
conï¬dence interval for the loss is useful even when it is â€œimpossibleâ€ to es-
timate the loss, as a conï¬dence interval can provide non-trivial upper and
lower bounds for the loss. In terms of convergence rate over Î˜0(k) or Î˜(k),

the minimax rate of the expected length of conï¬dence intervals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

q
coincides with the minimax estimation rate. We also consider the adaptivity
of conï¬dence intervals. (The framework for adaptive conï¬dence intervals is

4

T. T. CAI AND Z. GUO

n

n

discussed in detail in Section 3.1.) Regarding conï¬dence intervals for the
(cid:96)2 loss in the case of known Î£ = I and Ïƒ = Ïƒ0, a procedure is proposed
and is shown to achieve the optimal length 1âˆš
n adaptively over Î˜0(k) for
âˆš
log p (cid:28) k (cid:46) n
log p . Furthermore, it is shown that this is the only regime where
adaptive conï¬dence intervals exist, even over two given parameter spaces.
For example, when k1 (cid:28) âˆš
log p and k1 (cid:28) k2, it is impossible to construct a
conï¬dence interval for the (cid:96)2 loss with guaranteed coverage probability over
Î˜0(k2) (consequently also over Î˜0(k1)) and with expected length automat-
ically adjusted to the sparsity. Similarly, for the (cid:96)q loss with 1 â‰¤ q < 2,
construction of adaptive conï¬dence intervals is impossible over Î˜0(k1) and
Î˜0(k2) for k1 (cid:28) k2 (cid:46) n
log p . Regarding conï¬dence intervals for the (cid:96)q loss with
1 â‰¤ q â‰¤ 2 in the case of unknown Î£ and Ïƒ, the impossibility of adaptivity
also holds over Î˜(k1) and Î˜(k2) for k1 (cid:28) k2 (cid:46) n
log p .

Establishing rate-optimal lower bounds for the minimax estimation error
and the expected length of minimax and adaptive conï¬dence intervals for the
(cid:96)q loss requires the development of new technical tools. One main diï¬€erence
between loss estimation and the traditional parameter estimation is that for

loss estimation the constraint is on the performance of the estimator (cid:98)Î² of
its loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
of adaptive conï¬dence intervals for the loss (cid:107)(cid:98)Î²âˆ’ Î²(cid:107)2

the regression vector Î², but the lower bound is on the diï¬ƒculty of estimating
q in terms of the minimax estimation error and the expected
length of conï¬dence intervals. Conventional lower bound methods are either
not applicable or do not yield rate-optimal results. We introduce useful lower
bound techniques for the minimax estimation error and the expected length
q. In one interesting case,
it is necessary to test a composite null against a composite alternative in
order to establish rate-optimal lower bounds. The technical tools developed
in this paper can also be of independent interest.

1.2. Comparison with other works. Statistical inference on the loss of
speciï¬c estimators of Î² has been considered in the recent literature. In
particular, the limit of a normalized loss has been studied in [10, 1, 17].
More speciï¬cally, the papers [10, 1] established, in the setting Î£ = I and
n

p â†’ Î´ âˆˆ (0,âˆ), the limit of the normalized loss 1

p(cid:107)(cid:98)Î²(Î») âˆ’ Î²(cid:107)2

2 where (cid:98)Î²(Î»)

is the Lasso estimator with a pre-speciï¬ed tuning parameter Î». Although
[10, 1] provided an exact asymptotic expression of the normalized loss, the
limit itself depends on the unknown Î². In a similar setting, the paper [17]
established the limit of a normalized (cid:96)2 loss of the square-root Lasso estima-
tor in the case Î£ = I. Such a limit also depends on the true signal Î². These
limits of the normalized losses help understand the theoretical properties
of the corresponding estimators of Î², but they do not lead to an estimate

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

5

the (cid:96)2 loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

of the loss. Comparing the above line of work with the present paper, our
results imply that although these normalized losses have a limit under some
regularity conditions, such losses cannot be estimated well in most settings.
In a recent paper [11], the authors constructed a conï¬dence interval for
2 in the case of known design covariance matrix Î£ =
I and unknown noise level Ïƒ. They considered the moderate-dimensional
setting where the dimension p is of the same order as the sample size n,
i.e., n/p â†’ Î¾ âˆˆ (0, 1) and no sparsity is assumed on Î². No optimality on
the expected length of conï¬dence intervals is discussed. By further assuming
that Î² is sparse, an application of the convex algorithm proposed in [11] can
produce a conï¬dence interval of length 1âˆš
n . However, taking the scaled Lasso
2 , then the
minimax expected length of conï¬dence intervals over the parameter space
{(Î², I, Ïƒ) : (cid:107)Î²(cid:107)0 â‰¤ k, Ïƒ â‰¤ M2} is of order k log p
n in
the regime n/p â†’ Î¾ âˆˆ (0, 1) and k = cpÎ³ with 0 â‰¤ Î³ < 1
2 , hence a simple
application of the conï¬dence interval constructed in [11] does not achieve
the optimal rate.

estimator (cid:98)Î² as an example, if k = cpÎ³ for some constant 0 â‰¤ Î³ < 1

n . Note that k log p

n (cid:28) 1âˆš

2 establishes the minimax convergence rate for estimating the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
tion of (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
adaptivity of conï¬dence intervals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
lower bounds of estimating the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

1.3. Organization. The rest of this paper is organized as follows. Section
with 1 â‰¤ q â‰¤ 2 over both Î˜0(k) and Î˜(k). We then turn to interval estima-
q. Sections 3 and 4 present the minimax expected length and
q over the parameter spaces
Î˜0(k) and Î˜(k), respectively. Section 5 presents general tool for minimax
q. Section 6 compares the mini-
maxity and adaptivity behaviors over Î˜(k) and Î˜0(k). Section 7 proves the
main lower bound results and more proofs of lower bound and upper bound
results are presented in the supplemental material [5].

q

1.4. Notation. For a matrix X âˆˆ RnÃ—p, XiÂ·, XÂ·j, and Xi,j denote re-
spectively the i-th row, j-th column, and (i, j) entry of the matrix X. For
a subset J âŠ‚ {1, 2,Â·Â·Â· , p}, |J| denotes the cardinality of J, J c denotes the
complement {1, 2,Â·Â·Â· , p}\J, XJ denotes the submatrix of X consisting of
columns XÂ·j with j âˆˆ J and for a vector x âˆˆ Rp, xJ is the subvector of
x with indices in J. For a vector x âˆˆ Rp, supp(x) denotes the support of
q for q â‰¥ 0 with
(cid:107)x(cid:107)0 = |supp(x)| and (cid:107)x(cid:107)âˆ = max1â‰¤jâ‰¤p |xj|. For a âˆˆ R, a+ = max{a, 0}.
We use max(cid:107)XÂ·j(cid:107)2 as a shorthand for max1â‰¤jâ‰¤p (cid:107)XÂ·j(cid:107)2 and min(cid:107)XÂ·j(cid:107)2 as a
shorthand for min1â‰¤jâ‰¤p (cid:107)XÂ·j(cid:107)2. For a matrix A, (cid:107)A(cid:107)2 = sup(cid:107)x(cid:107)2=1 (cid:107)Ax(cid:107)2 is
the spectral norm; For a symmetric matrix A, Î»min (A) and Î»max (A) denote

x and the (cid:96)q norm of x is deï¬ned as (cid:107)x(cid:107)q = ((cid:80)q

i=1 |xi|q)

1

6

T. T. CAI AND Z. GUO

respectively the smallest and largest eigenvalue of A. We use c and C to
denote generic positive constants that may vary from place to place. For
two positive sequences an and bn, an (cid:46) bn means an â‰¤ Cbn for all n and
an (cid:38) bn if bn (cid:46) an and an (cid:16) bn if an (cid:46) bn and bn (cid:46) an, and an (cid:28) bn if
lim supnâ†’âˆ an
bn

= 0 and an (cid:29) bn if bn (cid:28) an.

2. Minimax estimation of the (cid:96)q loss.

presenting the minimax framework for estimating the (cid:96)q loss, (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
a given estimator (cid:98)Î², and then establish the minimax lower bound for the
estimation error for a broad collection of estimators (cid:98)Î². We also show that

In this section, we begin by
q, of

such minimax lower bound can be achieved for the Lasso and scaled Lasso
estimators.

2.1. Problem formulation. Recall the high-dimensional linear model,

(2.1)

ynÃ—1 = XnÃ—pÎ²pÃ—1 + nÃ—1,

 âˆ¼ Nn(0, Ïƒ2I).
iidâˆ¼ N (0, Î£) and XiÂ· and  are

We focus on the random design with XiÂ·

independent. Let Z = (X, y) denote the observed data and (cid:98)Î² be a given
estimator of Î². Denoting by (cid:98)Lq(Z) any estimator of the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
minimax rate of convergence for estimating (cid:107)(cid:98)Î²âˆ’ Î²(cid:107)2
Î˜ is deï¬ned as the largest quantity Î³(cid:98)Î²,(cid:96)q
(cid:16)|(cid:98)Lq(Z) âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
q| â‰¥ Î³(cid:98)Î²,(cid:96)q
for some positive constant Î´ not depending on n or p. We shall write (cid:98)Lq for
(cid:98)Lq(Z) when there is no confusion.

q, the
q over a parameter space

(cid:17) â‰¥ Î´,

(Î˜) such that

(2.2)

inf(cid:98)Lq

PÎ¸

sup
Î¸âˆˆÎ˜

(Î˜)

We denote the parameter by Î¸ = (Î², Î£, Ïƒ), which consists of the signal Î²,
the design covariance matrix Î£ and the noise level Ïƒ. For a given parameter
Î¸ = (Î², Î£, Ïƒ), we use Î²(Î¸) to denote the corresponding Î² of this parameter.
The problem of loss estimation is studied in two settings: One is the setting
with known design covariance matrix Î£ = I and known noise level Ïƒ = Ïƒ0
and the other is unknown Î£ and Ïƒ. In the ï¬rst setting, we consider the
following parameter space that consists of k-sparse signals,

(2.3)

Î˜0(k) = {(Î², I, Ïƒ0) : (cid:107)Î²(cid:107)0 â‰¤ k} ,

and in the second setting, we consider
(2.4)

(cid:26)

Î˜(k) =

(Î², Î£, Ïƒ) : (cid:107)Î²(cid:107)0 â‰¤ k,

1
M1

â‰¤ Î»min (Î£) â‰¤ Î»max (Î£) â‰¤ M1, 0 < Ïƒ â‰¤ M2

(cid:27)

,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

7

where M1 â‰¥ 1 and M2 > 0 are constants. The parameter space Î˜0(k) is a
proper sub-space of Î˜(k), which consists of k-sparse signals with unknown
â‰¤ Î»min (Î£) â‰¤ Î»max (Î£) â‰¤ M1 and 0 < Ïƒ â‰¤ M2
Î£ and Ïƒ. The conditions
are two regularity conditions on the covariance matrix and the noise level.

(Î˜) of estimating (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
Note that the minimax convergence rate Î³(cid:98)Î²,(cid:96)q
also depends on the particular estimator (cid:98)Î². Diï¬€erent estimators (cid:98)Î² could lead
to diï¬€erent losses (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

q varies with the estimator (cid:98)Î².

q and in general the diï¬ƒculty of estimating the

1
M1

q

We ï¬rst recall the properties of some state-of-art estimators in high-
dimensional linear regression and then specify the collection of estimators
that we focus on in this paper. As shown in [8, 3, 2, 16], Lasso, Dantzig
Selector, scaled Lasso and square-root Lasso satisfy the following property
if the tuning parameter is chosen properly,
q â‰¥ Ck

(cid:18)
(cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

â†’ 0,

(cid:19)

(2.5)

log p

PÎ¸

2
q

n

sup
Î¸âˆˆÎ˜(k)

n

2
q log p

where C > 0 is a constant. The minimax lower bounds established in [19,
14, 20] imply that k
is the minimax optimal rate of estimating Î².
It should be stressed that all of these algorithms do not require a prior
knowledge of the sparsity k and is thus adaptive to the sparsity provided
k (cid:46) n
log p . Throughout the paper, we consider a broad collection of estimators

(cid:98)Î² satisfying one of the following two assumptions.
(A1) The estimator (cid:98)Î² satisï¬es the following property at a single point Î¸0 =

where 0 â‰¤ Î±0 < 1

(A2) The estimator(cid:98)Î² satisï¬es the following property on the set {Î¸ = (Î²âˆ—, I, Ïƒ) , Ïƒ â‰¤ 2Ïƒ0},

4 and C > 0 are constants and Ïƒ0 > 0 is given.

(2.7)

inf

{Î¸=(Î²âˆ—,I,Ïƒ):Ïƒâ‰¤2Ïƒ0}

q â‰¤ C(cid:107)Î²âˆ—(cid:107) 2

q
0

log p

n

Ïƒ2

â‰¥ 1 âˆ’ Î±0,

where 0 â‰¤ Î±0 < 1

4 and C > 0 are constants and Ïƒ0 > 0 is given.

In view of the minimax rate given in (2.5), the assumption (A1) requires

(cid:98)Î² to be a good estimator of Î² at least at a single point {Î¸0}. The assumption
(A2) is slightly stronger than (A1) and requires (cid:98)Î² to estimate Î² well for a
any rate-optimal estimator (cid:98)Î² satisfying (2.5) satisï¬es both (A1) and (A2) .

single Î²âˆ— but over a range of noise levels Ïƒ â‰¤ 2Ïƒ0 while Î£ = I. Of course,

(Î²âˆ—, I, Ïƒ0),

(2.6)

PÎ¸0

(cid:18)
(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2
q â‰¤ C(cid:107)Î²âˆ—(cid:107) 2
(cid:18)
(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

PÎ¸

q
0

(cid:19)

log p

n

Ïƒ2
0

â‰¥ 1 âˆ’ Î±0,

(cid:19)

8

T. T. CAI AND Z. GUO

rem establishes the minimax lower bound for estimating the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

2.2. Minimax estimation of the (cid:96)q loss over Î˜0(k). The following theo-

q

over the parameter space Î˜0 (k).

(cid:110)

(cid:111)

c min

Theorem 1. Suppose that 1 â‰¤ q â‰¤ 2, k0 (cid:28) min{k,

âˆš
log p} and k â‰¤
(cid:98)Î² satisfying the assumption (A1) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then we have
2 . For any estimator
(cid:27)

for some constants c > 0 and 0 â‰¤ Î³ < 1

pÎ³, n
log p

(cid:26)

(cid:19)

n

(cid:18)
|(cid:98)L2 âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

Î¸âˆˆÎ˜0(k)

For any estimator (cid:98)Î² satisfying the assumption (A2) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then

n

log p

k

,

1âˆš
n

2| â‰¥ c min

inf(cid:98)L2

sup

PÎ¸

â‰¥ Î´;

Ïƒ2
0

(2.8)

we have

(2.9)

inf(cid:98)Lq

PÎ¸

sup

Î¸âˆˆÎ˜0(k)

(cid:18)
|(cid:98)Lq âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

(cid:19)

q| â‰¥ ck

2
q

log p

n

Ïƒ2
0

â‰¥ Î´,

for 1 â‰¤ q < 2,

where Î´ > 0 and c > 0 are positive constants.

Theorem 1 establishes the minimax lower bounds for estimating the (cid:96)2

(A2). We will take the Lasso estimator as an example and demonstrate
the implications of the above theorem. We randomly split Z = (y, X) into

2 of any estimator (cid:98)Î² satisfying the assumption (A1) and the (cid:96)q
loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
q with 1 â‰¤ q < 2 of any estimator (cid:98)Î² satisfying the assumption
loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
subsamples Z(1) =(cid:0)y(1), X (1)(cid:1) and Z(2) =(cid:0)y(2), X (2)(cid:1) with sample sizes n1
and n2, respectively. The Lasso estimator (cid:98)Î²L based on the ï¬rst subsample
Z(1) =(cid:0)y(1), X (1)(cid:1) is deï¬ned as
(cid:114) log p
(cid:98)Î²L = arg min

|Î²j| with Î» = A

(cid:107)y(1) âˆ’ X (1)Î²(cid:107)2

(cid:107)X (1)Â·j (cid:107)2âˆš

p(cid:88)

(2.10)

+ Î»

Ïƒ0,

2

n1

n1

j=1

n1

(cid:16)|0 âˆ’ (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

ply that the estimation of the (cid:96)q loss (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2
where A >
2 is a pre-speciï¬ed constant. Without loss of generality, we
assume n1 (cid:16) n2. For the case 1 â‰¤ q < 2, (2.5) and (2.9) together im-
(cid:17) â†’ 0.
q is impossible since the
lower bound can be achieved by the trivial estimator of the loss, 0. That is,
supÎ¸âˆˆÎ˜0(k)
For the case q = 2, in the regime k (cid:28) âˆš
(cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

in
(2.8) can be achieved by the zero estimator and hence estimation of the loss

2 is impossible. However, the interesting case is when

log p , the lower bound k log p

âˆš
log p (cid:28) k (cid:46)

q| â‰¥ Ck

2
q log p

PÎ¸

n

n

n

n

Î²âˆˆRp
âˆš

9

n2

(2.11)

(cid:19)

(cid:18) 1

(cid:101)L2 =

n
bound 1âˆš

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

n in (2.8), which cannot be achieved by the zero estimator. We now

log p , the loss estimator (cid:101)L2 proposed in (2.11) achieves the minimax lower
detail the construction of the loss estimator (cid:101)L2. Based on the second half
sample Z(2) =(cid:0)y(2), X (2)(cid:1), we propose the following estimator,
(cid:13)(cid:13)(cid:13)y(2) âˆ’ X (2)(cid:98)Î²L(cid:13)(cid:13)(cid:13)2
Note that the ï¬rst subsample Z(1) =(cid:0)y(1), X (1)(cid:1) is used to produce the Lasso
estimator (cid:98)Î²L deï¬ned in (2.10) and the second subsample Z(2) =(cid:0)y(2), X (2)(cid:1)
is retained for evaluating the loss (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2
The following proposition establishes that the estimator (cid:101)L2 achieves the
log p and (cid:98)Î²L is the Lasso estima-

2. Such sample splitting tech-
nique is same with cross-validation in nature and has been used in [13] for
constructing conï¬dence sets for Î² and in [11] for constructing conï¬dence
intervals for the (cid:96)2 loss.

Proposition 1. Suppose that k (cid:46) n

minimax lower bound of (2.8) over the regime

âˆš
log p (cid:28) k (cid:46) n
log p .

âˆ’ Ïƒ2

0

.

+

n

2

âˆš

tor deï¬ned in (2.10) with A >
satisï¬es, for any sequence Î´n,p â†’ âˆ,

(2.12)

lim sup
n,pâ†’âˆ sup
Î¸âˆˆÎ˜0(k)

PÎ¸

2, then the estimator proposed in (2.11)

(cid:18)(cid:12)(cid:12)(cid:12)(cid:101)L2 âˆ’ (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

2

(cid:12)(cid:12)(cid:12) â‰¥ Î´n,p

(cid:19)

1âˆš
n

= 0.

2.3. Minimax estimation of the (cid:96)q loss over Î˜(k). We now turn to the
case of unknown Î£ and Ïƒ and establish the minimax lower bound for esti-
mating the (cid:96)q loss over the parameter space Î˜(k).

(cid:111)
2 . For any estimator (cid:98)Î² satisfying the

pÎ³, n
log p

(cid:110)

for

Theorem 2. Suppose that 1 â‰¤ q â‰¤ 2, k0 (cid:28) k â‰¤ c min

some constants c > 0 and 0 â‰¤ Î³ < 1
assumption (A1) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then we have
q| â‰¥ ck

(cid:18)
|(cid:98)Lq âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

(2.13)

PÎ¸

inf(cid:98)Lq

sup
Î¸âˆˆÎ˜(k)

(cid:19)

2
q

log p

n

â‰¥ Î´,

where Î´ > 0 and c > 0 are positive constants.

any estimator (cid:98)Î² satisfying the assumption (A1), including the scaled Lasso

Theorem 2 provides a minimax lower bound for estimating the (cid:96)q loss of

10

T. T. CAI AND Z. GUO

estimator deï¬ned as

(2.14)

{(cid:98)Î²SL, Ë†Ïƒ} = arg min
(cid:113) log p

âˆš

Î²âˆˆRp,ÏƒâˆˆR+

(cid:107)y âˆ’ XÎ²(cid:107)2

2

2nÏƒ

+

Ïƒ
2

+ Î»0

p(cid:88)

j=1

(cid:107)XÂ·j(cid:107)2âˆš

n

|Î²j|,

n with A >

where Î»0 = A
2. Note that for the scaled Lasso estimator,
the lower bound in (2.13) can be achieved by the trivial loss estimate 0
in the sense, supÎ¸âˆˆÎ˜(k)
estimation of loss is impossible in this case.

(cid:17) â†’ 0, and hence

(cid:16)|0 âˆ’ (cid:107)(cid:98)Î²SL âˆ’ Î²(cid:107)2

q| â‰¥ Ck

2
q log p

PÎ¸

n

intervals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

3. Minimaxity and adaptivity of conï¬dence intervals over Î˜0(k).
We focused in the last section on point estimation of the (cid:96)q loss and showed
the impossibility of loss estimation except for one regime. The results natu-
rally lead to the next question: Is it possible to construct â€œusefulâ€ conï¬dence
q that can provide non-trivial upper and lower bounds
for the loss. In this section, after introducing the framework for minimaxity
and adaptivity of conï¬dence intervals, we consider the case of known Î£ = I
and Ïƒ = Ïƒ0 and establish the minimax expected length and adaptivity re-
sults for conï¬dence intervals over the parameter space Î˜0(k). Minimaxity
and adaptivity of conï¬dence intervals for the case of unknown Î£ and Ïƒ will
be the focus of next section.

3.1. Framework for minimaxity and adaptivity of conï¬dence intervals.

In
this section, we introduce the following decision theoretical framework for
q. Given 0 < Î± < 1 and the parameter
q, denote by IÎ±
the set of

conï¬dence intervals of the loss (cid:107)(cid:98)Î²âˆ’Î²(cid:107)2
space Î˜ and the loss function (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
all (1 âˆ’ Î±) level conï¬dence intervals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:16)
Î˜,(cid:98)Î², (cid:96)q

(cid:17)
(cid:16)
Î˜,(cid:98)Î², (cid:96)q
(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²(Î¸)(cid:107)2

(3.1)
IÎ±

q over Î˜,

q âˆˆ CIÎ±

(cid:26)

(cid:17)

PÎ¸

=

= [l (Z) , u (Z)] : inf
Î¸âˆˆÎ˜

We will write CIÎ± for CIÎ±

when there is no confusion. For any

conï¬dence interval CIÎ±

= [l (Z) , u (Z)], its length is denoted by
= u (Z) âˆ’ l (Z) and the maximum expected length over

CIÎ±

L
a parameter space Î˜1 is deï¬ned as

(cid:16)

(cid:16)(cid:98)Î², (cid:96)q, Z
(cid:16)

(3.2)

L

CIÎ±

(cid:16)

(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17)(cid:17)

.

= sup
Î¸âˆˆÎ˜1

EÎ¸L

CIÎ±

(cid:17)

CIÎ±

(cid:16)(cid:98)Î², (cid:96)q, Z
(cid:17)
(cid:16)(cid:98)Î², (cid:96)q, Z
(cid:17)
(cid:16)(cid:98)Î², (cid:96)q, Z
(cid:17)(cid:17)
(cid:17)
(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17)

, Î˜1

(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17)(cid:17) â‰¥ 1 âˆ’ Î±

(cid:27)

.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:16)
(cid:17)(cid:17)

Î±

11

Î˜1, Î˜2,(cid:98)Î², (cid:96)q

(cid:17)

,

.

Î±

Î±

Î±

=

inf

EÎ¸L

CIÎ±

(cid:16)

(cid:16)

(cid:16)

sup
Î¸âˆˆÎ˜1

for Lâˆ—

, which is the minimax

We will write Lâˆ—

(cid:16)(cid:98)Î², (cid:96)q, Z

Î˜1, Î˜2,(cid:98)Î², (cid:96)q
(cid:16)
Î˜1, Î˜2,(cid:98)Î², (cid:96)q
(cid:17)

(cid:17)
CIÎ±((cid:98)Î²,(cid:96)q,Z)âˆˆIÎ±(Î˜2,(cid:98)Î²,(cid:96)q)
(cid:17)
Î˜1,(cid:98)Î², (cid:96)q
(cid:17)

For two nested parameter spaces Î˜1 âŠ† Î˜2, we deï¬ne the benchmark Lâˆ—
measuring the degree of adaptivity over the nested spaces Î˜1 âŠ‚ Î˜2,
(3.3)
Lâˆ—

(cid:16)
Î˜1,(cid:98)Î², (cid:96)q

(cid:17)
Î˜1, Î˜1,(cid:98)Î², (cid:96)q
expected length of conï¬dence intervals of (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:16)

q over Î˜1. The bench-
mark Lâˆ—
is the inï¬mum of the maximum expected length
over Î˜1 among all (1 âˆ’ Î±)-level conï¬dence intervals over Î˜2. In contrast,
is considering all (1 âˆ’ Î±)-level conï¬dence intervals over Î˜1.
Lâˆ—
In words, if there is prior information that the parameter lies in the smaller
parameter space Î˜1, Lâˆ—
measures the benchmark length of con-
ï¬dence intervals over the parameter space Î˜1, which is illustrated in the left
of Figure 1; however, if there is only prior information that the parameter
lies in the larger parameter space Î˜2, Lâˆ—
measures the bench-
mark length of conï¬dence intervals over the parameter space Î˜1, which is
illustrated in the right of Figure 1.

Î˜1, Î˜2,(cid:98)Î², (cid:96)q

Î˜1,(cid:98)Î², (cid:96)q

(cid:17)

(cid:16)

(cid:16)

(cid:17)

Î±

Î±

Î±

Î±

(cid:16)
Î˜1,(cid:98)Î², (cid:96)q

(cid:17)

(cid:16)

Î˜1, Î˜2,(cid:98)Î², (cid:96)q

(cid:17)

.

and Lâˆ—

Î±

Fig 1. The plot demonstrates the deï¬nition of Lâˆ—

Î±

(cid:17)

(cid:16)
Î˜1,(cid:98)Î², (cid:96)q

(cid:16)

Î˜2,(cid:98)Î², (cid:96)q
(cid:17)

Rigorously, we deï¬ne a conï¬dence interval CIâˆ— to be simultaneously adap-

tive over Î˜1 and Î˜2 if CIâˆ— âˆˆ IÎ±
(3.4) L (CIâˆ—, Î˜1) (cid:16) Lâˆ—
The condition (3.4) means that the conï¬dence interval CIâˆ— has coverage over
the larger parameter space Î˜2 and achieves the minimax rate over both Î˜1
and Î˜2. Note that L (CIâˆ—, Î˜1) â‰¥ Lâˆ—

Î˜2,(cid:98)Î², (cid:96)q
Î˜1, Î˜2,(cid:98)Î², (cid:96)q

, and L (CIâˆ—, Î˜2) (cid:16) Lâˆ—

Î˜1, Î˜2,(cid:98)Î², (cid:96)q

(cid:17)
(cid:17) (cid:29)

. If Lâˆ—

(cid:16)

(cid:16)

(cid:17)

(cid:16)

Î±

Î±

,

.

Î±

Î±

Î©Î˜(cid:2869)Î˜(cid:2870)ğ‹(cid:3080)âˆ—(Î˜(cid:2869),Î˜(cid:2870),(cid:4632)ğ›½,â„“ğ“(cid:3044))Î˜(cid:2869)ğ‹(cid:3080)âˆ—(Î˜(cid:2869),(cid:4632)ğ›½,â„“ğ“(cid:3044))(cid:16)

(cid:17)

Î±

Î˜1,(cid:98)Î², (cid:96)q

T. T. CAI AND Z. GUO

12
Lâˆ—
, then the rate-optimal adaptation (3.4) is impossible to achieve
for Î˜1 âŠ‚ Î˜2. Otherwise, it is possible to construct conï¬dence intervals si-
multaneously adaptive over parameter spaces Î˜1 and Î˜2. The possibility of
adaptation over parameter spaces Î˜1 and Î˜2 can thus be answered by in-
vestigating the benchmark quantities Lâˆ—
.
Such framework has already been introduced in [6], which studies the mini-
maxity and adaptivity of conï¬dence intervals for linear functionals in high-
dimensional linear regression.

Î˜1, Î˜2,(cid:98)Î², (cid:96)q

Î˜1,(cid:98)Î², (cid:96)q

and Lâˆ—

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Î±

Î±

In the following sections, we will adopt the minimax and adaptation

framework discussed above and establish the minimax expected length Lâˆ—
and the adaptation benchmark Lâˆ—
. In terms of the
minimax expected length and the adaptivity behavior, there exists funda-
mental diï¬€erence between the case q = 2 and 1 â‰¤ q < 2 . We will discuss
them separately in the following two sections.

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)q

Î±

Î±

(cid:17)

(cid:16)

(cid:16)

Î˜0(k),(cid:98)Î², (cid:96)q

(cid:17)

expected length of conï¬dence intervals of (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

3.2. Conï¬dence intervals for the (cid:96)2 loss over Î˜0(k). We begin with the
(cid:96)2 loss. The following theorem establishes the minimax lower bound for the
2 over the parameter space

Î˜0(k).

Theorem 3. Suppose 0 < Î± < 1

for some constants c > 0 and 0 â‰¤ Î³ < 1
assumption (A1) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then there is some constant c > 0 such
that

(cid:111)

pÎ³, n
log p

n

(cid:110)
2 . For any estimator (cid:98)Î² satisfying the
4 , k0 (cid:28) min{k,
(cid:26) k log p

âˆš
log p} and k â‰¤ c min
(cid:27)

Î˜0(k),(cid:98)Î², (cid:96)2

(cid:17) â‰¥ c min

Lâˆ—

(3.5)

Î±

âˆš
2,
then the minimax expected length for (1 âˆ’ Î±) level conï¬dence intervals of

In particular, if (cid:98)Î²L is the Lasso estimator deï¬ned in (2.10) with A >
(cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

2 over Î˜0(k) is

n

,

1âˆš
n

Ïƒ2
0.

Î˜0(k),(cid:98)Î²L, (cid:96)2

(cid:17) (cid:16) min

(cid:26) k log p

,

1âˆš
n

n

(cid:27)

(3.6)

Lâˆ—

Î±

(cid:16)

(cid:16)

We now consider adaptivity of conï¬dence intervals for the (cid:96)2 loss. The fol-

lowing theorem gives the lower bound for the benchmark Lâˆ—
We will then discuss Theorems 3 and 4 together.

Î±

Ïƒ2
0.

(cid:16)

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)2

(cid:17)

.

(cid:111)
(cid:16)

(cid:110)

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

13

Theorem 4. Suppose 0 < Î± < 1

âˆš
log p} and k1 â‰¤ k2 â‰¤
tor (cid:98)Î² satisfying the assumption (A1) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then there is some
2 . For any estima-

for some constants c > 0 and 0 â‰¤ Î³ < 1

4 , k0 (cid:28) min{k1,

pÎ³, n
log p

c min

n

constant c > 0 such that

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)2

Lâˆ—

(3.7)

(cid:17) â‰¥ c min

(cid:26) k2 log p

(cid:27)

,

1âˆš
n

Ïƒ2
0.

Î±

In particular, if (cid:98)Î²L is the Lasso estimator deï¬ned in (2.10) with A >

n

âˆš

2,

the above lower bound can be achieved.

Î±

(cid:17)

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)2

. In the regime k2 (cid:28) âˆš
(cid:46) k2 (cid:46) n

(cid:16)
0. For the Lasso estimator (cid:98)Î²L deï¬ned in (2.10), the lower bound

The lower bound established in Theorem 4 implies that of Theorem 3
and both lower bounds hold for a general class of estimators satisfying the
assumption (A1). There exists a phase transition for the lower bound of the
benchmark Lâˆ—
log p , the lower
bound in (3.7) is k2 log p
log p , the lower bound in
(3.7) is 1âˆš
k log p
n Ïƒ2
0 in (3.7) can be achieved by the conï¬dence
interval CI0
Î± (Z, k2, 2) deï¬ned in (3.15), respectively. Such
an interval estimator is also used for the (cid:96)q loss with 1 â‰¤ q < 2. The mini-
max lower bound 1âˆš
0 in (3.6) and (3.7) can be achieved by the following
constructed conï¬dence interval,

0 in (3.5) and k2 log p
n Ïƒ2
Î± (Z, k, 2) and CI0

0; when

âˆš
n
log p

n Ïƒ2

n Ïƒ2

n Ïƒ2

n

ï£«ï£­(cid:32)

(cid:33)

(cid:32)

,

0

(n2)

âˆ’ Ïƒ2

Ïˆ (Z)
Ï‡2
1âˆ’ Î±
(n2) are the 1 âˆ’ Î±

+

2

1
n2

Î±
2

(cid:26) 1

n2

(cid:13)(cid:13)(cid:13)y(2) âˆ’ X (2)(cid:98)Î²L(cid:13)(cid:13)(cid:13)2

2

(3.8)

CI1

Î± (Z) =

where Ï‡2
respectively, and

1âˆ’ Î±

2

(n2) and Ï‡2

(3.9)

Ïˆ (Z) = min

(cid:33)

ï£¶ï£¸ ,

Ïˆ (Z)
Ï‡2

(n2)

Î±
2

1
n2

âˆ’ Ïƒ2

0

+

2 and Î±

2 quantiles of Ï‡2 (n2),

(cid:27)

, Ïƒ2

0 log p

.

Note that the two-sided conï¬dence interval (3.8) is simply based on the
observed data Z, not depending on any prior knowledge of the sparsity k.
Furthermore, it is a two-sided conï¬dence interval, which tells not only just
an upper bound, but also a lower bound for the loss. The coverage property
and expected length of CI1
Î± (Z) are established in the following proposition.

Proposition 2. Suppose k â‰¤ c n

âˆš

T. T. CAI AND Z. GUO

log p and (cid:98)Î²L is the estimator deï¬ned in

(2.10) with A >

2. Then CI1

Î± (Z) deï¬ned in (3.8) satisï¬es,

lim inf
n,pâ†’âˆ inf

Î¸âˆˆÎ˜0(k)

14

(3.10)

and

(3.11)

(cid:17) â‰¥ 1 âˆ’ Î±,

Î± (Z)

P(cid:16)(cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2
Î± (Z) , Î˜0 (k)(cid:1) (cid:46) 1âˆš
L(cid:0)CI1

2 âˆˆ CI1

Ïƒ2
0.

n

(cid:16)

(cid:17)

Î±

(cid:17)

on the

left

illustrates Lâˆ—

Î±

Î˜0(k1),(cid:98)Î²L, (cid:96)2
log p (cid:28) k2 (cid:46) n

on the right column. The top, middle and bottom plots

(cid:16)
Î˜0(k1), Î˜0(k2),(cid:98)Î²L, (cid:96)2
Fig 2. The ï¬gure
Lâˆ—
represent regimes k1 â‰¤ k2 (cid:46) âˆš
Regarding the Lasso estimator (cid:98)Î²L deï¬ned in (2.10), we will discuss the
adaptivity of conï¬dence intervals of the loss (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2
behavior of conï¬dence intervals for (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n

log p , k1 (cid:46) âˆš

column and

respectively.

n

n

log p and

n

log p ,

2. The adaptivity
2 is demonstrated in Figure
âˆš
log p (cid:28) k1 â‰¤

n

2. As illustrated in the bottom plot of Figure 2, in the regime

Î©Î˜(cid:2868)ğ‘˜(cid:2869)Î˜(cid:2868)ğ‘˜(cid:2870)ğ‘˜(cid:2869)logğ‘ğ‘›ğ‘˜(cid:2870)logğ‘ğ‘›Î˜(cid:2868)ğ‘˜(cid:2869)Î©Î©Î˜(cid:2868)ğ‘˜(cid:2869)Î˜(cid:2868)ğ‘˜(cid:2870)ğ‘˜(cid:2869)logğ‘ğ‘›1ğ‘›Î˜(cid:2868)ğ‘˜(cid:2869)Î˜(cid:2868)ğ‘˜(cid:2869)Î˜(cid:2868)ğ‘˜(cid:2870)1ğ‘›1ğ‘›Î˜(cid:2868)ğ‘˜(cid:2869)n

and k1 (cid:28) k2.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:16)

Î±

Î˜0(k2), Î˜0(k2),(cid:98)Î²L, (cid:96)2

(cid:17) (cid:16) Lâˆ—

Î±

(cid:16)

Î˜0(k1),(cid:98)Î²L, (cid:96)2

15

(cid:17) (cid:16)

(cid:17)

Lâˆ—

Î±

(cid:16)

Î±

n

log p since

(cid:17) (cid:29) Lâˆ—

Î˜0(k1),(cid:98)Î²L, (cid:96)2

log p , we obtain Lâˆ—

k2 (cid:46) n
1âˆš
n , which implies that adaptation is possible over this regime. As shown
in Proposition 2, the conï¬dence interval CI1
Î± (Z) deï¬ned in (3.8) is fully
adaptive over the regime

âˆš
log p (cid:28) k (cid:46) n

log p in the sense of (3.4).

Illustrated in the top and middle of Figure 2, it is impossible to construct

Î˜0(k1), Î˜0(k2),(cid:98)Î²L, (cid:96)2

an adaptive conï¬dence interval for the Lasso estimator (cid:98)Î²L over the regime
k1 (cid:28) âˆš
(cid:16)
To sum up, the adaptation of conï¬dence intervals for (cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2
vals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:110)
Î² : (cid:107)Î² âˆ’(cid:98)Î²(cid:107)2
conï¬dence interval for the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:110)(cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

balls constructed in [13] are of form
be the Lasso estimator and Un (Z) is a data dependent squared radius. See
[13] for further details. Such a conï¬dence ball directly leads to a one-sided

Comparison with conï¬dence balls. The construction of conï¬dence inter-
2 is related to that of conï¬dence balls for Î². Conï¬dence

2 â‰¤ Un (Z)
(cid:111)
2 â‰¤ Un (Z)

(cid:111)
, where (cid:98)Î² can

2,

2 : (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

possible over the regime

(cid:46) k (cid:46) n

if k1 (cid:28)

CIinduced

Î±

2 is only

âˆš

n
log p

âˆš
n
log p

(Z) =

(3.12)

log p .

.

This induced one-sided conï¬dence interval for the loss is not as informative
as the two-sided conï¬dence interval constructed in (3.8) since the conï¬dence
interval (3.12) does not contain the information whether the loss is close to
zero or not. Furthermore, as shown in [13], the length of conï¬dence interval
CIinduced
n . The two-
sided conï¬dence interval CI1
Î± (Z) constructed in (3.8) is of expected length
n , which is much shorter than 1âˆš
1âˆš
log p . That
is, the two-sided conï¬dence interval (3.8) provides a more accurate interval
estimator of the (cid:96)2 loss. This is illustrated in Figure 3.

n + k log p
in the regime k (cid:29) âˆš

(Z) over the parameter space Î˜0(k) is of order 1âˆš

n + k log p

Î±

n

n

length and adaptivity of conï¬dence intervals for (cid:107)(cid:98)Î²âˆ’Î²(cid:107)2

3.3. Conï¬dence intervals for the (cid:96)q loss with 1 â‰¤ q < 2 over Î˜0(k).
We now consider the case 1 â‰¤ q < 2 and investigate the minimax expected
q over the parameter
space Î˜0(k). The following theorem characterizes the minimax convergence
rate for the expected length of conï¬dence intervals.

16

T. T. CAI AND Z. GUO

Fig 3. Comparison of the two-sided conï¬dence interval CI1
dence interval CIinduced

(Z).

Î±

Î± (Z) with the one-sided conï¬-

(cid:110)

(cid:111)

Theorem 5. Suppose 0 < Î± < 1

âˆš
log p} and
estimator (cid:98)Î² satisfying the assumption (A2) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then there is
2 . For any

4 , 1 â‰¤ q < 2, k0 (cid:28) min{k,
for some constants c > 0 and 0 â‰¤ Î³ < 1

k â‰¤ c min

pÎ³, n
log p

n

some constant c > 0 such that

Î˜0(k),(cid:98)Î², (cid:96)q

(cid:17) â‰¥ ck

Lâˆ—

log p

2
q

Ïƒ2
0.

(3.13)

Î±

âˆš
2,
then the minimax expected length for (1 âˆ’ Î±) level conï¬dence intervals of

In particular, if (cid:98)Î²L is the Lasso estimator deï¬ned in (2.10) with A > 4
(cid:107)(cid:98)Î²L âˆ’ Î²(cid:107)2

q over Î˜0(k) is

n

(cid:17) (cid:16) k

2
q

log p

n

Ïƒ2
0.

Î˜0(k),(cid:98)Î²L, (cid:96)q
(cid:18)

(3.14)

Lâˆ—

Î±

gence rate of (3.14),

(3.15)

CI0

Î± (Z, k, q) =

(cid:16)

(cid:16)

We now construct the conï¬dence interval achieving the minimax conver-

(cid:40)

log p

2
q

n

0, Câˆ—(A, k)k
(cid:17)4 ,

(cid:16) 3Î·0
(cid:16) 1
4âˆ’(9+11Î·0)

Î·0+1 AÏƒ0

(22AÏƒ0)2

(cid:113) 2k log p

(cid:16) 1
4âˆ’42

n1

(cid:41)

(cid:17)4

with Î·0 =

(cid:19)
(cid:17)2
(cid:113) 2k log p

,

n1

where Câˆ—(A, k) = max

âˆš
âˆš

âˆš
Aâˆ’âˆš

A+

2
2

1.01
the expected length of CI0

. The following proposition establishes the coverage property and

Proposition 3. Suppose k (cid:46) n

Î± (Z, k, q).

log p and (cid:98)Î²L is the estimator deï¬ned in

âˆš
2. For 1 â‰¤ q â‰¤ 2, the conï¬dence interval CI0

Î± (Z, k, q)

(2.10) with A > 4

0âˆ¥(cid:4632)ğ›½âˆ’ğ›½âˆ¥(cid:2870)(cid:2870)CI(cid:3080)(cid:2869)(Z)CI(cid:3080)(cid:2919)(cid:2924)(cid:2914)(cid:2931)(cid:2913)(cid:2915)(cid:2914)(Z)HIGH-DIMENSIONAL ACCURACY ASSESSMENT

17

(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

deï¬ned in (3.15) satisï¬es

(3.16)

lim inf
n,pâ†’âˆ inf

Î¸âˆˆÎ˜0(k)

PÎ¸

and

(3.17)

L(cid:0)CI0
(cid:98)Î²L deï¬ned in (2.10) with A >

âˆš

2.

(cid:17)

q âˆˆ CI0

Î± (Z, k, q)

= 1,

Î± (Z, k, q) , Î˜0 (k)(cid:1) (cid:46) k

2
q

log p

n

Ïƒ2
0.

In particular, for the case q = 2, (3.16) and (3.17) also hold for the estimator

(cid:110)

(cid:111)

The above proposition shows that the conï¬dence interval CI0

Î± (Z, k, q)
achieves the minimax convergence rate given in (3.14). In contrast to the
(cid:96)2 loss where the two-sided conï¬dence interval (3.8) is signiï¬cantly shorter
than the one-sided interval and achieves the optimal convergence rate over
log p , for the (cid:96)q loss with 1 â‰¤ q < 2, the one-sided
the regime
conï¬dence interval is rate-optimal.

âˆš
log p (cid:28) k (cid:46) n

n

We now consider the adaptivity of conï¬dence intervals. The following
with 1 â‰¤

theorem establishes the lower bound for Lâˆ—
q < 2.

Î±

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)q

(cid:16)

(cid:17)

n

c min

pÎ³, n
log p

4 , k0 (cid:28) min{k1,

Theorem 6. Suppose 0 < Î± < 1

constant c > 0 such that
(3.18)

for some constants c > 0 and 0 â‰¤ Î³ < 1

âˆš
log p} and k1 â‰¤ k2 â‰¤
tor (cid:98)Î² satisfying the assumption (A2) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, then there is some
2 . For any estima-
(cid:16)

âˆš
log p (cid:28) k2 (cid:46) n
log p ;
âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n
In particular, if p â‰¥ n and (cid:98)Î²L is the Lasso estimator deï¬ned in (2.10) with
log p .

Î˜0(k1), Î˜0(k2),(cid:98)Î², (cid:96)q

if k1 â‰¤ k2 (cid:46)
if k1 (cid:46)

log p
n Ïƒ2
0
1âˆš
n Ïƒ2

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

(cid:17) â‰¥

q âˆ’1
q âˆ’1

2
q
ck
2
2

ck
2
2

ck
2

k1

log p
n Ïƒ2
0

âˆš
n
log p ;

Lâˆ—

Î±

if

n

n

0

âˆš

A > 4

2, the above lower bounds can be achieved.

The lower bounds of Theorem 6 imply that of Theorem 5 and both lower
bounds hold for a general class of estimators satisfying the assumption (A2).
However, the lower bound (3.18) in Theorem 6 has a signiï¬cantly diï¬€erent

18

T. T. CAI AND Z. GUO

meaning from (3.13) in Theorem 5 where (3.18) quantiï¬es the cost of adapta-

tion without knowing the sparsity level. For the Lasso estimator (cid:98)Î²L deï¬ned

in (2.10), by comparing Theorem 5 and Theorem 6, we obtain

(cid:16)

Lâˆ—

Î±

Î˜0(k1), Î˜0(k2),(cid:98)Î²L, (cid:96)q

(cid:17) (cid:29) Lâˆ—

Î±

(cid:16)

Î˜0(k1),(cid:98)Î²L, (cid:96)q

(cid:17)

if k1 (cid:28) k2,

n

that the conï¬dence interval CI0

âˆš
log p (cid:28) k (cid:46) n
log p .

which implies the impossibility of constructing adaptive conï¬dence intervals
for the case 1 â‰¤ q < 2. There exists marked diï¬€erence between the case
1 â‰¤ q < 2 and the case q = 2, where it is possible to construct adaptive
conï¬dence intervals over the regime

For the Lasso estimator (cid:98)Î²L deï¬ned in (2.10), it is shown in Proposition 3
ï£¶ï£¸ ,

Î± (Z, k2, q) deï¬ned in (3.15) achieves the lower
2
1âˆš
n Ïƒ2
q
bound k
2
of (3.18) can be achieved by the following proposed conï¬dence interval,
(3.19)

0 of (3.18). The lower bounds k
2

ï£«ï£­(cid:32)

Î± (Z, k2, q) =

0 and k

log p
n Ïƒ2

log p
n Ïƒ2

, (16k2)

âˆ’ Ïƒ2

âˆ’ Ïƒ2

(cid:33)

(cid:32)

(cid:33)

2

q âˆ’1

2

q âˆ’1

CI2

2

q âˆ’1

2

k1

0

0

0

Ïˆ (Z)
Ï‡2
1âˆ’ Î±

2

1
n2

(n2)

Ïˆ (Z)
Ï‡2

(n2)

Î±
2

1
n2

+

+

where Ïˆ (Z) is deï¬ned in (3.9). The following proposition veriï¬es the above
claim.

Proposition 4. Suppose p â‰¥ n, k1 â‰¤ k2 (cid:46) n

âˆš

tor deï¬ned in (2.10) with A > 4
satisï¬es,

n,pâ†’âˆ inf
lim inf

2. Then CI2

PÎ¸

Î¸âˆˆÎ˜0(k2)

(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
Î± (Z, k2, q) , Î˜0 (k1)(cid:1) (cid:46) k

q âˆˆ CI2
(cid:18)

q âˆ’1

2

2

L(cid:0)CI2

(3.20)

and

(3.21)

Î± (Z, k2, q)

log p and (cid:98)Î²L is the estima-

Î± (Z, k2, q) deï¬ned in (3.19)

(cid:17) â‰¥ 1 âˆ’ Î±,
(cid:19)

Ïƒ2
0.

k1

log p

n

+

1âˆš
n

4. Minimaxity and adaptivity of conï¬dence intervals over Î˜(k).
In this section, we focus on the case of unknown Î£ and Ïƒ and establish the
rates of convergence for the minimax expected length of conï¬dence intervals
q with 1 â‰¤ q â‰¤ 2 over the parameter space Î˜(k) deï¬ned in
(cid:17)
q. The
following theorem establishes the lower bounds for the benchmark quantities
Lâˆ—

for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(2.4). We also study the adaptivity of conï¬dence intervals for (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:16)
Î˜ (ki) ,(cid:98)Î², (cid:96)q

Î˜ (k1) , Î˜ (k2) ,(cid:98)Î², (cid:96)q

with i = 1, 2 and Lâˆ—

(cid:16)

(cid:17)

.

Î±

Î±

2
q
2

log p

n

.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

19

(cid:110)

(cid:111)

(4.1)

pÎ³, n
log p

k2 â‰¤ c min

Theorem 7. Suppose that 0 < Î± < 1

4 , 1 â‰¤ q â‰¤ 2, k0 (cid:28) k1 and k1 â‰¤
estimator (cid:98)Î² satisfying the assumption (A1) at Î¸0 = (Î²âˆ—, I, Ïƒ0) with (cid:107)Î²âˆ—(cid:107)0 â‰¤
2 . For any

for some constants c > 0 and 0 â‰¤ Î³ < 1

k0, then there is some constant c > 0 such that

Î˜ (ki) ,(cid:98)Î², (cid:96)q
(cid:17)

(cid:16)
(cid:17)(cid:17) â‰¥ ck
(cid:16){Î¸0} , Î˜ (k2) ,(cid:98)Î², (cid:96)q
In particular, if (cid:98)Î²SL is the scaled Lasso estimator deï¬ned in (2.14) with

CIÎ±((cid:98)Î²,(cid:96)q,Z)âˆˆIÎ±(Î˜(k2),(cid:98)Î²,(cid:96)q)

(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17) â‰¥ ck

and
(4.2)
Lâˆ—

i = 1, 2;

EÎ¸0L

(cid:16)

log p

Lâˆ—

CIÎ±

for

inf

2
q
i

=

n

Î±

Î±

,

âˆš

2, then the above lower bounds can be achieved,

A > 2

(4.3)

and

(4.4)

Lâˆ—

Î±

(cid:16)

(cid:17) (cid:16) k
Î˜ (ki) ,(cid:98)Î²SL, (cid:96)q
(cid:16){Î¸0} , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q

2
q
i

Lâˆ—

Î±

log p

n

(cid:17) (cid:16) ck

for

i = 1, 2;

2
q
2

log p

n

.

Î±

Î±

Î±

2
q
2

Lâˆ—

(cid:16)

log p
n .

The lower bounds (4.1) and (4.2) hold for any estimator satisfying the
assumption (A1) at an interior point Î¸0, including the scaled Lasso estimator
as a special case. We will demonstrate the impossibility of adaptivity of

conï¬dence intervals for the (cid:96)q loss of the scaled Lasso estimator (cid:98)Î²SL deï¬ned

in (2.14). By Theorem 7, we have Lâˆ—
Since Lâˆ—

(cid:16)
Î˜ (k1) , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q
Î˜ (k1) , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q

(cid:17) â‰¥ ck
(cid:17)

(cid:16){Î¸0} , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q
(cid:16){Î¸0} , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q
(cid:17) â‰¥ Lâˆ—
(cid:16)
(cid:17) (cid:29) Lâˆ—
Î˜ (k1) ,(cid:98)Î²SL, (cid:96)q
(cid:16)
(cid:17)
Î˜ (k1) ,(cid:98)Î²SL, (cid:96)q

and Lâˆ—

(cid:17)
Î˜ (k1) , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q

if k1 (cid:28) k2.

The comparision of Lâˆ—
is illustrated in Figure 4. Referring to the adaptivity deï¬ned in (3.4), it
is impossible to construct adaptive conï¬dence intervals for the loss of the

(cid:16)
scaled Lasso estimator (cid:98)Î²SL.
loss of any estimator (cid:98)Î² satisfying the assumption (A1), under the coverage
(cid:16)(cid:98)Î², (cid:96)q, Z

Theorem 7 shows that for any conï¬dence interval CIÎ±

Î˜ (k2) ,(cid:98)Î², (cid:96)q

(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17) âˆˆ IÎ±

, its expected length at

constraint that CIÎ±

for the

, then

(cid:17)

(cid:17)

(cid:16)

(cid:17)

Î±

Î±

Î±

Î±

20

T. T. CAI AND Z. GUO

4. The

(cid:16)
Î˜ (k1) , Î˜ (k2) ,(cid:98)Î²SL, (cid:96)q

ï¬gure

Fig
Lâˆ—

Î±

(cid:17)

demonstrates Lâˆ—

Î±

on the right.

(cid:16)

Î˜ (k1) ,(cid:98)Î²SL, (cid:96)q

(cid:17)

on

the

left

and

any given Î¸0 = (Î²âˆ—, I, Ïƒ) âˆˆ Î˜ (k0) must be of order k
log p
n . In contrast to
Theorem 4 and 6, Theorem 7 demonstrates that conï¬dence intervals must
be long at a large subset of points in the parameter space, not just at a small
number of â€œunluckyâ€ points. Therefore, the lack of adaptivity for conï¬dence
intervals is not due to the conservativeness of the minimax framework.

2
q
2

In the following, we detail the construction of conï¬dence intervals for
q. The construction of conï¬dence intervals is based on the following

(cid:107)(cid:98)Î²SLâˆ’Î²(cid:107)2

deï¬nition of restricted eigenvalue, which is introduced in [3],
(cid:107)XÎ´(cid:107)2
n(cid:107)Î´J01(cid:107)2

Îº(X, k, s, Î±0) = min
|J0|â‰¤k

min
Î´(cid:54)=0,
(cid:107)1â‰¤Î±0(cid:107)Î´J0(cid:107)1

J0âŠ‚{1,Â·Â·Â· ,p},

(4.5)

(cid:107)Î´Jc

âˆš

0

,

where J1 denotes the subset corresponding to the s largest in absolute value
coordinates of Î´ outside of J0 and J01 = J0 âˆª J1. Deï¬ne the event B =

{Ë†Ïƒ â‰¤ log p} . The conï¬dence interval for (cid:107)(cid:98)Î²SL âˆ’ Î²(cid:107)2
(cid:26) [0, Ï• (Z, k, q)]

(4.6)

CIÎ± (Z, k, q) =

q is deï¬ned as
on B
on Bc,

{0}

where

Ï• (Z, k, q) = min

ï£±ï£²ï£³
ï£«ï£­

nÎº2(cid:16)

2(cid:98)Ïƒ
(cid:16) max (cid:107)XÂ·j(cid:107)2

16Amax(cid:107)XÂ·j(cid:107)2
X, k, k, 3

min (cid:107)XÂ·j(cid:107)2

ï£¶ï£¸2

(cid:17)(cid:17)

(cid:18)

,

k

2
q

k

log p

n

2
q

log p

n

log p

(cid:19)(cid:98)Ïƒ2

ï£¼ï£½ï£¾ .

Properties for the coverage probability and the expected length of CIÎ± (Z, k, q)

are established as follows.

Î©Î˜ğ‘˜(cid:2869)Î˜ğ‘˜(cid:2870)ğ‘˜(cid:2869)(cid:2870)(cid:3044)logğ‘ğ‘›ğ‘˜(cid:2870)(cid:2870)(cid:3044)logğ‘ğ‘›Î˜ğ‘˜(cid:2869)HIGH-DIMENSIONAL ACCURACY ASSESSMENT

21

2. For 1 â‰¤ q â‰¤ 2, then CIÎ± (Z, k, q) deï¬ned in (4.6)

log p and (cid:98)Î²SL is the estimator deï¬ned in
(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

q âˆˆ CIÎ± (Z, k, q)

(cid:17)

= 1,

Proposition 5. Suppose k (cid:46) n

âˆš
(2.14) with A > 2
satisï¬es the following properties,

lim inf
n,pâ†’âˆ inf
Î¸âˆˆÎ˜(k)

PÎ¸

(4.7)

and

(4.8)

L (CIÎ± (Z, k, q) , Î˜ (k)) (cid:46) k

2
q

log p

n

.

Proposition 5 shows that the conï¬dence interval CIÎ± (Z, ki, q) deï¬ned in
(4.6) achieves the minimax convergence rate in (4.3), for i = 1, 2, and the
conï¬dence interval CIÎ± (Z, k2, q) deï¬ned in (4.6) achieves the minimax con-
vergence rate in (4.4).

5. General tools for minimax lower bounds. A major step in our
analysis is to establish rate sharp lower bounds for the minimax estimation
error and the expected length of minimax and adaptive conï¬dence inter-
vals for the (cid:96)q loss. We introduce in this section new technical tools that are
needed to establish these lower bounds. A signiï¬cant distinction of the lower
bound results given in the previous sections from those for the traditional
parameter estimation problems is that the constraint is on the performance

of the estimator(cid:98)Î² of the regression vector Î², but the lower bounds are on the
diï¬ƒculty of estimating its loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
dence intervals for the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

q. Conventional lower bound methods
are either not applicable or do not yield rate-optimal results. We introduce
useful lower bound techniques to establish rate-optimal lower bounds for
the minimax estimation error and the expected length of adaptive conï¬-
q. These technical tools may also be of

independent interest.

Before stating the technical tools, we ï¬rst introduce the following no-
tations used in the later discussion. Let Z denote a random variable with
parameter Î¸ âˆˆ Î˜ and let Ï€ denote a prior on the parameter space Î˜. We will
use fÎ¸(z) to denote the distribution of Z with the parameter Î¸ and fÏ€ (z) to
denote the marginal distribution of Z with a prior Ï€ on the parameter space
Î˜. For a function g, we shall write EÏ€ (g(Z)) for the expectation with respect
to the marginal distribution of Z with a prior Ï€ on Î˜. More speciï¬cally, we
write

(cid:90)

(cid:90)

(5.1)

fÏ€ (z) =

fÎ¸ (z) Ï€ (Î¸) dÎ¸

and EÏ€ (g(Z)) =

g (z) fÏ€ (z) dz.

T. T. CAI AND Z. GUO

function. The L1 distance between two probability distributions is given by

22
We will use PÏ€ to denote the probability distribution of Z corresponding

to fÏ€ (z), deï¬ned as, PÏ€ (A) =(cid:82) 1zâˆˆAfÏ€ (z) dz, where 1zâˆˆA is the indicator
L1(f1, f0) =(cid:82) |f1(z) âˆ’ f0(z)| dz.
under the constraint that (cid:98)Î² is a good estimator at at least one interior point.
4 , 1 â‰¤ q â‰¤ 2, Î¸0 =
a prior over the parameter space F. Assume the estimator (cid:98)Î² satisï¬es
(Î²âˆ—, I, Ïƒ0) âˆˆ Î˜ and F âŠ‚ Î˜. Deï¬ne d = minÎ¸âˆˆF (cid:107)Î² (Î¸) âˆ’ Î²âˆ—(cid:107)q. Let Ï€ denote

The following theorem establishes the minimax lower bounds for the esti-
mation error and the expected length of conï¬dence intervals for the (cid:96)q loss,

Theorem 8. Suppose 0 < Î± < 1

4 , 0 < Î±0 < 1

PÎ¸0

(cid:19)

(cid:18)
(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2
q â‰¤ 1
16
(cid:18)
|(cid:98)Lq âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

d2

â‰¥ 1 âˆ’ Î±0,

(5.2)

then

(5.3)

and
(5.4)
Lâˆ—

Î±

sup

PÎ¸

Î¸âˆˆ{Î¸0}âˆªF

inf(cid:98)Lq
(cid:17)
(cid:16){Î¸0} , Î˜,(cid:98)Î², (cid:96)q
CIÎ±((cid:98)Î²,(cid:96)q,Z)âˆˆIÎ±(Î˜,(cid:98)Î²,(cid:96)q)
(cid:110) 1
10 ,(cid:0) 9
10 âˆ’ Î±0 âˆ’ L1 (fÏ€, fÎ¸0)(cid:1)

inf

=

(cid:111)

+

d2

q| â‰¥ 1
4
(cid:16)

EÎ¸0L

CIÎ±

where Â¯c1 = min

â‰¥ Â¯c1,

(cid:19)
(cid:16)(cid:98)Î², (cid:96)q, Z

(cid:17)(cid:17) â‰¥ câˆ—

2d2,

and câˆ—

2 = 1

2 (1 âˆ’ 2Î± âˆ’ Î±0 âˆ’ 2L1 (fÏ€, fÎ¸0))+ .

Remark 1. The minimax lower bound (5.3) for the estimation error
and (5.4) for the expected length of conï¬dence intervals hold as long as the

estimator (cid:98)Î² estimates Î² well at an interior point Î¸0. Besides the assumption
bound (5.3), constraining that (cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2
loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
we will show that (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

(5.2), another key ingredient for the lower bounds (5.3) and (5.4) is to con-
struct the least favorable space F with the prior Ï€ such that the marginal
distributions fÏ€ and fÎ¸0 are non-distinguishable. For the estimation lower
q can be well estimated at Î¸0, due
to the non-distinguishability between fÏ€ and fÎ¸0, we can establish that the
q cannot be estimated well over F. For the lower bound (5.4),
by the assumption (5.2) and the non-distinguishability between fÏ€ and fÎ¸0,
q and hence

q over F is much larger than (cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

the honest conï¬dence intervals must be suï¬ƒciently long.

Theorem 8 is used to establish the minimax lower bounds for both the
estimation error and the expected length of conï¬dence intervals of the (cid:96)q loss

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

23

over Î˜(k). By taking Î¸0 âˆˆ Î˜(k0) and Î˜ = Î˜(k), Theorem 2 follows from
(5.3) with a properly constructed subspace F âŠ‚ Î˜(k). By taking Î¸0 âˆˆ Î˜(k0)
and Î˜ = Î˜(k2), Theorem 7 follows from (5.4) with a properly constructed
F âŠ‚ Î˜(k2). In both cases, the assumption (A1) implies Condition (5.2).

Furthermore, several minimax lower bounds over the parameter space
Î˜0(k) can also be implied by Theorem 8. For the estimation error, the
âˆš
minimax lower bounds (2.8) and (2.9) over the regime k (cid:46)
log p in Theorem
1 follow from (5.3). For the expected length of conï¬dence intervals, the
minimax lower bounds (3.7) in Theorem 4 and (3.18) in the region k1 â‰¤
k2 (cid:46)
log p in Theorem 6 follow from (5.4). In
these cases, the assumption (A1) or (A2) can guarantee that Condition (5.2)
is satisï¬ed.

âˆš
log p (cid:28) k2 (cid:46) n

âˆš
log p and k1 (cid:46)

n

n

n

n

gion

âˆš
log p (cid:28) k (cid:46) n

However, the minimax lower bound for estimation error (2.9) in the re-
log p and for the expected length of conï¬dence intervals
âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n
(3.18) in the region
log p cannot be established using the
above theorem. The following theorem, which requires testing a composite
null against a composite alternative, establishes the reï¬ned minimax lower
bounds over Î˜0(k).

n

Theorem 9. Suppose that 0 < Î± < 1

4 , 1 â‰¤ q â‰¤ 2 and
Î¸0 = (Î²âˆ—, I, Ïƒ0) âˆˆ Î˜0 (k0). Assume that for given disti and di, there exists
Fi âŠ‚ Î˜0 (ki) satisfying
(cid:107)Î² (Î¸) âˆ’ Î²âˆ—(cid:107)2 = disti
for Î¸ âˆˆ Fi where i = 1, 2.
Let Ï€i denote a prior over the parameter space Fi for i = 1, 2. Suppose that

(cid:107)Î² (Î¸)âˆ’ Î²âˆ—(cid:107)q = di

4 , 0 < Î±0 < 1

and

for Î¸1 =(cid:0)Î²âˆ—, I, Ïƒ2

(5.5)

PÎ¸i

0 + dist2
1

(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

(cid:1),

0 + dist2
2

q â‰¤ c2
i d2
i

(cid:1) and Î¸2 =(cid:0)Î²âˆ—, I, Ïƒ2
(cid:17) â‰¥ 1 âˆ’ Î±0,
(cid:16)|(cid:98)Lq âˆ’ (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
(cid:16)
(cid:17) â‰¥ câˆ—
(1 âˆ’ c2)2 d2
(cid:27)
(cid:17)
i=1 L1 (fÏ€i , fÎ¸i ) âˆ’ 2L1 (fÏ€2 , fÏ€1 )(cid:1)

4 âˆ’ (1 + c1)2 d2

q| â‰¥ câˆ—
3d2
2

, câˆ—

1
d2
2

4

+

sup

PÎ¸

Î¸âˆˆF1âˆªF2

inf(cid:98)Lq
(cid:16)
Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
(cid:26)
(cid:16)
(cid:110) 1
10 ,(cid:0) 9
10 âˆ’ 2Î±0 âˆ’(cid:80)2

(1 âˆ’ c2)2 âˆ’ 1

1
4 ,

(5.6)

and

(5.7)

Lâˆ—

Î±

where câˆ—

3 = min

and Â¯c3 = min

where c1 and c2 are positive constants. Then we have

for i = 1, 2,

(cid:17) â‰¥ Â¯c3,

(cid:17)
2 âˆ’ (1 + c1)2 d2
4 =(cid:0)1 âˆ’ 2Î±0 âˆ’ 2Î± âˆ’(cid:80)2
i=1 L1 (fÏ€i , fÎ¸i ) âˆ’ 2L1 (fÏ€2 , fÏ€1 )(cid:1)

+

1

,

+

(cid:111)

.

+

24

Remark 2. As long as the estimator (cid:98)Î² performs well at two points, Î¸1

T. T. CAI AND Z. GUO

and Î¸2, the minimax lower bounds (5.6) for the estimation error and (5.7)
for the expected length of conï¬dence intervals hold. Note that Î¸i in the
above theorem does not belong to the parameter space Î˜0 (ki), for i = 1, 2.
In contrast to Theorem 8, Theorem 9 compares composite hypothesises F1
and F2, which will lead to a sharper lower bound than comparing the sim-
ple null {Î¸0} with the composite alternative F. For simplicity, we construct
least favorable parameter spaces Fi such that the points in Fi is of ï¬xed
(cid:96)2 distance and ï¬xed (cid:96)q distance to Î²âˆ—, for i = 1, 2, respectively. More im-
portantly, we construct F1 with the prior Ï€1 and F2 with the prior Ï€2 such
that fÏ€1 and fÏ€2 are not distinguishable, where Î¸1 and Î¸2 are introduced to
facilitate the comparison. By the assumption (5.5) and the construction of
F1 and F2, we establish that the (cid:96)q loss cannot be simultaneously estimated
well over F1 and F2. For the lower bound (5.7), under the same conditions,
it is shown that the (cid:96)q loss over F1 and F2 are far apart and any conï¬-
dence interval with guaranteed coverage probability over F1 âˆª F2 must be
suï¬ƒciently long. Due to the prior information Î£ = I and Ïƒ = Ïƒ0, the lower
bound construction over Î˜0(k) is more involved than that over Î˜(k). We
shall stress that the construction of F1 and F2 and the comparison between
composite hypothesises are of independent interest.

The minimax lower bound (2.9) for the estimation error in the region
âˆš
log p (cid:28) k (cid:46) n
n
log p follows from (5.6) and the minimax lower bound (3.18)
âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n
in the region
log p for the expected length of conï¬dence
intervals follows from (5.7). In these cases, the assumption (A2) implies
Condition (5.5).

n

6. Discussion.

for the loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2

In this paper, we have investigated the minimax esti-
mation rate, minimax expected length and adaptivity of conï¬dence intervals
q with 1 â‰¤ q â‰¤ 2 over the parameter spaces Î˜0(k) and
Î˜(k). It is interesting to compare the minimaxity and adaptivity behav-
iors between loss estimation over the parameter spaces Î˜(k) and Î˜0(k).
The comparison shows signiï¬cant diï¬€erences between estimating the (cid:96)2 loss
and the (cid:96)q loss with 1 â‰¤ q < 2 as well as the diï¬€erences between the two
parameter spaces Î˜(k) and Î˜0(k).

(cid:110) k log p

(cid:111)

In terms of the minimax estimation rate and minimax expected length of
conï¬dence intervals, the prior information Î£ = I and Ïƒ = Ïƒ0 reduces the
.
With this prior information, the adaptive estimation of (cid:96)2 loss is made pos-
log p . In contrast, even with such prior
sible over the regime

convergence rate for the (cid:96)2 loss (cid:107)(cid:98)Î² âˆ’ Î²(cid:107)2
âˆš
log p (cid:28) k (cid:46) n

2 from k log p

n , 1âˆš

n

to min

n

n

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

25

information, the minimax convergence rate remains unchanged for the case
1 â‰¤ q < 2.

n

n

log p

n

to 1âˆš

âˆš
log p (cid:28) k (cid:46) n

Regarding adaptivity of conï¬dence intervals, the prior knowledge Î£ = I
and Ïƒ = Ïƒ0 is extremely useful for the construction of adaptive conï¬dence
intervals for the (cid:96)2 loss in the regime
log p . Though adaptivity is
still impossible outside this regime, we have seen that, with this prior knowl-
edge, the expected length of optimal conï¬dence intervals over the regime
k1 (cid:46)

is still impossible to construct adaptive conï¬dence intervals for (cid:107)(cid:98)Î²âˆ’Î²(cid:107)2

In contrast, for (cid:96)q loss with 1 â‰¤ q < 2, even with this prior information, it
q with
1 â‰¤ q < 2. However, a comparison of Theorem 6 with Theorem 7 reveals
that the expected length of conï¬dence intervals is reduced with such prior

âˆš
log p (cid:28) k2 (cid:46) n

log p is reduced from k2

2
q
knowledge, from k
2

n in the regime k1 (cid:46)
1âˆš

âˆš
log p (cid:28) k2 (cid:46) n
assumption (A1) or (A2), which basically requires the estimator(cid:98)Î² to perform
interesting to investigate estimation of the loss for more general estimator (cid:98)Î²

well at least at a point or a small subset of k-sparse parameter spaces. It is

The focus of this paper is on the collection of estimators satisfying the

âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n
log p .

in the regime

and from k

2

q âˆ’1

n
2

q âˆ’1

to k

2

to k

2

log p

n

n .

k1

log p

n

log p

log p

2
q
2

n

n

that does not satisfy the assumption (A1) or (A2). We leave this for future
research.

7. Proofs.

In this section, we present the proofs of the lower bound re-
sults. In Section 7.1, we establish the general lower bound result, Theorem
8. By applying Theorem 8 and Theorem 9, we establish Theorem 4, 6 in Sec-
tion 7.2 and Theorem 3, 5 in Section 7.3. For reasons of space, the proofs of
Theorem 1, 2, 7 and Theorem 9, the upper bound results, including Propo-
sition 1, 2, 3, 4 and 5 and the proofs of technical lemmas are postponed to
the supplement [5].

We deï¬ne the Ï‡2 distance between two density functions f1 and f0 by

(7.2)
Let PZ,Î¸âˆ¼Ï€ denote the joint probability of Z and Î¸ with the joint density
function f (Î¸, z) = fÎ¸ (z) Ï€ (Î¸) . We introduce the following lemma, which is
used in the proofs of Theorem 8 and Theorem 9. The proof of this lemma
can be found in the supplement [5].

Ï‡2(f1, f0).

(7.1)

Ï‡2(f1, f0) =

and it is well known that

(cid:90) f 2

dz =

dz âˆ’ 1

1 (z)
f0(z)

(cid:90) (f1(z) âˆ’ f0(z))2
L1(f1, f0) â‰¤(cid:112)

f0(z)

26

T. T. CAI AND Z. GUO

Lemma 1. For any event A, we have

(7.3)

PÏ€ (Z âˆˆ A) = PZ,Î¸âˆ¼Ï€ (Z âˆˆ A) ,

(cid:90)

and
(7.4)

(7.6)

|fÏ€1 (z) âˆ’ fÏ€2 (z)| dz = L1 (fÏ€2, fÏ€1) .
|PÏ€1 (Z âˆˆ A) âˆ’ PÏ€2 (Z âˆˆ A)| â‰¤
and PZ,Î¸âˆ¼Ï€(Z âˆˆ A) respectively, if there is no confusion. Recall that (cid:98)Lq(Z)
In the following proofs, we will write PÏ€(A) and PZ,Î¸âˆ¼Ï€(A) for PÏ€(Z âˆˆ A)

denotes the data-dependent loss estimator and Î²(Î¸) denotes the correspond-
ing Î² of the parameter Î¸.

7.1. Proof of Theorem 8. We will ï¬rst prove (5.3) and then prove (5.4).

4 and Î±1 = 1
10 .

We set c0 = 1
Proof of (5.3)
We assume

(7.5)

PÎ¸0

d2

4

(cid:19)

â‰¥ 1 âˆ’ Î±1.

Otherwise, we have

(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq(Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î²âˆ—(cid:107)2
(cid:12)(cid:12)(cid:12) â‰¤ 1
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq(Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î²âˆ—(cid:107)2
(cid:12)(cid:12)(cid:12) â‰¥ 1
(cid:26)
(cid:12)(cid:12)(cid:12)(cid:98)Lq(z) âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2
z : (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2
(cid:90)

(cid:27)
and hence (5.3) follows. Deï¬ne the event
(7.7) A0 =
.
By (5.2) and (7.5), we have PÎ¸0 (A0) â‰¥ 1 âˆ’ Î±0 âˆ’ Î±1. By (7.4), we obtain

(cid:12)(cid:12)(cid:12) â‰¤ 1

q â‰¤ c2

â‰¥ Î±1.

(cid:19)

d2

4

d2

4

0d2 ,

PÎ¸0

q

q

q

PÏ€ (A0) â‰¥ 1 âˆ’ Î±0 âˆ’ Î±1 âˆ’

|fÎ¸0 (z) âˆ’ fÏ€ (z)| dz.

(7.8)
For z âˆˆ A0 and Î¸ âˆˆ F, by triangle inequality,

(cid:107)(cid:98)Î²(z) âˆ’ Î²(Î¸)(cid:107)q â‰¥(cid:12)(cid:12)(cid:12)(cid:107)Î²(Î¸) âˆ’ Î²âˆ—(cid:107)q âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)q
q âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2

(cid:12)(cid:12)(cid:12) â‰¥(cid:12)(cid:12)(cid:12)(cid:107)(cid:98)Î²(z) âˆ’ Î² (Î¸)(cid:107)2

(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ c0) d.
(cid:12)(cid:12)(cid:12) âˆ’(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2

(7.9)
For z âˆˆ A0 and Î¸ âˆˆ F, we have
(7.10)

(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î² (Î¸)(cid:107)2

q

q

q

(cid:12)(cid:12)(cid:12)

â‰¥(1 âˆ’ 2c0 âˆ’ 1
4

)d2,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

27

where the ï¬rst inequality follows from triangle inequality and the last in-
equality follows from (7.7) and (7.9). Hence, for z âˆˆ A0, we obtain

(7.11)

Note that

sup
Î¸âˆˆF
â‰¥ sup
Î¸âˆˆF

PÏ€

inf
Î¸âˆˆF

(7.12) sup
Î¸âˆˆF

PÎ¸

(cid:16)(cid:98)Î², (cid:96)q, Z

For CIÎ±

(cid:19)

.

PÎ¸

PÎ¸

(cid:18)

inf
Î¸âˆˆF

q

q

q

4

)d2

4

)d2.

inf
Î¸âˆˆF

(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ 2c0 âˆ’ 1
(cid:19)
(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ 2c0 âˆ’ 1
(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ 2c0 âˆ’ 1
(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ 2c0 âˆ’ 1
(cid:12)(cid:12)(cid:12) â‰¥ (1 âˆ’ 2c0 âˆ’ 1

(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î² (Î¸)(cid:107)2
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î² (Î¸)(cid:107)2
(cid:18)
(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î² (Î¸)(cid:107)2
(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î² (Î¸)(cid:107)2
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) âˆ’ (cid:107)(cid:98)Î²(Z) âˆ’ Î² (Î¸)(cid:107)2
(cid:17) âˆˆ IÎ±
(cid:16)
(cid:17)
Î˜,(cid:98)Î², (cid:96)q
(cid:16)(cid:107)(cid:98)Î²(Z) âˆ’ Î² (Î¸)(cid:107)2
(cid:17)(cid:17) â‰¥ 1 âˆ’ Î±.
(cid:110)
z : (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)q < c0d, (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2

(cid:16)(cid:98)Î², (cid:96)q, Z

q âˆˆ CIÎ±

, we have

(cid:19)

PÎ¸

)d2

4

q

q

)d2

4

(cid:19)

)d2

.

4

Combining (7.6), (7.8) and (7.12), we establish (5.3).
Proof of (5.4)

Since the max risk is lower bounded by the Bayesian risk, we can further
lower bound the right hand side of above equation by

Combined with (7.11), we establish

â‰¥ PÏ€(A0).

inf
Î¸âˆˆÎ˜

(7.13)

(cid:17)(cid:111)
Deï¬ne the event A =
By (5.2) and (7.13), we have PÎ¸0 (A) â‰¥ 1 âˆ’ Î± âˆ’ Î±0. By (7.3) and (7.4), we
have

(cid:16)(cid:98)Î², L, z

q âˆˆ CIÎ±

.

(cid:17)(cid:111)
PZ,Î¸âˆ¼Ï€ (A) = PÏ€ (A) â‰¥ 1 âˆ’ Î± âˆ’ Î±0 âˆ’ L1 (fÏ€, fÎ¸0) .

(cid:110)
z : (cid:107)(cid:98)Î²(z) âˆ’ Î² (Î¸)(cid:107)2

(cid:16)(cid:98)Î², (cid:96)q, z

(7.14)
Deï¬ne the event BÎ¸ =
âˆªÎ¸âˆˆFBÎ¸. By (7.13), we have

(cid:19)

q âˆˆ CIÎ±
(cid:90) (cid:18)(cid:90)

(cid:90) (cid:18)(cid:90)

and M =

(cid:19)

PZ,Î¸âˆ¼Ï€ (M) =

1zâˆˆMfÎ¸(z)dz

Ï€ (Î¸) dÎ¸ â‰¥

1zâˆˆBÎ¸ fÎ¸(z)dz

Ï€ (Î¸) dÎ¸ â‰¥ 1âˆ’Î±.

28

T. T. CAI AND Z. GUO

Combined with (7.14), we have

;

(7.15)

q âˆˆ CIÎ±

q âˆˆ CIÎ±

(cid:17)
(cid:16)(cid:98)Î², (cid:96)q, z
PZ,Î¸âˆ¼Ï€ (A âˆ© M) â‰¥ 1 âˆ’ 2Î± âˆ’ Î±0 âˆ’ L1 (fÏ€, fÎ¸0) .
For z âˆˆ M, there exists Â¯Î¸ âˆˆ F such that (cid:107)(cid:98)Î²(z) âˆ’ Î²(Â¯Î¸)(cid:107)2
(cid:16)(cid:98)Î², (cid:96)q, z
(cid:17)
For z âˆˆ A, we have (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)2
and (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)q < c0d.
(cid:16)(cid:98)Î², (cid:96)q, z
(cid:17)
q,(cid:107)(cid:98)Î²(z)âˆ’ Î²âˆ—(cid:107)2
Hence, for z âˆˆ Aâˆ©M, we have (cid:107)(cid:98)Î²(z)âˆ’ Î²(Â¯Î¸)(cid:107)2
and (cid:107)(cid:98)Î²(z) âˆ’ Î²(Â¯Î¸)(cid:107)q â‰¥ (cid:107)Î²(Â¯Î¸) âˆ’ Î²âˆ—(cid:107)q âˆ’ (cid:107)(cid:98)Î²(z) âˆ’ Î²âˆ—(cid:107)q â‰¥ (1 âˆ’ c0) d and hence
(cid:16)(cid:98)Î², (cid:96)q, z
(cid:17)(cid:17) â‰¥ (1 âˆ’ 2c0) d2.
(cid:17)(cid:17) â‰¥ (1 âˆ’ 2c0) d2(cid:111)
(cid:16)(cid:98)Î², (cid:96)q, z
(cid:16)

Deï¬ne the event C =
have
(7.17) PÏ€ (C) = PZ,Î¸âˆ¼Ï€ (C) â‰¥ PZ,Î¸âˆ¼Ï€ (A âˆ© M) â‰¥ 1 âˆ’ 2Î± âˆ’ Î±0 âˆ’ L1 (fÏ€, fÎ¸0) .
By (7.4), we establish PÎ¸0 (C) â‰¥ 1 âˆ’ 2Î± âˆ’ Î±0 âˆ’ 2L1 (fÏ€, fÎ¸0) and hence (5.4).

. By (7.16), we

q âˆˆ CIÎ±

(7.16)

(cid:110)

(cid:16)

z : L

CIÎ±

CIÎ±

L

7.2. Proof of Theorems 4 and 6. We ï¬rst prove a general theorem for

1 â‰¤ q â‰¤ 2, which implies Theorem 4 and 6.

Theorem 10. Suppose 0 < Î± < 1

4 , k0 (cid:28) min{k1,

(cid:110)

(cid:111)

n

âˆš
2 . Suppose that (cid:98)Î²
log p} and k1 â‰¤ k2 â‰¤

pÎ³, n
log p

c min
satisï¬es the assumption (A2) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0.

for some constants c > 0 and 0 â‰¤ Î³ < 1

1. If k2 (cid:46)

âˆš
log p , then there is some constant c > 0 such that

n

Î±

Lâˆ—

(cid:16)
(7.18)
âˆš
log p (cid:28) k2 (cid:46) n
(cid:16)
Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q

(cid:17) â‰¥ ck
Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
ï£±ï£²ï£³
(cid:18)
(cid:17) â‰¥ c max

(7.19)
Lâˆ—

Î±

n

2
q
2

log p

n

Ïƒ2
0.

2. If

log p , then there is some constant c > 0 such that

(1 âˆ’ Î´âˆ—(n, p))k

2

q âˆ’1

2

k1

log p

n

âˆ’ k

2
q
1

log p

n

where lim sup Î´âˆ—(n, p) = 0.

q âˆ’1
2âˆš
In particular, the minimax lower bound (7.18) and the term k
n Ïƒ2
can be established under the weaker assumption (A1) with (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0.

2

0 in (7.19)

(cid:19)

,

+

2

q âˆ’1
2âˆš
k

n

ï£¼ï£½ï£¾ Ïƒ2

0,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

29

n

n

n

Lâˆ—

Î±

(7.20)

Î˜0 (kâˆ—

By Theorem 10, we establish (3.7) in Theorem 4 and (3.18) in Theorem
âˆš
6. In the regime k2 (cid:46)
log p , the lower bound (3.7) for q = 2 and (3.18) for
âˆš
log p (cid:28) k2 (cid:46)
1 â‰¤ q < 2 follow from (7.18). For the case q = 2, in the regime
n
log p , the ï¬rst term of the right hand side of (7.19) is 0 while the second
1 = min{k1, Î¶0k2} for
term is
some constant 0 < Î¶0 < 1, an application of (7.19) leads to

n , which leads to (3.7). For 1 â‰¤ q < 2, let kâˆ—
1âˆš
(cid:16)

ï£±ï£²ï£³k
(cid:17)
1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
(cid:40)
(cid:17) â‰¥ c max

(cid:17) â‰¥ c max
1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
If k1 â‰¤ Î¶0k2, (7.20) leads to the lower bounds (3.18) in the regions k1 (cid:46)
(cid:17) â‰¥ Lâˆ—
(cid:16)
âˆš
âˆš
log p (cid:28) k1 â‰¤ k2 (cid:46) n
log p ; if Î¶0k2 < k1 â‰¤ k2, by the fact
log p (cid:28) k2 (cid:46) n
that Lâˆ—
Î˜0 (kâˆ—
Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
1 =
Î¶0k2 â‰¥ Î¶0k1, we establish Lâˆ—
and hence the lower bounds (3.18) in the regions k1 (cid:46)
âˆš
The following lemma shows that (3.7) holds for (cid:98)Î²L deï¬ned in (2.10) with
log p (cid:28) k1 â‰¤ k2 (cid:46) n
2 by verifying the assumption (A1) and (3.18) holds for (cid:98)Î²L deï¬ned

Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q
(cid:16)

k1
âˆš
log p (cid:28) k2 (cid:46) n

ï£¼ï£½ï£¾ Ïƒ2

and kâˆ—
q âˆ’1

log p follow.

q âˆ’1
2âˆš
k

log p and

log p and

(cid:16)

2

q âˆ’1

log p

kâˆ—

1

âˆš

2

k

2

0.

,

n

Î±

2

n

n

n

Î±

Î±

n

2

âˆš
A >
in (2.10) with A > 4
lemma can be found in the supplement [5].

2 by verifying the assumption (A2). The proof of this
âˆš

Lemma 2.

2, then we have

(cid:19)

â‰¥ 1 âˆ’ c exp(cid:0)âˆ’c(cid:48)n(cid:1) âˆ’ pâˆ’c;

If A >

PÎ¸0
âˆš
2, then we have

(cid:18)
(cid:107)(cid:98)Î²L âˆ’ Î²âˆ—(cid:107)2
2 â‰¤ C(cid:107)Î²âˆ—(cid:107)0
(cid:18)
(cid:107)(cid:98)Î²L âˆ’ Î²âˆ—(cid:107)2
(cid:16)

PÎ¸

inf

If A > 4

{Î¸=(Î²âˆ—,I,Ïƒ):Ïƒâ‰¤2Ïƒ0}

into the following lower bounds,

(7.21)

log p

n

Ïƒ2
0

q â‰¤ C(cid:107)Î²âˆ—(cid:107) 2

q
0

(cid:41)

Ïƒ2
0

log p

2

q âˆ’1
2âˆš
n , k

n

(cid:19)

â‰¥ 1âˆ’c exp(cid:0)âˆ’c(cid:48)n(cid:1)âˆ’pâˆ’c.

log p

n

Ïƒ2

Proof of Theorem 10. The lower bound (7.19) can be further decomposed

Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q

Lâˆ—

Î±

(cid:17) â‰¥ ck

2

q âˆ’1

2

1âˆš
n

Ïƒ2
0,

(cid:16)

and
(7.22)
Lâˆ—

Î±

Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q

(cid:17) â‰¥ c

(cid:18)

(1 âˆ’ Î´âˆ—(n, p))k

2

q âˆ’1

2

k1

log p

n

âˆ’ k

2
1
1

log p

n

(cid:19)

+

Ïƒ2
0,

T. T. CAI AND Z. GUO

30
where lim sup Î´âˆ—(n, p) = 0. Hence, the proof of Theorem 10 is equivalent to
establishing (7.18), (7.21) and (7.22).
Proof of (7.18)
The proof of (7.18) is an application of (5.4) of Theorem 8. For Î¸0 =
(Î²âˆ—, I, Ïƒ0) âˆˆ Î˜0 (k0) , we construct
(7.23)

F = {(Î²âˆ— + Î´, I, Ïƒ0) : Î´ âˆˆ (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï)} âŠ‚ Î˜0 (k2) ,

where
(7.24)
(cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï) = {Î´ : supp (Î´) âŠ‚ supp (Î²âˆ—)c ,(cid:107)Î´(cid:107)0 = k2 âˆ’ k0, Î´i âˆˆ {0, Ï}} .
Let S = supp (Î²âˆ—). Without loss of generality, we assume S = {1, 2,Â·Â·Â· , k0}.
Let p1 denote the size of Sc and hence p1 = p âˆ’ k0. Let Ï€ denote the
uniform prior on the parameter space F, which is induced by the uniform
prior of Î´ on (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï). Under the Gaussian random design model,
Zi = (yi, XiÂ·) âˆˆ Rp+1 follows a joint Gaussian distribution with mean 0. Let
Î£z denote the covariance matrix of Zi. For the indices of Î£z, we use 0 as the
index of yi and {1,Â·Â·Â· , p} as the indices for (Xi1,Â·Â·Â· , Xip) âˆˆ Rp. Decompose
Î£z into blocks
xy denote the variance

(cid:18)Î£z

(cid:0)Î£z

xx and Î£z

, where Î£z

yy, Î£z

(cid:1)(cid:124)

(cid:19)

yy
Î£z
xy

xy
Î£z
xx

of y, the variance of X and the covariance of y and X, respectively. There
exists a bijective function h : Î£z â†’ (Î², Î£, Ïƒ) and the inverse mapping hâˆ’1 :
(Î², Î£, Ïƒ) â†’ Î£z, where hâˆ’1 ((Î², Î£, Ïƒ)) =

Î£Î² + Ïƒ2 Î²

and

Î£

(cid:124)

(cid:124)

(cid:19)

Î£

(cid:18)Î²
yy âˆ’(cid:0)Î£z

Î£Î²

xy

(cid:1)(cid:124)

(cid:17)

(cid:16)

ï£«ï£­ (cid:107)Î²âˆ—(cid:107)2

2 + (cid:107)Î´(cid:107)2

2 + Ïƒ2
0

ï£¶ï£¸ .

(cid:124)

(cid:124)

Î´

(Î²âˆ—
S)
Ik0Ã—k0 0k0Ã—p1
Ip1Ã—p1
0p1Ã—k0

(7.25)

h(Î£z) =

(Î£z

xx)

âˆ’1 Î£z

xy, Î£z

xx, Î£z

(Î£z

xx)

âˆ’1 Î£z

xy

.

Based on the bijection, the control of Ï‡2 (fÏ€, fÎ¸0) is reduced to the control
of the Ï‡2 distance between two multivariate Gaussian distributions.

The parameter spaces for Î£z corresponding to {Î¸0} and F are
(Î²âˆ—
01Ã—p1
S)
Ik0Ã—k0 0k0Ã—p1
Ip1Ã—p1
0p1Ã—k0

0} , where Î£z

2 + Ïƒ2
0
Î²âˆ—
S
0p1Ã—1

ï£«ï£­ (cid:107)Î²âˆ—(cid:107)2

H1 = {Î£z

0 =

(cid:124)

ï£¶ï£¸ ,

and

H2 = {Î£z

Î´ : Î´ âˆˆ (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï)} , where Î£z

Deï¬ne Î¸1 = (cid:0)Î²âˆ—, I, Ïƒ2
(k2 âˆ’ k0)Ï2 and hence Î¸1 = (cid:0)Î²âˆ—, I, Ïƒ2

(cid:1). For Î´ âˆˆ (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï), we have (cid:107)Î´(cid:107)2
0 + (k2 âˆ’ k0)Ï2(cid:1). By L1 (fÏ€, fÎ¸0) â‰¤

0 + (cid:107)Î´(cid:107)2

2 =

2

Î²âˆ—
S
Î´

Î´ =

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

31

L1 (fÏ€, fÎ¸1)+L1 (fÎ¸1, fÎ¸0), it is suï¬ƒcient to control L1 (fÏ€, fÎ¸1) and L1 (fÎ¸1, fÎ¸0) .
By (7.2), it is suï¬ƒcient to establish Ï‡2 (fÏ€, fÎ¸1) â‰¤ 2
1 and Ï‡2 (fÎ¸1, fÎ¸0) â‰¤ 2
. Let EÎ´,(cid:101)Î´ denote the expectation with respect to the
1,
independent random variables Î´,(cid:101)Î´ with a uniform prior over the parame-
where 1 = 1âˆ’2Î±âˆ’2Î±0
ter space (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï). The following two lemmas are useful to control
Ï‡2 (fÏ€, fÎ¸1) and Ï‡2 (fÎ¸1, fÎ¸0). The proof of Lemma 3 is given in the supple-
ment [5].

12

Lemma 3.

(7.26) Ï‡2 (fÏ€, fÎ¸1) + 1 = EÎ´,(cid:101)Î´

(cid:32)

1 âˆ’

(7.27)

Ï‡2 (fÎ¸1, fÎ¸0) + 1 = EÎ´,(cid:101)Î´

(cid:33)âˆ’ n

2

.

2(cid:32)
(cid:33)âˆ’ n

(cid:124)(cid:101)Î´

Î´
(cid:107)Î´(cid:107)2
2 + Ïƒ2
0

(cid:32)
1 âˆ’ (cid:107)Î´(cid:107)2

2(cid:107)(cid:101)Î´(cid:107)2

2

Ïƒ4
0

1 âˆ’

(cid:124)(cid:101)Î´
(cid:107)(cid:101)Î´(cid:107)2
(cid:33)âˆ’ n

2

Î´
2 + Ïƒ2
0

.

The following lemma (Lemma 3 in [6]) controls the right hand side of

(7.26).

Lemma 4. Suppose the random variable J follows Hypergeometric (p, k, k)

with P (J = j) =

, then we have

j)(pâˆ’k
(k
kâˆ’j)
(p
k)

(7.28)

E exp (tJ) â‰¤ e

k2
pâˆ’k

1 âˆ’ k
p

+

k
p

(cid:18)

(cid:19)k

exp (t)

.

By the construction (7.23), we have (cid:107)(cid:101)Î´(cid:107)2
1âˆ’x â‰¤ exp(2x) for x âˆˆ (cid:104)
(cid:124)(cid:101)Î´ â‰¤ (k2 âˆ’ k0)Ï2. By the inequality 1
(cid:33)âˆ’ n
(7.29)(cid:32)
(cid:124)(cid:101)Î´

(cid:105)
2 = (k2 âˆ’ k0)Ï2 and
, if
(cid:33)

(cid:33)âˆ’ n
2(cid:32)

Î´
(k2âˆ’k0)Ï2

2 = (cid:107)Î´(cid:107)2

2 , we have

0, log 2
2

(cid:124)(cid:101)Î´

< log 2

(cid:32)

Ïƒ2
0

Î´

2 â‰¤ exp

(cid:124)(cid:101)Î´
(cid:107)(cid:101)Î´(cid:107)2

Î´
2 + Ïƒ2
0

1 âˆ’

2n

(k2 âˆ’ k0)Ï2 + Ïƒ2

0

1 âˆ’

Î´
(cid:107)Î´(cid:107)2
2 + Ïƒ2
0

(cid:32)

2n

(cid:33)

.

(cid:124)(cid:101)Î´

Î´
Ïƒ2
0

â‰¤ exp

Let J denote the hypergeometric distribution with parameters (p1, k2 âˆ’

32

T. T. CAI AND Z. GUO

k0, k2 âˆ’ k0). We further have
(7.30)

(cid:32)

(cid:18)

E exp

2n

= EJ exp

(cid:33)
(cid:124)(cid:101)Î´
(cid:18)

Î´
Ïƒ2
0

â‰¤ e

(k2âˆ’k0)2
p1âˆ’(k2âˆ’k0)

1 âˆ’ k2 âˆ’ k0
(cid:114)

p1

+

p1

(cid:19)

(k2âˆ’k0)2
p1âˆ’(k2âˆ’k0)

JÏ2
2n
Ïƒ2
0
k2 âˆ’ k0

â‰¤ e

(cid:114) p1

p1

(k2 âˆ’ k0)2

(cid:18) 1

(cid:19)(cid:19)(k2âˆ’k0)

k2 âˆ’ k0

exp

(cid:18)

Ïƒ2
0

2nÏ2

(cid:19)(k2âˆ’k0)

,

1 +

1âˆš
p1

+

p1

(cid:18)
1 âˆ’ k2 âˆ’ k0
(cid:19)(k2âˆ’k0) â‰¤ e
(cid:110) n
log p , pÎ³(cid:111)

p1
(k2âˆ’k0)2
p1âˆ’(k2âˆ’k0)

where the ï¬rst inequality applies Lemma 4 and the second inequality follows

n

log

(k2âˆ’k0)2

Ïƒ0. If (k2 âˆ’ k0) â‰¤ c min

by plugging Ï = 1
2
suï¬ƒciently small positive constant c, we have (k2 âˆ’ k0)Ï2 < log 2
2 Ïƒ2
(k2 âˆ’ k0) â‰¤ cpÎ³ with 0 < Î³ < 1
(cid:107)(cid:101)Î´(cid:107)2
1. Since

2 , we have the following control of (7.27),

< log 2

2

for a

0. Since
(cid:107)Î´(cid:107)2
=
Ïƒ2
0

2

Ïƒ2
0

= (k2âˆ’k0)Ï2

(cid:32)
1 âˆ’ (cid:107)Î´(cid:107)2

2(cid:107)(cid:101)Î´(cid:107)2

2

Ïƒ2
0
(7.31)

EÎ´,(cid:101)Î´

Ïƒ4
0

(cid:33)âˆ’ n

2 â‰¤ exp

where the inequality follows from 1

(cid:19)2(cid:33)

2 , we have Ï‡2 (fÏ€, fÎ¸1) â‰¤ 2
(cid:32)

(cid:18) (k2 âˆ’ k0)Ï2
1âˆ’x â‰¤ exp(2x) for x âˆˆ(cid:104)
(cid:114)

= exp

Ïƒ2
0

n

log

p1

(cid:16)

ï£«ï£¬ï£­

(k2 âˆ’ k0)log
16n

(cid:105)

0, log 2
2

and the

(cid:17)2

ï£¶ï£·ï£¸ ,

p1

(k2âˆ’k0)2

n

n

(k2âˆ’k0)2
âˆš
log p , we have Ï‡2 (fÎ¸1, fÎ¸0) â‰¤ 2
(cid:18)
(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

equality follows by plugging in Ï = 1
Ïƒ0. Under the assump-
2
tion (k2 âˆ’ k0) â‰¤ c
1 and hence L1 (fÏ€, fÎ¸1) â‰¤
(cid:19)
(cid:17) 2
1, L1 (fÎ¸1, fÎ¸0) â‰¤ 1 and L1 (fÏ€, fÎ¸0) â‰¤ 21. Note that d = (k2 âˆ’ k0)
1
q Ï.
(cid:17) 1
By (2.6) and (cid:107)Î²âˆ—(cid:107)0 â‰¤ k0, we establish PÎ¸0
â‰¥
q d2
1 âˆ’ Î±0. By (5.4) and the fact C
Proof of (7.21)
The proof of (7.21) is based on the exactly same argument with (7.18) by

k2âˆ’k0
q (cid:28) 1, we establish (7.18).

(cid:16) k0

(cid:16) k0

q â‰¤ C

k2âˆ’k0

Ïƒ0. Since k2 (cid:29) âˆš

n

log p and k0 (cid:28) k2, then

taking Ï = (log(cid:0)1 + 2
(log(cid:0)1 + 2

(cid:1))

1
4

1

âˆš

(cid:1))

1
4

1
4

n

Ïƒ0 â‰¤ 1

Ïƒ0 and hence (7.30) holds. It is

p1

(k2âˆ’k0)2

n
suï¬ƒcient to control the following term

n

1
4

1

1
k2âˆ’k0

(cid:32)
1 âˆ’ (cid:107)Î´(cid:107)2

2(cid:107)(cid:101)Î´(cid:107)2

2

Ïƒ4
0

(7.32) EÎ´,(cid:101)Î´

(cid:32)

(cid:18) (k2 âˆ’ k0)Ï2

Ïƒ2
0

n

(cid:19)2(cid:33)

2 â‰¤ exp

â‰¤ 1 + 2
1.

1
k2âˆ’k0
log

âˆš

(cid:114)
(cid:33)âˆ’ n

2

(cid:113)
log(cid:0)1 + 2

(cid:1)(k2âˆ’k0)

In this case, d2 =

1 âˆ’ Î±0. Since k0 (cid:28) min{k1,

2

q âˆ’1 1âˆš

0 and PÎ¸0

n Ïƒ2

1
âˆš
log p} and k1 â‰¤ k2, we have C

n

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:32)
(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

q â‰¤ C

33

2
q
k
0

(k2âˆ’k0)

log p

n
2

q âˆ’1 1âˆš

n

2
q
k
0

log p

n
2

q âˆ’1 1âˆš

n

(k2âˆ’k0)
(cid:28) 1

(cid:33)

d2

â‰¥

(cid:32)

(cid:40)

F2 =

and the lower bound (7.21) follows from (5.4) of Theorem 8.
Proof of (7.22) The proof of (7.22) is an application of (5.7) of Theorem
9. The key is to construct parameter spaces F1, F2 and the points Î¸1 and Î¸2
and then control the distribution distances between the density functions.
For Î¸0 = (Î²âˆ—, I, Ïƒ0), we construct
(cid:33)(cid:41)
(cid:114) k1 âˆ’ k0
(7.33)F1 = {(Î²âˆ— + Î½, I, Ïƒ0) : Î½ âˆˆ (cid:96) (Î²âˆ—, k1 âˆ’ k0, Ï)} âŠ‚ Î˜0 (k1) ;
(cid:113) k1âˆ’k0
(cid:17)
ï£«ï£­ (cid:107)Î²âˆ—(cid:107)2

where (cid:96) (Î²âˆ—, k2 âˆ’ k0, Ï) and (cid:96)
are deï¬ned in (7.24).
Let Ï€i denote the uniform prior on the parameter space Fi for i = 1, 2. The
corresponding parameter spaces for Î£z corresponding to F1 and F2 are
(Î²âˆ—
S)
Ik0Ã—k0
0p1Ã—k0

(Î²âˆ— + Î´, I, Ïƒ0) : Î´ âˆˆ (cid:96)

Î²âˆ—, k2 âˆ’ k0,

Î²âˆ—, k2 âˆ’ k0,

âŠ‚ Î˜0 (k2) ,

H1 = {Î£z

2 + (cid:107)Î½(cid:107)2

k2 âˆ’ k0

2 + Ïƒ2
0

k2âˆ’k0

Î²âˆ—
S
Î½

(cid:16)

Î½ =

Ï

Ï

(cid:124)

Î½ : Î½ âˆˆ (cid:96) (Î²âˆ—, k1 âˆ’ k0, Ï)} , where Î£z
(cid:33)(cid:41)

(cid:114) k1 âˆ’ k0

(cid:32)

Î´ : Î´ âˆˆ (cid:96)
Î£z

Î²âˆ—, k2 âˆ’ k0,

(cid:40)

k2 âˆ’ k0

and

H2 =

ï£«ï£­ (cid:107)Î²âˆ—(cid:107)2

2 + (cid:107)Î´(cid:107)2

2 + Ïƒ2
0

Î²âˆ—
S
Î´

Ï

, where Î£z

Î´ =

0 + dist2

2). Since dist2

argument of (7.26) in Lemma 3, we have

Deï¬ne Î¸1 = (Î²âˆ—, I, Ïƒ2
(cid:107)Î´(cid:107)2

(cid:0)Î²âˆ—, I, Ïƒ2
0 + (k1 âˆ’ k0)Ï2(cid:1). In this case, we have L1 (fÎ¸2, fÎ¸1) = 0. By the same
Ï‡2 (fÏ€1, fÎ¸1) + 1 = EÎ½,(cid:101)Î½

1) and Î¸2 = (Î²âˆ—, I, Ïƒ2
0 + dist2
1 =
2 = (cid:107)Î½(cid:107)2
2 = (k1 âˆ’ k0)Ï2 and dist2
2 = (k1 âˆ’ k0)Ï2, we have Î¸1 = Î¸2 =
(cid:19)âˆ’ n
2(cid:18)
(cid:18)
(cid:124)(cid:101)Î½
(cid:33)âˆ’ n
(cid:32)
2(cid:32)

(cid:19)âˆ’ n
(cid:33)âˆ’ n

Î½
(cid:107)Î´(cid:107)2
2 + Ïƒ2
0

Î½
2 + Ïƒ2
0

1 âˆ’

1 âˆ’

and

,

2

2

Ï‡2 (fÏ€2, fÎ¸2) + 1 = EÎ´,(cid:101)Î´

1 âˆ’

Î´
(cid:107)Î´(cid:107)2
2 + Ïƒ2
0

1 âˆ’

(cid:124)(cid:101)Î½
(cid:107)(cid:101)Î½(cid:107)2
(cid:124)(cid:101)Î´
(cid:107)(cid:101)Î´(cid:107)2

Î´
2 + Ïƒ2
0

(cid:124)(cid:101)Î´

.

ï£¶ï£¸ .

(cid:124)

Î½

0k0Ã—p1
Ip1Ã—p1

(cid:124)

(Î²âˆ—
S)
Ik0Ã—k0
0p1Ã—k0

(cid:124)

Î´

0k0Ã—p1
Ip1Ã—p1

ï£¶ï£¸ .

1
q
Ck
0
q âˆ’ 1
1

2 (k1âˆ’k0)

(cid:28)

1
2

(k2âˆ’k0)

2

q âˆ’1(k1 âˆ’ k0)Ï2 âˆ’ (1 + c1)2 (k1 âˆ’ k0)

q Ï2(cid:17)

2

.

+

34

(cid:114)

log

p1

(k2âˆ’k0)2

T. T. CAI AND Z. GUO

Ïƒ0, a similar argument to (7.29) and (7.30) leads
2 = (k2 âˆ’
q â‰¤ c2
i d2
i

(cid:16)(cid:107)(cid:98)Î² âˆ’ Î²âˆ—(cid:107)2

1 = (k1 âˆ’ k0)

(cid:17) â‰¥

q Ï2, d2

2

n

q âˆ’1(k1 âˆ’ k0)Ï2. The assumption (2.7) leads to PÎ¸i
(cid:28) 1 and c2 =

Taking Ï = 1
2
to L1 (fÏ€i, fÎ¸i) â‰¤ 1 for i = 1, 2. Note that d2
k0)
1âˆ’ Î±0, for i = 1, 2, where c1 = Ck
(k1âˆ’k0)
1. By (5.7), we obtain
Lâˆ—

Î˜0 (k1) , Î˜0 (k2) ,(cid:98)Î², (cid:96)q

(1 âˆ’ c2)2 (k2 âˆ’ k0)

(cid:17) â‰¥ c

(cid:16)

(cid:16)

1
q
0

1
q

2

Î±

Since k0 (cid:28) min{k1,

âˆš
log p} and k1 â‰¤ k2, we establish (7.22).

n

7.3. Proof of Theorems 3 and 5. The minimax lower bound of Theorem
3 follows from Theorem 4. We take k1 = k2 = k and (3.5) follows from
(3.7). The minimax lower bound (3.6) follows from (3.5) and Lemma 2.
The minimax lower bound of Theorem 5 follows from Theorem 6. We take
k1 = k2 = k and (3.13) follows from (3.18). The minimax lower bound (3.14)
follows from (3.13) and Lemma 2.

References.
[1] Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. Infor-

mation Theory, IEEE Transactions on, 58(4):1997â€“2017, 2012.

[2] Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal
recovery of sparse signals via conic programming. Biometrika, 98(4):791â€“806, 2011.
[3] Peter J Bickel, Yaâ€™acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of

lasso and dantzig selector. The Annals of Statistics, 37(4):1705â€“1732, 2009.

[4] Peter BÂ¨uhlmann and Sara Van De Geer. Statistics for high-dimensional data: meth-

ods, theory and applications. Springer Science & Business Media, 2011.

[5] T Tony Cai and Zijian Guo.

Supplement to â€œaccuracy assessment for high-

dimensional linear regressionâ€. 2016.

[6] T Tony Cai and Zijian Guo. Conï¬dence intervals for high-dimensional linear regres-

sion: Minimax rates and adaptivity. The Annals of Statistics, to appear.

[7] T. Tony Cai and Harrison H Zhou. A data-driven block thresholding approach to

wavelet estimation. The Annals of Statistics, 37(2):569â€“595, 2009.

[8] Emmanuel Cand`es and Terence Tao. The dantzig selector: statistical estimation when

p is much larger than n. The Annals of Statistics, 35(6):2313â€“2351, 2007.

[9] David L Donoho and Iain M Johnstone. Adapting to unknown smoothness via wavelet
shrinkage. Journal of the American Statistical Association, 90(432):1200â€“1224, 1995.
[10] David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity
Information Theory, IEEE Transactions

phase transition in compressed sensing.
on, 57(10):6920â€“6941, 2011.

[11] Lucas Janson, Rina Foygel Barber, and Emmanuel Cand`es. Eigenprism: Inference

for high-dimensional signal-to-noise ratios. arXiv preprint arXiv:1505.02097, 2015.

[12] Ker-Chau Li. From steinâ€™s unbiased risk estimates to the method of generalized cross

validation. The Annals of Statistics, 13(4):1352â€“1377, 1985.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

35

[13] Richard Nickl and Sara van de Geer. Conï¬dence sets in sparse regression. The Annals

of Statistics, 41(6):2852â€“2876, 2013.

[14] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation
for high-dimensional linear regression over-balls. Information Theory, IEEE Trans-
actions on, 57(10):6976â€“6994, 2011.

[15] Charles M Stein. Estimation of the mean of a multivariate normal distribution. The

Annals of Statistics, 9(6):1135â€“1151, 1981.

[16] Tingni Sun and Cun-Hui Zhang.

Scaled sparse linear regression. Biometrika,

101(2):269â€“284, 2012.

[17] Christos Thrampoulidis, Ashkan Panahi, and Babak Hassibi. Asymptotically exact

error analysis for the generalized (cid:96)2

2-lasso. arXiv preprint arXiv:1502.06287, 2015.

[18] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society. Series B (Methodological), 58(1):267â€“288, 1996.

[19] Nicolas Verzelen. Minimax risks for sparse regressions: Ultra-high dimensional phe-

nomenons. Electronic Journal of Statistics, 6:38â€“90, 2012.

[20] Fei Ye and Cun-Hui Zhang. Rate minimaxity of the lasso and dantzig selector for the
(cid:96)q loss in (cid:96)r balls. The Journal of Machine Learning Research, 11:3519â€“3540, 2010.
[21] Feng Yi and Hui Zou. SURE-tuned tapering estimation of large covariance matrices.

Computational Statistics & Data Analysis, 58:339â€“351, 2013.

DEPARTMENT OF STATISTICS
THE WHARTON SCHOOL
UNIVERSITY OF PENNSYLVANIA
PHILADELPHIA, PENNSYLVANIA 19104
USA
E-mail: tcai@wharton.upenn.edu
URL: http://www-stat.wharton.upenn.edu/âˆ¼tcai/

zijguo@wharton.upenn.edu

