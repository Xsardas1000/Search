6
1
0
2

 
r
a

 

M
1
1

 
 
]
I

A
.
s
c
[
 
 

1
v
5
1
5
3
0

.

3
0
6
1
:
v
i
X
r
a

Dimension Coupling: Optimal Active Learning

of Halfspaces via Query Synthesis

Lin Chen1, Hamed Hassani2, and Amin Karbasi1

1 Department of Electrical Engineering, Yale University

2 Computer Science Department, ETH Zurich

March 14, 2016

Abstract

In this paper, we consider the problem of actively learning a linear classiï¬er
through query synthesis where the learner can construct artiï¬cial queries in order
to estimate the true decision boundaries. This problem has recently gained a lot of
interest in automated science and adversarial reverse engineering for which only
heuristic algorithms are known. In such applications, queries can be constructed
de novo to elicit information (e.g., automated science) or to evade detection with
minimal cost (e.g., adversarial reverse engineering).

We develop a general framework, called dimension coupling (DC), that 1) re-
duces a d-dimensional learning problem to d âˆ’ 1 low-dimensional sub-problems,
2) solves each sub-problem efï¬ciently, and 3) appropriately aggregates the results
and outputs a linear classiï¬er. We consider the three most common scenarios in
the literature: idealized noise-free, independent noise realizations, and agnostic
settings. We show that the DC framework avoids the curse of dimensionality: its
computational complexity in all three cases scales linearly with the dimension.
Moreover, in the noiseless and noisy cases, we show that the query complexity of
DC is near optimal (within a constant factor of the optimum algorithm). We also
develop an agnostic variant of DC for which we provide strong theoretical guaran-
tees. To further support our theoretical analysis, we compare the performance of
DC with the existing work in all three settings. We observe that DC consistently
outperforms the prior arts in terms of query complexity while often running orders
of magnitude faster.

Introduction

In contrast to the passive model of supervised learning, where all the labels are pro-
vided without any interactions with the learning mechanism, the key insight in active
learning is that the learning algorithm can perform signiï¬cantly better if it is allowed to
choose which data points to label. This approach has found far-reaching applications,
including the classical problems in AI (e.g., classiï¬cation [17], information retrieval
[16], speech recognition [8]) as well as the modern ones (e.g., interactive recommender
systems [10], optimal decision making [9]).

1

achieve an estimation error of Ç« [6].

In statistical learning theory, one assumes that a set of hypotheses H along with a
set of unlabeled data points X are given, where each data point x âˆˆ X is drawn i.i.d.
from some distribution D. Classical probably approximately correct (PAC) bounds
then yield the sample complexity (i.e., the number of required i.i.d. examples) from D
to output a hypothesis h âˆˆ H that will have estimation error at most Ç« with probability
at least 1 âˆ’ Î´, for some ï¬xed Ç«, Î´ > 0. Here, the estimation error is deï¬ned as error(h) =

case of learning a halfspace, i.e., when hâˆ— âˆˆ Rd perfectly separates the data points into
a linear separator with an estimation error Ç«. In contrast, a simple counting argument

Prxâˆ¼D[h(x) â‰  hâˆ—(x)], where hâˆ— is the unknown true hypothesis. In the realizable
positive and negative labels, it is known that with ËœO(d~Ç«)1 i.i.d. samples one can ï¬nd
based on sphere packing shows that any algorithm needs â„¦(d log(1~Ç«)) examples to

If one is not careful, active learning may require more samples than passive learn-
ing to achieve the same estimation error. One setting that is guaranteed to perform at
least as well as passive learning is pool-based active learning [12]: a set of unlabeled
examples are drawn i.i.d. where instead of obtaining all labels at once, the learning
algorithm sequentially decides which labels to request and which ones to discard. The
key challenge is to develop an algorithm that requests informative labels from the pool
in such a way that the remaining labels can be inferred as quickly as possible. A prin-
cipled way is to deï¬ne a version space V containing all hypotheses consistent with the
labels obtained so far and try to shrink it signiï¬cantly by requesting new labels from
the pool. An effective but computationally expensive method is the halving algorithm,
also referred to as generalized binary search (GBS), where at each iteration the next
example to label is chosen in such a way that it approximately cuts the version space
by one half. The new version space is constructed and the process is repeated until the
estimation error is less than Ç«. In many cases, computing the version space is compu-
tationally prohibitive. Dasgupta [6], in the idealized case that the labels are noise-free

ing this bound is generally intractable. Perhaps the most common query methods that
approximate the idealized halving algorithm are uncertainty sampling [12] and query-

and there exists a realizable linear separator, showed that one requires ËœO(d log2(1~Ç«))
labels, an exponential improvement over ËœO(d~Ç«). Unfortunately,the algorithm achiev-
by-committee (QBC) [7]. Freund et al. [7] showed that QBC requests ËœO(d log(1~Ç«))
reduction in sample complexity, requiring ËœO(d2 log(1~Ç«)) labels. Unfortunately, most

labels, but comes at the price of prohibitive computational cost in each iteration. Sim-
ilarly, Balcan et al. [4] showed that uncertainty sampling also achieves an exponential

approximation methods are only guaranteed to work if the pool size grows exponen-
tially fast in each iteration. Hence, one needs to always store a large amount of data in
addition to computing a complex version space.

An attractive alternative to the pool-based framework is query synthesis [3]: a
learner can request for any unlabeled data instance from the input space, including
queries that the learner synthesizes from scratch. This way the pool size limitation
is entirely eliminated. In many recent applications, ranging from automated science
[11], to robotics [5], and to adversarial reverse engineering [13], query synthesis is the
appropriate model. For instance, in security-sensitive applications (e.g., spam ï¬lters

1We use the ËœO notation to ignore terms that are logarithmic or dependent on Î´.

2

and intrusion detection systems) that routinely use machine learning tools, a growing
concern is the ability of adversarial attacks to identify the blind spots of the learning
algorithms. Concretely, classiï¬ers are commonly deployed to detect miscreant activ-
ities. However, they are attacked by adversaries who generate exploratory queries to
elicit information that in return allows them to evade detection [14]. In this work, we
show how an adversary can use active learning methods by making synthetically de
novo queries and thus identify the linear separator used for classiï¬cation.

We should emphasize that in active learning via synthesized queries the learning
algorithm can query the label of any points (irrespective of the underlying distribution
D) in order to explore the hypothesis space. However, for evaluating the performance
of the algorithm in terms of the estimation error, one may use the distribution D. This
is quite different from pool-based framework where queries should be among the data
points drawn i.i.d. from the underlying distribution D.

There is little known about the theoretical performance of actively learning a linear
classiï¬er via query synthesis. The closest work to our efforts is [2] where they pro-
pose a heuristic algorithm that approximates the version space by a convex body with
promising empirical results in the noise-free setting. Their method is not appropriate
for high dimensional data, as its computational complexity scales cubically with the
dimension. Moreover, their algorithm fails in the presence of noise.

In this paper, we develop a framework, called Dimension Coupling (DC), with the
following guarantees. In the noiseless setting, we show that its computational com-

ËœÇ«) and its query complexity is ËœO(d log 1

plexity is ËœO(d log 1
setting, we show that the computational complexity of DC is ËœO(d(log 1
its query complexity is ËœO(d(log 1

ËœÇ«). Similarly, in the noisy
ËœÎ´)2) and
ËœÎ´)). Note that in both settings the computa-

tional complexity scales linearly with the dimension. Moreover, the query complexity
in both settings is near-optimal. Finally, we generalize our results to the agnostic case.
Our empirical experiments demonstrate that DC runs orders of magnitude faster than
the existing methods.

ËœÇ« + log 1

ËœÇ« + log 1

Problem Formulation

in Rd. Let us emphasize here that the only information we obtain from a query is the

where ei is the ith standard basis vector, will only reveal the sign of the ith component
of hâˆ— (and nothing further about its value).

Our objective is to estimate an unknown halfspace Hâˆ— ={x âˆˆ Rd âˆ¶âŸ¨hâˆ—, xâŸ© > 0}, using
as few queries as possible. Here,âŸ¨â‹…, â‹…âŸ© is the standard inner product of the Euclidean
space (also â‹… denotes the Euclidean norm), hâˆ— is some (hidden) unit vector that we
want to estimate, and a generic query is of the form sign(âŸ¨hâˆ—, xâŸ©) where x is selected
sign of the inner product and not the value. E.g., the queries of the form sign(âŸ¨hâˆ—, eiâŸ©),
In the noiseless setting, we observe the true outcome of the query, i.e. signâŸ¨hâˆ—, xâŸ© âˆˆ
{1, âˆ’1}. In the noisy setting, the outcome is a ï¬‚ipped version of the true sign with
independent ï¬‚ip probability Ï. That is, denoting the outcome by Y we have Y âˆˆ{âˆ’1, 1}
and Pr[Y â‰  signâŸ¨hâˆ—, xâŸ©] âˆ¶= Ï < 1~2.
we only query the points on the unit sphere Sdâˆ’1 ={x âˆˆ Rd âˆ¶x = 1}. Hence, we term

Since the length of the selected vector x will not affect the outcome of the query,

3

X = Sdâˆ’1 as the query space. All possible unit vectors also reside on the unit sphere
Sdâˆ’1. Therefore, the initial version space, denoted by H, is also Sdâˆ’1.
As aforementioned, to evaluate the performance of halfspace learning algorithms,
we assume that unlabeled data points are drawn from the distribution D with a con-

Ç«, Î´ > 0, we seek an algorithm that (i) adaptively selects vectors x1, x2, â‹¯, (ii) observes

tinuous probability density function fD. The estimation error is deï¬ned as error(h) =
Prxâˆ¼D[h(x) â‰  hâˆ—(x)] = âˆ« 1{h(x) â‰  hâˆ—(x)}fD(x)dx â‰¤ 4MD arcsin(h âˆ’ hâˆ—~2),
where MD = maxx fD(x). To make sure the estimation error error(h) â‰¤ Ç« , it suf-
ï¬ces to ensureh âˆ’ hâˆ— < Ç«D, where Ç«D = 2 sin[Ç«~(4MD)]; e.g., when D is uni-
form, Ç«D = 2 sin(Ï€Ç«~2). Therefore, hereinafter we only discuss how to guarantee
h âˆ’ hâˆ— < Ç«D with high probability (say, at least 1 âˆ’ Î´). We report all the results
in terms ofh âˆ’ hâˆ— but it is easy to convert them to the estimation error. Thus, given
the (noisy) responses to each query signâŸ¨hâˆ—, xiâŸ©, (iii) and outputs, using as few queries
as possible, an estimate Ë†h of hâˆ— such thatË†h âˆ’ hâˆ— < Ç« with probability at least 1 âˆ’ Î´.
i=1 is an arbitrarily chosen
i = 1).
i=1 within a given precision by using

Suppose that hâˆ— has the form hâˆ— = âˆ‘d
orthonormal basis for Rd. We assume w.l.o.g. that hâˆ— is normalised (i.e., âˆ‘d
the (noisy) responses to the selected sign queries. The key insight here is that this
task can be partitioned in a divide-and-conquer fashion into many smaller tasks, each

Our objective is then to learn the coefï¬cients{ci}d
involving a few dimensions. The ï¬nal answer (the values of{ci}d

i=1) will then be
obtained by aggregating the answers of these subproblems.
Example 1. Assume hâˆ— = c1e1 + c2e2 + c3e3 + c4e4, where eiâ€™s are the standard basis
vectors for R4. Deï¬ne

Dimension Coupling Based Framework

i=1 ciei, where{ei}d

i=1 c2

Ë†e1 = c1e1 + c2e2
+ c2
2

, Ë†e2 = c3e3 + c4e4
+ c2
4

c2

3

1

c2
2Ë†e1 +c2

3

Note here that Ë†e1 is the (normalised) orthogonal projection of hâˆ— onto span{e1, e2}
and Ë†e2 is the (normalised) orthogonal projection of hâˆ— onto span{e3, e4}. Consider
relation hâˆ— =c2

the following procedure to learn hâˆ—: ï¬rst ï¬nd out what Ë†e1 and Ë†e2 are, and then use the
4Ë†e2 to ï¬nd hâˆ— based on the orthonormal vectors
Ë†e1, Ë†e2. By this procedure, the original â€œfour-dimensionalâ€ problem has been broken
into three â€œtwo-dimensionalâ€ problems.

+ c2

+ c2

1

For general d, the idea is similar: We break the problem into at most d âˆ’ 1 â€œtwo-
dimensionalâ€ problems that each can be solved efï¬ciently. More formally, let us as-

sume that we have an algorithm, called DC2(e1, e2, Ç«, Î´), that takes as input two or-

thonormal vectors e1, e2 and outputs with probability at least 1 âˆ’ Î´ a vector Ë†e with the
following three properties:

Ë†e âˆˆ span{e1, e2},Ë†e = 1,Ë†e âˆ’ hâˆ—,e1e1+hâˆ—,e2e2

âŸ¨hâˆ—,e1âŸ©e1+âŸ¨hâˆ—,e2âŸ©e2 < Ç«.

In other words, the unit vector Ë†e is within a distance Ç« to the (normalised) projection of

hâˆ— onto the subspace span{e1, e2}. In the next section, we will explain in detail how

4

,

+ c2
2j

(1)

hâˆ— =

d
Q
i=1

ciei =

Ë†cj

c2jâˆ’1e2jâˆ’1 + c2je2j

to design an optimal candidate for DC2 that uses (noisy) responses to queries of the

form signâŸ¨x, hâˆ—âŸ©. In the current section, we explain a framework DC that estimates
hâˆ— using at most d âˆ’ 1 calls to DC2 (a formal description is given in Algorithm 1).
Consider the decomposition hâˆ— = âˆ‘d
i=1 ciei. For simplicity assume that d is an even
number. We can write
d~2
Q
j=1

c2
where in the last step we have taken Ë†cj â‰œc2
is the (normalised) orthogonal projection of hâˆ— onto span{e2jâˆ’1, e2j}. Hence, by using
DC2(e2jâˆ’1, e2j, Ç«, Î´) we can obtain, with probability at least 1 âˆ’ Î´, a good approxima-
j=1 Ë†cj Ë†ej. Since hâˆ— is now expressed (approximately) in terms of d~2 known or-
thonormal vectors{Ë†ej}d~2
j=1, we have effectively reduced the dimensionality of problem
from d to d~2. The idea is then to repeat the same procedure as in (1) to the newly
obtained representation of hâˆ—. Hence, by repeating this procedure at most log2 d times
we will reach a vector which is the ï¬nal approximation of hâˆ—.
Algorithm 1 Dimension Coupling (DC)

tion Ë†ej (within a distance Ç«) of this projection. Therefore, for small enough Ç« we have
hâˆ— â‰ˆ âˆ‘d~2

2j. Now, note that c2jâˆ’1e2jâˆ’1+c2j e2j

2jâˆ’1
+ c2

c2

2jâˆ’1+c2

2j

2jâˆ’1

while d > 2

Input: an orthonormal basis E ={e1, e2, . . . , ed} of Rd.
1. for j = 1 toâŒŠd~2âŒ‹
Replace the two vectors e2jâˆ’1, e2j in E with the vector DC2(e2jâˆ’1, e2j, Ç«, Î´).
2. Set dâ†âŒˆd~2âŒ‰ .
return DC2(e1, e2, Ç«, Î´).

end while

end for

Theorem 2. For DC (outlined in Algorithm 1) we have:

1. DC will call the two-dimensional subroutine DC2 at most d âˆ’ 1 times.

2. Provided that the output of DC2 is with probability 1 âˆ’ Î´ within distance Ç« of
6d , DC ensures an estimation error of at most 6Ç«d with

the true value and Ç« â‰¤ 1
probability at least 1 âˆ’ Î´d.

We defer the proof of all theorems to the appendix. As a result of Theorem 2, if we
desire the framework DC to estimate hâˆ— within distance ËœÇ« and with probability at least
6d and Î´ = ËœÎ´
1 âˆ’ ËœÎ´, then it is enough to ï¬x the corresponding parameters of DC2 to Ç« = ËœÇ«
d .

5

DC2: Solving in 2 Dimensions

.

(2)

+ c2

2 =

1

by hâŠ¥, i.e.,

Before illustrating the algorithm DC2, let us review some notation. Given two or-

signâŸ¨x, hâˆ—âŸ© = signâŸ¨x, hâŠ¥âŸ© .

hâŠ¥ = âŸ¨hâˆ—, e1âŸ© e1 +âŸ¨hâˆ—, e2âŸ© e2
âŸ¨hâˆ—, e1âŸ© e1 +âŸ¨hâˆ—, e2âŸ© e22

thonormal vectors e1, e2 we denote the (normalised) projection of hâˆ— onto span{e1, e2}
The objective of DC2(e1, e2, Ç«, Î´) is to ï¬nd a unit vector Ë†e âˆˆ span{e1, e2} such that
Ë†e âˆ’ hâŠ¥ < Ç«. In fact, we require the latter to hold with probability at least 1 âˆ’ Î´.
Any unit vector inside span{e1, e2}, e.g., hâŠ¥, can equivalently be represented as a
pair(c1, c2) on the two-dimensional unit circle S1 (e.g., hâŠ¥ = c1e1 + c2e2 and c2
1). To simplify notation, we use a point(c1, c2) âˆˆ S1 and its corresponding unit vector
c1e1+c2e2 interchangeably. In this setting, it is easy to see that for any x âˆˆ span{e1, e2}

(3)
We take a Bayesian approach. In the beginning, when no queries have been performed,
DC2 assumes no prior information about the vector hâŠ¥. Therefore, it takes the uniform
2Ï€ ) as its prior belief about hâŠ¥. After performing
each query, the posterior (belief) about hâŠ¥ will be updated according to the observation.
this manner, DC2 runs in total of TÇ«,Î´ rounds, where in each round a speciï¬c query
is selected and posed to the oracle. The number TÇ«,Î´ will be speciï¬ed later (see The-
orems 3 and 4). Upon the completion of round TÇ«,Î´, the algorithm returns as its ï¬nal

distribution on S1 (with pdf p0(h) = 1
We let pm(h) denote the (pdf of the) posterior after performing the ï¬rst m queries. In
output a vector Ë†e âˆˆ S1 that maximises the posterior pdf pTÇ«,Î´(h). If there are multiple

such maximisers, it picks one arbitrarily. We now proceed with a detailed description
of DC2 (a formal description is provided in Algorithm 2). We ï¬rst consider the simpler
noise-free case and the other settings will follow afterwards.
Noise-Free Case: We explain DC2 (outlined in Algorithm 2) with the help of a run-
ning example given in Figure 1. As we will see, after each round of DC2 the possible
region that hâŠ¥ can belong to will be â€œhalvedâ€.
We ï¬rst note that as the initial distribution p0 is assumed to be the uniform distri-
bution on S1, the vector x1 (see step 2-(a) of Algorithm 2) can indeed be any point on
the unit circle S1. Thus, DC2 chooses x1 arbitrarily on S1. By (3), using the query

easy to verify that only half of S1 can possibly contain hâŠ¥ (see Figure 1). Let us denote

signâŸ¨x1, hâˆ—âŸ© will also give us the value of signâŸ¨x1, hâŠ¥âŸ©. Depending on this value, it is
this region by R1. Hence, the probability distribution p1(h) (which is our current be-
lief about hâŠ¥) is updated as follows: for h âˆ‰ R1 we have that p1(h) = 0, and as all the
points inside the half-circle R1 are equiprobable, we have for h âˆˆ R1 that p1(h) = 1~Ï€.
In other words, at time m = 0 the vector hâŠ¥ could have been anywhere on the unit
circle, but, after round m = 1 it can only belong to the half-circle R1. Thus, after the
ï¬rst round, DC2 â€œhalvesâ€ the admissible region of hâŠ¥. Continuing in this theme, it is
not hard to verify that (see Figure 1) at round m = 2 the value of p2(h) is non-zero and
uniform only on a region R2 which is a quarter-circle. In an inductive manner, letting
Rmâˆ’1 denote the admissible region (sector) at round m âˆ’ 1 (see Figure 1) and assum-
ing that pmâˆ’1 is only non-zero and uniform on the sector Rmâˆ’1, then xm at round m is

6

Algorithm 2 DC2
Input: orthonormal vectors e1, e2, estimation error at most Ç«, success probability at
least 1 âˆ’ Î´.

1. Set p0(h) to be uniform, i.e., âˆ€h âˆˆ S1 âˆ¶ p0(h) = 1~2Ï€.

2. for m = 1 to TÇ«,Î´

arbitrarily.

(a) ï¬nd a vector xm âˆˆ S1 which is a solution to the following equation:

âˆ«S 1 signâŸ¨x, hâŸ© pmâˆ’1(h)dh = 0. If there are multiple solutions, choose one
(b) ask from the oracle the value of signâŸ¨xm, hâˆ—âŸ©.
pmâˆ’1(h) to pm(h).

(c) based on the response obtained from the oracle, update the distribution

end for

3. return Ë†e = arg maxhâˆˆS 1 pTÇ«,Î´(h).

Figure 1: An example to illustrate DC2 in the noiseless setting. In the ï¬rst round, x1 is arbitrarily chosen
on S 1. For the choice in the ï¬gure, we have sign âŸ¨x1, hâˆ—âŸ© = sign âŸ¨x1, hâŠ¥âŸ© = âˆ’1. For any point h above the
red line we have that sign âŸ¨x, hâŸ© = âˆ’1 and for the points outside this half-circle the result is +1. Therefore,
the distribution (pdf of) p1 is uniform on the region above the red line and is zero below it. For round m = 2
it is easy to see that the direction of x2 should be along the red line. For x2 chosen as in the ï¬gure, we
have sign âŸ¨x2, hâˆ—âŸ© = +1 and hence at the end of the second round DC2 concludes that the vector hâŠ¥ could
uniformly be any point inside R2. In a generic round m, any vector orthogonal to the mid-point of sector
Rmâˆ’1 can be considered as a candidate for xm. For the choice in the ï¬gure, we have sign âŸ¨xm, hâŠ¥âŸ© = âˆ’1.
Thus, at the end of round m, DC2 concludes that hâŠ¥ can uniformly be any point inside Rm.

precisely the vector that is orthogonal to the midpoint of the sector Rmâˆ’1. Therefore,
after observing the value of signâŸ¨xm, hâˆ—âŸ©, the admissible region Rm is the better half
of Rmâˆ’1 that is compatible with the observation (i.e., it contains hâŠ¥). Also, Rm is again
a sector and pm will be uniform on Rm and zero outside. It is also easy to see that the
circular angle for the sector Rm is Ï€
Theorem 3. Consider DC in the absence of noise (Ï = 0). If we let TÇ«,Î´ =âŒˆlog2
Ç«âŒ‰,

then it outputs a vector that is within a distance Ç« of hâŠ¥.

2m . The following statement is now immediate.

Ï€

7

A few comments are in order: The above guarantee for DC2 holds with probability
one and thus the parameter Î´ is irrelevant in the noiseless setting. Furthermore, during
each round of DC2, the distribution pm can be represented by only two numbers (the
starting and ending points of the sector Rm), and the vector xm can be computed ef-
ï¬ciently (it is the orthogonal vector to the midpoint of Rm). Therefore, assuming one
unit of complexity for performing the queries, DC2 can be implemented with complex-

ity O(TÇ«,Î´). Finally, by using Theorem 2, we conclude that DC requires ËœO(d log 1
Ç«)
queries with computational complexity ËœO(d log 1
Ç«).

In general, DC2 follows a similar procedure as in the noiseless case
Noisy Case:
except that the distributions pm does not look as simple. However, as we now dis-
cuss, these distributions can still be stored efï¬ciently and as a result the vector xm
can be computed efï¬ciently. Indeed, (the pdf of) pm is piecewise constant on the unit
circle (see Figure 2). More precisely, at any round m, there are at most 2m points
u1, u2, â‹¯, u2m that are ordered clock-wise on the unit-circle and pm is constant when

We refer to Figure 2 for a schematic illustration. Once a noisy response to the query

step 2-(a) of Algorithm 2), DC2 ï¬rst ï¬nds a line that passes through the centre of S1 and
cuts S1 into two â€œhalvesâ€ which have the same measure with respect to pm. Note that

restricted to each of the sectors[ui, ui+1). At round m + 1, in order to ï¬nd xm+1 (see
ï¬nding such a line can be done in O(m) steps because pm has the piecewise constant
property. Once such a line is found, it is then easy to see that xm+1 can be any of the two
points orthogonal to the line. As a result, DC2 at round m + 1 can ï¬nd xm+1 in O(m)
operations. We denote the half-circle containing xm+1 by R+ and the other half by Râˆ’.
signâŸ¨xm+1, hâˆ—âŸ© is obtained, the probability distribution pm will be updated to pm+1 in
the following way. First, consider the event that the outcome of signâŸ¨xm+1, hâˆ—âŸ© is +1.
We have pm(signâŸ¨xm+1, hâˆ—âŸ© = +1) =(1âˆ’Ï) pm(R+)+Ï pm(Râˆ’) = 1~2, and similarly
pm(signâŸ¨xm+1, hâˆ—âŸ© = +1) = 1~2. Therefore, by Bayes theorem we obtain the follow-
ing update rules for pm+1. If we observe that signâŸ¨xm+1, hâˆ—âŸ© = +1, then for h âˆˆ R+ we
have pm+1(h) = 2(1 âˆ’ Ï)pm(h) and for h âˆˆ Râˆ’ we have pm+1(h) =(2Ï)pm(h). Also,
if we observe that signâŸ¨xm+1, hâˆ—âŸ© = âˆ’1, then for h âˆˆ R+ âˆ¶ pm+1(h) =(2Ï)pm(h) and
for h âˆˆ Râˆ’ âˆ¶ pm+1(h) = 2(1 âˆ’ Ï)pm(h). (note that the factor of 2 here is due to the
normalization.) It is easy to verify that pm+1 is also a piecewise constant distribution
(now on 2(m + 1) sectors; see Fig. 2).
Theorem 4. In the noisy setting (with independent ï¬‚ip probability Ï), having

is sufï¬cient to guarantee that DC2 outputs with probability at least 1 âˆ’ Î´ a vector that
, T1 =

8 log 2
Î´

TÇ«,Î´ â‰¥ M + max{T0, T1, T2, T3} = O(log

8

8 log 1
8Ï€Ç«

, T2 =

log(2(1âˆ’Ï))

is within a distance Ç« of hâŠ¥. Here, we have M =âŒˆ
log(2(1âˆ’Ï))log(2M) + log(
Theorem 2 indicates that DC requires ËœO(d(log 1
cussed above, the computational complexity of DC2 is O(T 2
Î´)2).
putational complexity ËœO(d(log 1

+ log 1

+ log 1

4

Ç«

Ç«

8

(4)

1

1
Ç«

+ log

Î´)
âˆ’ log(4Ï(1âˆ’Ï))âŒ‰, T0 =
log(2(1âˆ’Ï))) and T3 = 24Ï log2 1âˆ’Ï

log(2(1âˆ’Ï))

2 log 2
Î´

Ï

Î´).
log2(2(1âˆ’Ï))log(M) + log( 4

Î´)) queries. Also, as dis-
Ç«,Î´). Hence, DC has com-

Figure 2: Upon the completion of round m (left ï¬gure), the distribution (pdf of) pm is constant over each
of the sectors [ui, ui+1). In the next round (right ï¬gure), in order to ï¬nd xm+1, DC2 ï¬rst ï¬nds a diagonal
line (red line) which separates two half-circles (R+ and Râˆ’) that each has measure 1~2 w.r.t pm. The vector
xm+1 will then be one of the two points on the unit circle that are orthogonal to this line. For updating pm
to pm+1, we note that all the points inside R+ get the same factor (either 2Ï or 2(1 âˆ’ Ï) depending on the
outcome of the query). The same is true for Râˆ’. Thus, pm+1 is again a piecewise constant pdf but now on
2(m + 1) sectors.

Agnostic Case: A common approach used in the agnostic setting is the empirical
risk minimization (ERM), which generates queries by independently sampling from
the marginal distribution of X, i.e., PX , and then selects a unit vector that minimizes
the number of errors made on these n queries. We follow a similar approach as in [15].
Suppose that we are given a query budget of n. We allocate n~3 queries to DC and let
h1 denote the unit vector selected by DC. Then we allocate n~3 queries to ERM and let
h2 denote the unit vector proposed by ERM using its n~3 queries. Consider the region
âˆ† that h1 and h2 disagree on, i.e., âˆ† ={x âˆˆ Sdâˆ’1 âˆ¶ h1(x) â‰  h2(x)}. Let Pâˆ† be the
restriction of the probability measure PX on âˆ†. Here we sample the remaining n~3
queries from âˆ† according to Pâˆ†. We write Ë†Râˆ†(hi) for the average number of errors
made by hi on the sampled n~3 queries. Finally, we set Râˆ†(hi) = E[ Ë†Râˆ†(hi)]. The
Ë†Râˆ†(h) as the ï¬nal result.
algorithm will output Ë†h = arg minhâˆˆ{h1,h2}
bility in expectation E[R(Ë†h)] â‰¤ min{E[R(h1), E[R(h2)]} + 2 3

Theorem 5. Using the above procedure we have the following bounded error proba-
ne , where n is the

number of queries and e is the base of the natural logarithm.

Empirical Results

In this section, we extensively evaluate the performance of DC against the following
baselines:

â€¢ RANDOM-SAMPLING: Queries are generated by sampling uniformly at random
from the unit sphere Sdâˆ’1.
â€¢ UNCERTAINTY-SAMPLING: Queries are sampled uniformly at random from the

orthogonal complement of w, where w is the vector learned by linear SVM.

9

100

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

10-1

10-2

10-3

50

Random
Uncertainty
Bagging
Spectral
DC

100

150

Number of Queries

100

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

10-1

10-2

10-3

200

Random
Uncertainty
Bagging
Spectral
DC

100

150

200

250

300

Number of Queries

100

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

10-1

10-2

10-3

10-4

350

400

450

Random
Uncertainty
Bagging
Spectral
DC

200

400

600

Number of Queries

800

1000

(a) Noise-free (d = 25)

(b) Noise-free (d = 50)

(c) Noise-free (d = 100)

Random
Uncertainty
Bagging
Spectral
DC

102

100

10-2

10-4

i

 

e
m
T
n
o
i
t
u
c
e
x
E
e
v

 

i
t
a
l
e
R

10-6

10-8

d = 25

d = 50

Dimension

d = 100

(d) Execution time (noise-free)

Random
Uncertainty
Bagging
Repetitive
DC

102

100

10-2

i

 

e
m
T
n
o
i
t
u
c
e
x
E
e
v

 

l

i
t
a
e
R

10-4

10-6

Random
Uncertainty
Bagging
DC
Repetitive

700

800

100

200

300
Number of Queries

400

500

600

(e) Noisy (d = 25)

DC (noise = 0.01)
DC (noise = 0.1)
DC (noise = 0.2)
Rep. (noise = 0.01)
Rep. (noise = 0.1)
Rep. (noise = 0.2)

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

101

100

10-1

10-2

10-3

10-4

101

100

10-1

10-2

10-3

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

 

r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

101

100

10-1

10-2

10-3

10-4

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

y
t
i
l
i

b
a
b
o
r
P
 
r
o
r
r
E

Random
Uncertainty
Bagging
DC
Repetitive

200

400

600

800 1000 1200 1400 1600 1800

Number of Queries

(f) Noisy (d = 50)

Random
Uncertainty
Bagging
DC

600

700

d = 25

Dimension

d = 50

10-4

0

2

4

6

Number of Queries

8

10
# 104

0.05

0

100

200

300

400

500

Number of Queries

(g) Execution time (noisy)

(h) Noisy (d = 1000)

(i) Agnostic (d = 25)

Figure 3: Figures 3a, 3b, and 3c show the estimation error as we increase the number
of queries, for d = 25, 50, 100, respectively. Fig. 3d shows the corresponding execution
times. Fig. 3e and 3f show the estimation error as we increase the number of queries for
d = 25, 50 and the noise level Ï = 0.1. The corresponding execution times are shown
in Fig. 3g. Fig. 3h presents the estimation error of DC and REPETITIVE-DC as we
increase the number of queries for d = 1000 and noise levels Ï = 0.01, 0.1, 0.2. Finally,
Fig. 3i demonstrates the error probability w.r.t. the number of queries in the agnostic
setting for d = 25.

â€¢ QUERY-BY-BAGGING: The bag size is set to 20 and 1000 queries are generated

at each iteration. The query with the largest disagreement is picked [1].

â€¢ SPECTRAL: The version space is approximated by the largest ellipsoid consistent
with all previous query-label pairs. Then, at each iteration a query is selected to
approximately halve the ellipsoid [2].

â€¢ REPETITIVE-DC: In the noisy setting, one easy way to apply DC is to query

each point R times and use the majority rule to determine its label.

Our metrics to compare different algorithms are: a) estimation error, b) query com-
plexity, and c) execution time. In particular, as we increase the number of queries we

10

measure the average estimation errors and execution times for all the baselines (with
90% conï¬dence intervals). By nature, in active learning via query synthesis, all data
points and queries are generated synthetically. For all the baselines, we used the fastest
available implementations in MATLAB.

Noise-free setting: Figures 3a, 3b and 3c (with dimension d = 25, 50, 100, respec-
tively) show that in terms of estimation error, DC outperforms all other baselines,
and signiï¬cantly outperforms RANDOM-SAMPLING, UNCERTAINTY-SAMPLING and
QUERY-BY-BAGGING. Note that the estimation errors are plotted in log-scales. In
terms of execution times, we see in Fig. 3d that DC runs three orders of magni-
tude faster than other baselines. Training an SVM at each iteration for RANDOM-
SAMPLING, UNCERTAINTY-SAMPLING and QUERY-BY-BAGGING comes with a huge
computational cost. Similarly, SPECTRAL requires solving a convex optimization prob-
lem at each iteration; thus its performance drastically deteriorates as the dimension
increases, which makes it infeasible for many practical problems.

Noisy setting: We set the noise level to Ï = 0.1 and compare the performance of DC
against RANDOM-SAMPLING, UNCERTAINTY-SAMPLING, QUERY-BY-BAGGING, and
REPETITIVE-DC (for R = 5). As mentioned in [2], and we have also observed in our
experiments, SPECTRAL does not work even for small amounts of noise as it incor-
rectly shrinks the version space and misses the true linear separator. We see again in
Figures 3e and 3f (for d = 25, 50) that DC signiï¬cantly outperforms all other meth-
ods in terms of estimation error as we increase the number of queries. Figure 3g
shows that DC still runs âˆ¼100 times faster than RANDOM-SAMPLING, UNCERTAINTY-
SAMPLING, and QUERY-BY-BAGGING. Clearly, DC has a higher computational cost
than REPETITIVE-DC, as DC performs a Bayesian update after each query. Finally,
as we increase the dimension to d = 1000, RANDOM-SAMPLING, UNCERTAINTY-
SAMPLING, and QUERY-BY-BAGGING become signiï¬cantly slower. Hence, in Fig. 3h
we only show how the estimation error (for noise levels Ï = 0.01, 0.1, 0.2) decreases
for DC and REPETITIVE-DC with more queries.

Agnostic setting: We sample a set of size 1000 from the Von Misesâ€“Fisher dis-
tribution (the analog of the normal distribution on Sdâˆ’1) and compute the empiri-
cal error . The noise level at each point is drawn independently from the truncated

normal N(0.1, 0.5) on [0, 0.5]. Fig. 3i shows the error probability of RANDOM-

SAMPLING, UNCERTAINTY-SAMPLING, QUERY-BY-BAGGING and AGNOSTIC-DC
(for d = 25). Both AGNOSTIC-DC and UNCERTAINTY-SAMPLING achieve an error
probability âˆ¼0.1 after 700 queries. However, as shown by Theorem 5, AGNOSTIC-DC
has a strong theoretical guarantee which, by contrast, other baselines do not have.

11

References

[1] N. Abe and H. Mamitsuka. Query learning strategies using boosting and bagging.

In ICML, page 1. Morgan Kaufmann Pub, 1998.

[2] I. Alabdulmohsin, X. Gao, and X. Zhang. Efï¬cient active learning of halfspaces

via query synthesis. In AAAI 2015, 2015.

[3] D. Angluin. Queries and concept learning. Machine learning, 1988.

[4] M.-F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In Learning

Theory, pages 35â€“50. Springer, 2007.

[5] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical

models. JAIR, 1996.

[6] S. Dasgupta. Analysis of a greedy active learning strategy. Advances in neural

information processing systems, 17:337â€“344, 2005.

[7] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the

query by committee algorithm. Machine learning, 1997.

[8] D. Hakkani-Tur, G. Riccardi, and A. Gorin. Active learning for automatic speech

recognition. In ICASSP), volume 4, pages IVâ€“3904. IEEE, 2002.

[9] S. Javdani, Y. Chen, A. Karbasi, A. Krause, J. A. Bagnell, and S. Srinivasa. Near

optimal bayesian active learning for decision making. AISTAT, 2014.

[10] A. Karbasi, S. Ioannidis, and L. Massoulie. Comparison-based learning with rank

nets. ICML, 2012.

[11] R. D. e. a. King. The automation of science. Science, 2009.

[12] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classiï¬ers. In
Proceedings of the 17th annual international ACM SIGIR conference on Research
and development in information retrieval, 1994.

[13] D. Lowd and C. Meek. Adversarial learning. In KDD, pages 641â€“647. ACM,

2005.

[14] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. Ty-

gar. Query strategies for evading convex-inducing classiï¬ers. JMLR, 2012.

[15] R. Nowak. Noisy generalized binary search. Advances in neural information

processing systems, 21:1366â€“1374, 2009.

[16] S. Tong and E. Chang. Support vector machine active learning for image retrieval.
In Proceedings of the 9th ACM international conference on Multimedia, pages
107â€“118. ACM, 2001.

[17] S. Tong and D. Koller. Support vector machine active learning with applications

to text classiï¬cation. JMLR, 2:45â€“66, 2002.

12

Appendix A: Proof of Theorem 2

of DC2. As a result, if the probability of success for DC2 is at least 1 âˆ’ Î´, then by the

At each round of DC, the value of d is replaced byâŒˆ d
2âŒ‰. It is thus not hard to verify
that DC runs inâŒŠlog2 dâŒ‹ rounds until d â‰¤ 2 and in total there are at most d âˆ’ 1 usages
union bound the probability of success of DC is at least 1 âˆ’(d âˆ’ 1)Î´.
we run DC with an input being an orthonormal set{e1, e2, â‹¯, ed} where ei, hâˆ— âˆˆ RK
orthogonal projection of hâˆ— into span{e1, e2, â‹¯, ed}. More precisely, we deï¬ne

for some K â‰¥ d. We prove that DC outputs a vector that is close to the (normalised)

For the last part of the theorem, we prove a more general statement: Assume that

(5)

hâŠ¥ =

âˆ‘d

i=1âŸ¨ei, hâˆ—âŸ©ei
âŸ¨ei, hâˆ—âŸ©ei .

Then, DC runs in log2 d rounds, calls DC2 d âˆ’ 1 times, and outputs with probability at

above, we can conclude that DC runs in log2 d times and uses DC2 d âˆ’ 1 times. Also,
again by the union bound, with probability at least 1 âˆ’ Î´d all the outputs of DC2 are a
close estimate (within distance Ç«) of their corresponding objective. Thus, by assuming
that all the calls of DC2 have been successful (which happens w.p. at least 1 âˆ’ Î´d), we

least 1 âˆ’ Î´d a vector Ë†h for whichhâŠ¥ âˆ’ Ë†h < 6Ç«d. In exactly similar way as discussed
use an inductive argument to prove thathâŠ¥ âˆ’ Ë†h < 6Ç«d. We use induction on d. For

d = 2 the result is clear. We now prove the result when d = k assuming that it holds
for all d < k. For simplicity, we assume that k is an even number, i.e, k = 2t (the proof
follows very similarly for k being odd). We can then write

Ë†cjhâŠ¥j ,

(6)

hâŠ¥ =

=

ciei =

d
t
Q
Q
i=1
j=1
c2jâˆ’1ej+c2j e2j
c2
2jâˆ’1+c2

2j

+ c2

2jâˆ’1

2j and hâŠ¥j

where Ë†cj =c2
cisely the (normalised) orthogonal projection of hâˆ— (and also hâŠ¥) onto span{e2jâˆ’1, e2j}.
by the output of DC2(e2jâˆ’1, e2j, Ç«, Î´) which we denote by Ë†ej. Let us now deï¬ne the

As we explained in Section , in the ï¬rst round of DC each vector hâŠ¥j will be replaced

c2jâˆ’1ej+c2j e2j
c2
2jâˆ’1+c2

. Note that hâŠ¥j

vector Ë†hâŠ¥ as

is pre-

=

2j

It is easy to verify that  hâŠ¥1   = 1 as{Ë†e1, Ë†e2, â‹¯, Ë†et} is an orthonormal set. By the

assumption of the induction, the ï¬nal output of DC, which we denote by Ë†hâŠ¥, will
be within the distance 6Ç«t of Ë†hâŠ¥. That is,

hâŠ¥1

=

âˆ‘t

j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej
âˆ‘t
j=1âŸ¨hâŠ¥, Ë†ejâŸ©2

  hâŠ¥1
âˆ’ Ë†hâŠ¥   < 6Ç«t.
  hâŠ¥ âˆ’ hâŠ¥1   < 6Ç«t.

13

We now prove that

(7)

(8)

From (7) and (8) the induction hypothesis will be immediate as we can write

It thus remains to prove (8).

Firstly, we deï¬ne Î² â‰œâˆ‘t

  hâŠ¥ âˆ’ Ë†hâŠ¥   â‰¤  hâŠ¥ âˆ’ hâŠ¥1   +  hâŠ¥1

âˆ’ Ë†hâŠ¥   < 6Ç«t + 6Ç«t = 6Ç«d.

Î²

Î²

Î²

âˆ‘t

j=1âŸ¨hâŠ¥, Ë†ejâŸ©2. We have
j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej
  hâŠ¥ âˆ’ hâŠ¥1   =   hâŠ¥ âˆ’
  
j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej
=    Î²hâŠ¥ âˆ’ âˆ‘t
  
=   (Î² âˆ’ 1)hâŠ¥ + hâŠ¥ âˆ’ âˆ‘t
j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej
j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej
Î²   +   hâŠ¥ âˆ’ âˆ‘t
  .
â‰¤   1 âˆ’ Î²
j=1âŸ¨hâŠ¥, Ë†ejâŸ©2 âˆ’âŸ¨hâŠ¥, hâŠ¥jâŸ©2 
j=1 âŸ¨hâŠ¥, Ë†ejâŸ© âˆ’âŸ¨hâŠ¥, hâŠ¥jâŸ©  âŸ¨hâŠ¥, Ë†ejâŸ© +âŸ¨hâŠ¥, hâŠ¥jâŸ© 
j=1  hâŠ¥   â‹…  Ë†ej âˆ’ hâŠ¥j   â‹…(  hâŠ¥   â‹…  Ë†ej   +  hâŠ¥   â‹…  hâŠ¥j  )

â‰¤ 2Ç«t,

t
Q

t
Q

  

Î²

â‰¤

â‰¤

Secondly, we have

 Î²2 âˆ’ 1  =   t

Q

Î² âˆˆ[âˆš1 âˆ’ 2Ç«t,âˆš1 + 2Ç«t],
1âˆš1 âˆ’ 2Ç«t
  1 âˆ’ Î²
Î²   â‰¤ max{
j=1âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej   =   t
  hâŠ¥ âˆ’
j=1âŸ¨hâŠ¥, hâŠ¥jâŸ©hâŠ¥j

âˆ’ 1, 1 âˆ’

t
Q

Q

1âˆš1 + 2Ç«t} â‰¤ 2Ç«t.
âˆ’âŸ¨hâŠ¥, Ë†ejâŸ©Ë†ej   â‰¤ 2Ç«t.

Fourthly, similar as above we can show that

Now, by plugging (10), (11) and (12) into (9) we get (8).

(9)

(10)

(11)

(12)

where the last step follows from  hâŠ¥   =  hâŠ¥j   =  Ë†ej   = 1 and  Ë†ej âˆ’ hâŠ¥j   â‰¤ Ç«. Hence, by

noting the fact that 2Ç«t = Ç«d â‰¤ 1

6 we obtain

and

14

Appendix B: Proof of Theorem 4

quence. At the m-th round of DC2, if Î¶m = 1 (which takes place with independent

Let{Î¶n, n â‰¥ 1} be a sequence of independent and identically distributed (iid) Bernoulli(Ï)
random variables. Denote by(F , â„¦, Pr) the probability space generated by this se-
probability Ï) then we observe a ï¬‚ipped version of signâŸ¨xm, hâˆ—âŸ©. Also, if Î¶m = 0 we
observe the correct version of signâŸ¨xm, hâˆ—âŸ©.
Consider a query of the form signâŸ¨x, hâˆ—âŸ©. This query divides the unit circle into
two parts (half-circles) depending on the sign ofâŸ¨x, hâˆ—âŸ© (see Figure 4). The two parts
are: (i) Preferred part: all h such that signâŸ¨x, hâŸ© = signâŸ¨x, hâŠ¥âŸ©, and (ii) Unpreferred
part: all h such that signâŸ¨x, hâŸ© = âˆ’ signâŸ¨x, hâŠ¥âŸ©. The two parts can be separated by a

line â„“x that passes through the origin. We refer to Figure 4 for a schematic explanation.

the preferred part of the query. Otherwise, we say that the query does not prefer z.

Figure 4: For any point z above the line â„“x we haveâŸ¨z, hâŠ¥âŸ© = âŸ¨x, hâŠ¥âŸ©. Once we
perform the queryâŸ¨x, hâŠ¥âŸ©, it is more likely that the (noisy) response is indeed the true
valueâŸ¨x, hâŠ¥âŸ©. Therefore, the region above the line â„“x is in general preferred by the
query. In the ï¬gure, the sector(y, z) is cut by the line â„“x and the sector(z, x) is not.
Also,(z, x) lies in the preferred part of the queryâŸ¨x, hâŠ¥âŸ©.
In this setting, we say that the query signâŸ¨x, hâˆ—âŸ© prefers a point z if z belongs to
Also, we frequently use the line â„“x rather than the query signâŸ¨x, hâˆ—âŸ© when it causes no
ambiguity. Finally, for a region A on the unit circle say that the query signâŸ¨x, hâˆ—âŸ© cuts
deï¬ne the distance d(x, y) to be the length of the (smaller) sector between them (see
Figure 4). Clearly, we have d(x, y) â‰¥x âˆ’ y2.
At round m of DC2 a vector xm is chosen and the (noisy) outcome of signâŸ¨xm, hâˆ—âŸ©
unpreferred parts have equal measures under pmâˆ’1, i.e., pmâˆ’1(Fxm) = pmâˆ’1(Uxm) =
we conduct the query signâŸ¨xm, hâˆ—âŸ©. As the result of the query is noisy, we have two

the region A if and only if the line â„“x passes through region A. Otherwise, we say that
the query does not cut A. If â„“x does not cut A, then â„“x prefers A if A is in the preferred
part and does not prefer A otherwise (see Figure 4). Finally, for two points x, y we

2 . Let us see what happens to pm (the posterior belief about hâŠ¥ at round m) after

is observed. As explained in Section ??, xm is chosen in a way that the preferred and

1

15

follows

pm is updated as follows

if h âˆˆ Fxm,
if h âˆˆ Uxm.

different update rules depending on each of the following cases: (i) Î¶m = 0, i.e., we

observe the correct value signâŸ¨xm, hâˆ—âŸ©. In this case, the measure pm is updated as
(ii) Î¶m = 1, i.e., we observe the ï¬‚ipped value âˆ’ signâŸ¨xm, hâˆ—âŸ©. In this case, the measure

pm+1(h) =â§âªâªâ¨âªâªâ©
2(1 âˆ’ Ï)pm(h)
(2Ï)pm(h)
pm+1(h) =â§âªâªâ¨âªâªâ©(2Ï)pm(h)
2(1 âˆ’ Ï)pm(h)
Prâˆƒy âˆˆ S1 âˆ¶ d(y, hâŠ¥) > Ï and pTÇ«,Î´(y) â‰¥ pTÇ«,Î´(hâŠ¥)(cid:6) < Î´.

Clearly, the result of the theorem follows from (13). For better illustration, we assume

Consider the number TÇ«,Î´ given in (4). Our goal is to show that

w.l.o.g that hâŠ¥ =(0, 1). Consider a point y on the right-hand side of the unit circle
2 . Also, Consider points z0, zK such that d(z0, hâŠ¥) = Ç«~4 and
such that d(y, hâŠ¥) > Ç«
d(hâŠ¥, zK) = Ç«~2. We now divide the sector starting with z0 and ending with zK into
K âˆ¶= TÇ«,Î´ + 1 pints. That is, for i = 1, 2, â‹¯, K we denote by zi the point that d(hâŠ¥, zi) =
(see Figure 5). Also, for i â‰¥ 1, we let the sector starting with ziâˆ’1 and

if h âˆˆ Fxm,
if h âˆˆ Uxm.

(13)

+ i

Ç«
4

Ç«

4(TÇ«,Î´+1)

Figure 5: Different regions for the proof of Theorem 4.

Ç«

Ç«

8Ï€â‹…(TÇ«,Î´ +1)

(as Ai  =

ending with zi be denoted by Ai. Note that in the very beginning of the algorithm

DC2 has in total TÇ«,Î´ rounds and in each round m it conducts a query with an

when we have uniform measure on the unit circle, each of the regions Ai has p0(Ai) =
associated line â„“xm. We let M âˆ¶=âŒˆ
log 4(Ï(1âˆ’Ï))âŒ‰ and consider the following events:

â€¢ E1: There is at least M lines which separate zK from hâŠ¥ or equivalently, there

4(TÇ«,Î´ +1)

2 log 2
Î´

).

is at least M lines that cut the region(hâŠ¥, zK).

â€¢ E2,j (1 â‰¤ j â‰¤ K): The region Aj is not cut by any of the lines â„“1, â„“2, . . . , â„“TÇ«,Î´ .

16

1

(14)

âˆ© E2,j] .

2 and pTÇ«,Î´(y) â‰¥ pTÇ«,Î´(hâŠ¥).

pigeon-hole principle there is always a region Aj that is not cut by any of the lines. We
can write:

TÇ«,Î´ +1
Q
j=1
Now using Lemma 6 (stated below), we have

â€¢ E3: âˆƒy such that d(y, hâŠ¥) > Ï
j=1 E2,j(cid:6) = 1 as we have TÇ«,Î´ queries and hence by the
It is easy to see that Prâ‹ƒK
Pr[E3]
= Pr[E3 âˆ© E1] + Pr[E3 âˆ© Ec
1]
Pr[E3 âˆ© Ec
â‰¤ Pr[E3  E1] +
Pr[E3  E1] â‰¤ Pr[E3  E1] â‰¤(4Ï(1 âˆ’ Ï)) M
âˆ© E2,j] â‰¤ Pr[E2,j âˆ© Ec
1] ,
1] â‰¤(M âˆ’ 1)(Î¸1 + Î¸2),
1] â‰¤(TÇ«,Î´ + 1)M(Î¸1 + Î¸2),

Let us now bound Pr[E3 âˆ© Ec
and using the fact that E2,j  =

Pr[E3 âˆ© Ec
Pr[E2,j âˆ© Ec
Pr[E2,j âˆ© Ec

where Î¸1 and Î¸2 are given in Lemma 7 with mâ† TÏ,Î´ and kâ† M . Now, we show that
the above expression is upper bounded by Î´~2, and hence by using relations (14) and

(15), we get the proof of the main theorem.

The value of T0 is chosen in such a way that we have

âˆ© E2,j] . We have

we obtain from Lemma 7 that

TÇ«,Î´ +1
Q
j=1

4(TÇ«,Î´ +1)

and thus

(16)

(15)

2 â‰¤

Î´
2

.

1

1

Ï

.

.

4

4

log(2(1 âˆ’ Ï))
log(2(1 âˆ’ Ï))
log(1 âˆ’ 2Ï)
log(2(1 âˆ’ Ï))

2Ï log 1âˆ’Ï
Ï

2

.

2â«âªâªâªâ¬âªâªâªâ­
ââ 

(17)

(18)

(19)

(20)

â‰¤

Î´
4

.

Now, by plugging in (17)-(20) into the values of Î¸1 and Î¸2 in (16) we conclude that the
right side of (16) is bounded by Î´
2 .

T1 ensures that

T2 and ensures that

Finally, T3 ensures that

2 log(TÇ«,Î´ + 1)

TÇ«,Î´ âˆ’ M

â‰¤

2

TÇ«,Î´ âˆ’ M

log

8Ï€
Ç«

â‰¤

2M

TÇ«,Î´ âˆ’ M

log(2Ï) â‰¤
â›â

T âˆ’ M

6

âˆ’Ï

17

(TÇ«,Î´ + 1)M expâ§âªâªâªâ¨âªâªâªâ©

We have for Î² > 0 that

Lemma 6. Let x1, x2, â‹¯, xm be the vectors chosen by DC2 up to round m with Fxi

and Uxi being their associated preferred and unpreferred parts (i.e. piâˆ’1(Fxi) =
piâˆ’1(Uxi) = 1~2). Consider two points h1, h2 such that h1 âˆˆ âˆ©m
i=1Uxi.
Pr[pm(x) < pm(y)] â‰¤(4Ï(1 âˆ’ Ï))m .
Proof. For i âˆˆ[m], deï¬ne the random variable Zi as Zi â‰œ log pi(x)
(1 âˆ’ 2Î¶i) log 1âˆ’Ï

. Using the update
rules of pi that we explained above, it is easy to see that for i â‰¥ 1: Zi = Ziâˆ’1 +
Ï . Also, as p0 is uniform over S1 we have Z0 = 0. We thus have

i=1Fxi and h2 âˆˆ âˆ©m

Ï . Hence,

Zm = âˆ‘m

pi(y)

i=1(1 âˆ’ 2Î¶i) log 1âˆ’Ï

Ï

1 âˆ’ Ï

m
Q

Pr[Zm â‰¤ 0]
= Prlog
i=1(1 âˆ’ 2Î¶i) â‰¤ 0
= Pr m
2
â‰¤ (4Ï(1 âˆ’ Ï)) m

Q
i=1

Î¶i â‰¥

2 ,

1

where the last step follows directly from the so called Chernoff bound.

We note that the vector hâŠ¥ is always a member of the preferred part of any test. As

a result, at any round of DC2 we have that hâŠ¥ âˆˆ âˆ©m
Lemma 7. Consider a region A on the unit circle which does not contain hâŠ¥. As-
sume we are at round m of DC2 where a sequence of queries with associated lines
â„“x1 , â„“x2 , . . . , â„“xm have been conducted. We deï¬ne events E1 and E2 as

i=1Fxi.

â€¢ E1 â‰œ None of the lines â„“xi cuts A;

â€¢ E2 â‰œ At most k of the lines do not prefer A,

where k is an an integer. We have

where

and

6

mâˆ’k log 2Ï€

Pr[E1 âˆ© E2] â‰¤ k(Î¸1 + Î¸2),
 A )
log(2(1 âˆ’ Ï)) âˆ’ 2
â›â
Ï log( 1âˆ’Ï
Ï )
log(2(1 âˆ’ Ï)) + 2k
mâˆ’k log(2Ï)
â›â
Ï log( 1âˆ’Ï
Ï )

6

,

.

2â«âªâªâªâ¬âªâªâªâ­
ââ 
2â«âªâªâªâ¬âªâªâªâ­
ââ 

m âˆ’ k

âˆ’Ï

m âˆ’ k

âˆ’Ï

Î¸1 = expâ§âªâªâªâ¨âªâªâªâ©
Î¸2 = expâ§âªâªâªâ¨âªâªâªâ©

18

Proof. We have

where we deï¬ne

Pr[E1 âˆ© E2] â‰¤ Pr[E2  E1] â‰¤

k
Q
j=1

Pr[E2,j  E1] ,

(21)

E2,j â‰œ Exactly j lines do not prefer A.

conduct the i-th query and condition on event E1 (i.e. given that none of the lines cut

that the line â„“xi does not cut A, Zi has different update rules depending on the two
cases whether the line â„“xi prefers A or does not prefer A. (i) ï¬rst case: if the line â„“xi

We will now calculate Pr[E2,j  E1] . In the beginning, p0 puts a uniform measure on
A and hence p0(A) =  A 2Ï€ . Let us ï¬rst investigate the dynamics of piâˆ’1(A) when we
A). In this setting, we deï¬ne the random variables Zi = log pi(A). At time i, assuming
prefers A, then we know that either with probability 1 âˆ’ Ï (if Î¶i = 0) we have pi(A) =
2(1âˆ’Ï)piâˆ’1(A) and with probability Ï (if Î¶i = 1) we have pi(A) =(2Ï)piâˆ’1(A). Thus,
we can write Zi = Ziâˆ’1 + Fi, where Fi â‰œ Î¶i log(2Ï) +(1 âˆ’ Î¶i) log(2(1 âˆ’ Ï)). (ii) second
where Ui â‰œ Î¶i log(2(1 âˆ’ Ï)) +(1 âˆ’ Î¶i) log(2Ï). Now, in order to ï¬nd an upper bound
on Pr[E2,j  E1], we assume without loss of generality that in the ï¬rst m âˆ’ j rounds

we the lines are as in the ï¬rst case and in the last j rounds the lines are as in the second
case (note that any other given order of the lines is statistically equivalent to this simple
order that we consider).

case: if â„“xi does not prefer A, then using a similar argument we obtain Zi = Ziâˆ’1 + Ui,

Zm = Z0 +

mâˆ’j
Q
i=1

+

Fi +

m
Q

i=mâˆ’j+1
Fi +

Ui

m
Q

Ui.

Fi +

m
Q

i=mâˆ’j+1

mâˆ’j
Q
i=1

mâˆ’j
Q
i=1

= log2 A 2Ï€
Pr[E2,j  E1]
â‰¤ Prâ¡â¢â¢â¢â£log2 p0(A) +
= Prâ¡â¢â¢â¢â£
Î±1 = Prâ¡â¢â¢â¢â¢â£
Î±2 = Prâ¡â¢â¢â¢â¢â¢â£

Now, noting that pm(A) â‰¤ 1 and hence log pm(A) â‰¤ 0, we obtain
Ui â‰¤ 0â¤â¥â¥â¥â¦
 A â¤â¥â¥â¥â¦
Aâ¤â¥â¥â¥â¥â¦
Ui â‰¤ 0â¤â¥â¥â¥â¥â¥â¦

mâˆ’j
Q
i=1+

Let us now deï¬ne

i=mâˆ’j+1
2Ï€

mâˆ’j
Q
i=1

i=mâˆ’j+1

2Ï€

Fi â‰¤ log

Fi +

m
Q

Fi +

m
Q

i=mâˆ’j+1

mâˆ’j

2

Q
i=1

Ui â‰¤ log2

and

mâˆ’j

2

19

Using the union bound, we have

Now, to bound Î±1 we obtain after some simpliï¬cations that

Pr[E2,j  E1] â‰¤ Î±1 + Î±2.

and by using the Chernoff bound we get

To bound Î±2 we can similarly write after some simple steps that

(22)

(23)

(24)

2

mâˆ’j

Q
i=1

Î±1 = Prâ¡â¢â¢â¢â¢â£
Î±1 â‰¤ expâ§âªâªâªâ¨âªâªâªâ©
Î±2 â‰¤ Prâ¡â¢â¢â¢â¢â¢â£
Î±2 â‰¤ expâ§âªâªâªâ¨âªâªâªâ©

mâˆ’j
Q
i=1+

mâˆ’j

2

Î¶i â‰¥ Ï Ã—

m âˆ’ j

2

Ã—

log(2(1 âˆ’ Ï)) âˆ’ 2

Ï log 1âˆ’Ï
Ï

mâˆ’j log 2Ï€
 A 

âˆ’Ï

m âˆ’ j

6

m âˆ’ j

Î¶i â‰¥ Ï Ã—

mâˆ’j log 2Ï€

â›â

log(2(1 âˆ’ Ï)) âˆ’ 2
Ï log( 1âˆ’Ï
Ï )
log(2(1 âˆ’ Ï)) + 2j

Ï log 1âˆ’Ï
Ï

2

,

â¤â¥â¥â¥â¥â¦

 A )

2â«âªâªâªâ¬âªâªâªâ­
ââ 
mâˆ’j log(2Ï)
2â«âªâªâªâ¬âªâªâªâ­
ââ 

.

â¤â¥â¥â¥â¥â¥â¦

.

,

and using the Chernoff bound we get

m âˆ’ j

âˆ’Ï

6 â›â

log(2(1 âˆ’ Ï)) + 2j
mâˆ’j log(2Ï)
Ï )
Ï log( 1âˆ’Ï

We further note that both of the upper bounds on Î±1 and Î±2 decrease when we increase
j. Hence, the proof of the theorem follows by letting j = k in (23) and (24), and also
plugging these bounds into (21).

Appendix C: Proof of Theorem 5

nÎ³2

Pr[ Ë†Î´ âˆ’ Î´  â‰¥ Î³] â‰¤ 2 expâˆ’

First of all, we consider Pr[R(Ë†h) > min{R(h1), R(h2)}]. Let Î´ â‰œ Râˆ†(h1) âˆ’ Râˆ†(h2)
and Ë†Î´ â‰œ Ë†Râˆ†(h1) âˆ’ Ë†Râˆ†(h2). By Hoeffdingâ€™s inequality, we have
6  .
If Î´ > 0, then letting Î³ = Î´ yields that Pr[Ë†Î´ < 0] â‰¤ 2 exp(âˆ’nÎ´2~6). Therefore
Pr[ Ë†Râˆ†(h1) < Ë†Râˆ†(h2)] â‰¤ 2 exp(âˆ’nÎ´2~6). Since we know that Râˆ†(h1) > Râˆ†(h2)
(Î´ > 0), we have R(h1) > R(h2) because h1 and h2 agree on the complement of
âˆ†. Thus R(Ë†h) > min{R(h1), R(h2)} with probability at most 2 exp(âˆ’nÎ´2~6). Sim-
ilarly if Î´ < 0, then we have Pr[Ë†Î´ > 0] â‰¤ 2 exp(âˆ’nÎ´2~6). Therefore Pr[ Ë†Râˆ†(h1) >
Ë†Râˆ†(h2)] â‰¤ 2 exp(âˆ’nÎ´2~6). Since R(h1) < R(h2), hence R(Ë†h) > min{R(h1), R(h2)}

20

with probability at most 2 exp(âˆ’nÎ´2~6). In sum, we have R(Ë†h) > min{R(h1), R(h2)}
with probability at most 2 exp(âˆ’n Râˆ†(h1) âˆ’ Râˆ†(h2) 2~6).
Let Î´n denote 2 exp(âˆ’n Râˆ†(h1) âˆ’ Râˆ†(h2) 2~6). Now we want to bound E[R(Ë†h) 
h1, h2]. We have

where the last inequality holds because R(h1) âˆ’ R(h2)  â‰¤ Râˆ†(h1) âˆ’ Râˆ†(h2) .
When u =3~n, the function f(u) = 2ueâˆ’nu2~6 achieves its maximum 2 3

Thus we obtain that

ne .

.

E[R(Ë†h)  h1, h2]
â‰¤ (1 âˆ’ Î´n) min{R(h1), R(h2)} + Î´n max{R(h1), R(h2)}
= min{R(h1), R(h2)} + Î´n R(h1) âˆ’ R(h2) 
= min{R(h1), R(h2)} + 2 R(h1) âˆ’ R(h2) 
exp(âˆ’n Râˆ†(h1) âˆ’ Râˆ†(h2) 2~6
â‰¤ min{R(h1), R(h2)} + 2 R(h1) âˆ’ R(h2) 
exp(âˆ’n R(h1) âˆ’ R(h2) 2~6,
E[R(Ë†h)  h1, h2] â‰¤ min{R(h1), R(h2)} + 2 3
E[R(Ë†h)] â‰¤ E[min{R(h1), R(h2)] + 2 3
â‰¤ min{E[R(h1), E[R(h2)]} + 2 3

ne

ne

.

ne

By Jensenâ€™s inequality, we conclude that

21

