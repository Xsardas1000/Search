Multilevel Sequential Monte Carlo Samplers for

Pierre Del Moralâˆ—

Normalizing Constants
Kody Lawâ€¡

Ajay Jasraâ€ 

Yan ZhouÂ§

March 4, 2016

Abstract

This article considers the sequential Monte Carlo (SMC) approxima-
tion of ratios of normalizing constants associated to posterior distributions
which in principle rely on continuum models. Therefore, the Monte Carlo
estimation error and the discrete approximation error must be balanced.
A multilevel strategy is utilized to substantially reduce the cost to obtain
a given error level in the approximation as compared to standard esti-
mators. Two estimators are considered and relative variance bounds are
given. The theoretical results are numerically illustrated for the example
of identifying a parametrized permeability in an elliptic equation given
point-wise observations of the pressure.

Key words: Multi-Level Monte Carlo, Sequential Monte Carlo, Bayesian
Inverse Problems.
AMS subject classiï¬cation: 82C80, 60K35.

1

Introduction

Over the past decades there has been an explosion of interest in accounting for
uncertainty in the simulation of systems in science and engineering applications
which are governed by continuum limiting systems such as partial diï¬€erential
equations (PDEs) [18, 23, 24]. The setting bears similarities to the case of
continuous stochastic processes, which have enjoyed attention for much longer
(e.g. [19]).

6
1
0
2

 
r
a

M
3

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
6
3
1
1
0

.

3
0
6
1
:
v
i
X
r
a

de Bordeaux I, 33405, FR

âˆ—Center INRIA Bordeaux Sud-Ouest & Institut de Mathematiques de Bordeaux, Universite
â€ Department of Statistics & Applied Probability, National University of Singapore, Singa-
â€¡Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge,
Â§Department of Statistics & Applied Probability, National University of Singapore, Singa-

pore, 117546, SG

37934 TN, USA

pore, 117546, SG

1

Consider a sequence of probability measures {Î·l}lâ‰¥0 on a common mea-
surable space (E,E); assume that the probabilities have common dominating
ï¬nite-measure du. In particular, for some known Îºl : E â†’ R+, let

where the normalizing constant Zl = (cid:82)

Î·l(du) =

Îºl(u)

Zl

du

(1)

E Îºl(u)du may be unknown. The con-
text of interest is when the sequence of densities is associated to an â€˜accuracyâ€™
parameter hl, with hl â†’ 0 as l â†’ âˆ with âˆ > h0 > h1 > Â·Â·Â· > hâˆ = 0.
When estimating statistics EÎ·âˆ [g(U )], for g : E â†’ R, in general one must
approximate the limiting measure by Î·L and perform statistical estimation with
respect to this. For larger L, the approximation of the limit is better, and yet the
simulations are more expensive and indeed the measure may also be supported
on a subspace of the underlying space E whose dimension is larger.
Monte Carlo methods for statistical estimation are robust and scalable, al-
though they are plagued by a â€œslowâ€ convergence rate of O(Nâˆ’1/2) for ap-
proximations using N degrees of freedom. Attempts to circumvent this issue,
for example using sophisticated deterministic high-dimensional approximation
methods typically result in some manifestation of the â€œcurse of dimensionalityâ€
[2], although recent work has indicated potential for the mitigation of such eï¬€ect
for suitably regular problems [6, 21].

The multilevel Monte Carlo framework [10, 11, 12] allows one to leverage in
an optimal way the nested problems arising in this context, hence minimizing
the necessary cost to obtain a given level of mean square error. In particular, the
multilevel Monte Carlo method seeks to sample from Î·0 as well as a sequence of
coupled pairs (Î·0, Î·1), . . . , (Î·Lâˆ’1, Î·L) and using a collapsing sum representation
of EÎ·L [g(U )]. Then using a suitable trade oï¬€ of computational eï¬€ort, one can
reduce the amount of work, relative to i.i.d. sampling from Î·L and using Monte
Carlo integration, for a given amount of error. However, we are concerned
with the scenario where such independent sampling is not possible, that is,
either Î·L or from the sequence of couples. As it is well-known, the use of
importance sampling to then use the collapsing sum representation, is often
not reasonable, in the sense that for importance proposals that can be sampled
independently, the associated variance typically explodes exponentially in the
dimension of the problem (e.g. [5]). As a result, there has been an extension of
multilevel Monte Carlo methods in which the approximate target distribution
can be sampled from directly, to more sophisticated Monte Carlo techniques
for inference; however, this is still in its infancy. Important examples include
the preliminary exploration of multilevel Markov chain Monte Carlo (MCMC)
[13, 17], multilevel sequential Monte Carlo samplers [4], multilevel ensemble
Kalman ï¬lter [14] and multilevel particle ï¬lters [16]. It should be noted that
MCMC and SMC can perform at a polynomial cost in the dimension; see e.g. [3]
and the references therein.
A signiï¬cant challenge for inference problems is estimation of the normalizing
constant ZL or ratios thereof Zl/Zk, L â‰¥ l > k â‰¥ 0. Such quantities are
central to Bayesian model comparison and choice [15, 25]. In addition, obtaining

2

unbiased estimates (in the sense that the expectation is equal to the value,
that is, potentially including discretization bias) are often central in pseudo-
marginal algorithms (e.g. [1]). In general the calculation of these quantities are
notoriously challenging (see for instance [26]) from a computational perspective.
In this article we extend the framework of [4] to consider the estimation of the
ratio of normalizing constants. This is a framework which uses SMC. We con-
sider both the â€˜standardâ€™ unbiased estimator ([8]) used in SMC, adapted to the
multilevel setting and an estimator which follows the collapsing sum approach
for multilevel methods. For the latter, we introduce a novel decomposition of the
normalizing constant of a Feynman-Kac formula, which corresponds to ZL/Z0,
which facilitates unbiased estimation. We consider new variance bounds for the
estimator [8] and our new estimate and show that, in general, both approaches
perform in a similar manner. For a given level of error, the cost is less than a
Monte Carlo estimate that uses i.i.d. sampling from Î·0, to estimate ZL/Z0; we
assume that the former is possible.

The paper is structured as follows. In Section 2 the setup will be given, along
with a description of the multilevel algorithm and the new novel estimator for
the normalizing constant. Section 3 contains the theoretical results, including
the main theorems of the paper which allow the multilevel theory to carry
through. Finally, section 4 presents the results of numerical experiments on an
example Bayesian inverse problem. The proofs are housed in the appendix.

2 Estimation

2.1 Notations
Let (E,E) be a measurable space. The notation Bb(E) denotes the class of
bounded and measurable real-valued functions. The supremum norm is written
as (cid:107)f(cid:107)âˆ = supuâˆˆE |f (u)| and P(E) is the set of probability measures on (E,E).
We will consider non-negative operators K : E Ã— E â†’ R+ such that for each
u âˆˆ E the mapping A (cid:55)â†’ K(u, A) is a ï¬nite non-negative measure on E and for
each A âˆˆ E the function u (cid:55)â†’ K(u, A) is measurable; the kernel K is Markovian
if K(u, dv) is a probability measure for every u âˆˆ E. For a ï¬nite measure Âµ on
(E,E), and a real-valued, measurable f : E â†’ R, we deï¬ne the operations:

ÂµK : A (cid:55)â†’

We also write Âµ(f ) =(cid:82) f (u)Âµ(du).

K(u, A) Âµ(du) ; Kf : u (cid:55)â†’

f (v) K(u, dv).

(cid:90)

(cid:90)

2.2 Algorithm

As described in the Introduction, the context of interest is when a sequence of
densities {Î·l}lâ‰¥0, as in (1), are associated to an â€˜accuracyâ€™ parameter hl, with
hl â†’ 0 as l â†’ âˆ, such that âˆ > h0 > h1 Â·Â·Â· > hâˆ = 0. In practice one cannot
treat hâˆ = 0 and so must consider these distributions with hl > 0. The laws

3

with large hl are easy to sample from with low computational cost, but are very
diï¬€erent from Î·âˆ, whereas, those distributions with small hl are hard to sample
with relatively high computational cost, but are closer to Î·âˆ. Thus, we choose
a maximum level L â‰¥ 1 and we will estimate

By the standard telescoping identity used in MLMC, one has

EÎ·L[g(U )] :=

E

g(u)Î·L(du) .

(cid:90)
(cid:110)EÎ·l [g(U )] âˆ’ EÎ·lâˆ’1[g(U )]
(cid:111)
(cid:17)

(cid:104)(cid:16) Îºl(U )Zlâˆ’1

âˆ’ 1

EÎ·lâˆ’1

Îºlâˆ’1(U )Zl

L(cid:88)
L(cid:88)

l=1

l=1

(cid:105)

EÎ·L [g(U )] = EÎ·0[g(U )] +

= EÎ·0[g(U )] +

g(U )

.

(2)

Suppose now that one applies an SMC sampler [9] to obtain a collection of
samples (particles) that sequentially approximate Î·0, Î·1, . . . , Î·L. We consider
the case when one initializes the population of particles by sampling i.i.d. from
Î·0, then at every step resamples and applies a MCMC Markov kernel to mutate
), with +âˆ > N0 â‰¥ N1 â‰¥
the particles. We denote by (U 1:N0
Â·Â·Â· NLâˆ’1 â‰¥ 1, the samples after mutation; one resamples U 1:Nl
according to the
l ), for indices l âˆˆ {0, . . . , L âˆ’ 1}. We will denote
weights Gl(U i
by {Ml}1â‰¤lâ‰¤Lâˆ’1 the sequence of MCMC kernels used at stages 1, . . . , L âˆ’ 1,
such that Î·lMl = Î·l. For Ï• : E â†’ R, l âˆˆ {1, . . . , L}, we have the following
estimator of EÎ·lâˆ’1[Ï•(U )]:

l ) = (Îºl+1/Îºl)(U i

, . . . , U 1:NLâˆ’1

Lâˆ’1

0

l

Î·Nlâˆ’1
lâˆ’1 (Ï•) =

1

Nlâˆ’1

Ï•(U i

lâˆ’1) .

Nlâˆ’1(cid:88)

i=1

We deï¬ne

Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1Ml(dul)) =

Nlâˆ’1(cid:88)

i=1

1

Nlâˆ’1

Glâˆ’1(U i

lâˆ’1)Ml(U i

lâˆ’1, dul) .

The joint probability distribution for the SMC algorithm is

N0(cid:89)

Lâˆ’1(cid:89)

Nl(cid:89)

Î·0(dui
0)

i=1

l=1

i=1

Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1Ml(dui

l))

Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1)

.

The algorithm is summarized in Figure 1. If one considers one more step in
the above procedure, that would deliver samples {U i
L}NL
i=1, a standard SMC
sampler estimate of the quantity of interest in (2) is Î·N
L (g); the earlier samples
are discarded. Within a multi-level context, a consistent SMC estimate of (2)
is

(cid:98)Y = Î·N0

0 (g) +

L(cid:88)

(cid:110) Î·Nlâˆ’1

lâˆ’1 (gGlâˆ’1)
Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1)

âˆ’ Î·Nlâˆ’1

lâˆ’1 (g)

l=1

,

(3)

(cid:111)

4

The motivation for such a procedure is that, as shown in [4], the amount of
work, for a given level of error, relative to i.i.d. sampling from Î·L is reduced.
Thus the idea of using the approach is clear. However, as is well known in
the literature (e.g. [9]) SMC samplers can also estimate ratios of normalizing
constants as a by-product of the algorithm. We now consider this and also the
amount of work to obtain a given level of error in this context.

0. Sample U 1

0 , . . . U N0
0

i âˆˆ {1, . . . , N0}: Set l = 0.

i.i.d. from Î·0 and compute G0(ui

0) for each sample

1. Sample Ë‡U 1

l , . . . , Ë‡U Nl+1

l

probabilities {Gl(u1

with replacement from u1:Nl
j=1 Gl(uj

l ), . . . , (Gl(uNl

l )/(cid:80)Nl

2. Sample U i

sample i âˆˆ {1, . . . , Nl+1}.

l from Ml+1(Ë‡ui

l+1|Ë‡ui

l,Â·) and compute Gl+1(ui

l )/(cid:80)Nl

l

with selection
l )}.
j=1 Gl(uj
l+1) for each

3. Set l = l + 1. If l = L stop, otherwise return to the start of Step 1.

Figure 1: The SMC algorithm.

2.3 Normalizing Constant
Deï¬ne, for l â‰¥ 0

(cid:90)

(cid:16) lâˆ’1(cid:89)

(cid:17)

l(cid:89)

p=1

Î·0(dul)

Mp(upâˆ’1, dup).

Î³l(dul) =

Gp(up)

El

p=0

In our context, it is well known that:

lâˆ’1(cid:89)

p=0

Î·p(Gp).

Î³l(1) =

Zl
Z0

=

This suggests the estimator:

Î³N0:lâˆ’1
l

(1) =

lâˆ’1(cid:89)

p=0

Î·Np
p (Gp)

(4)

which is known to be unbiased ([8]). We consider the relative variance of this
estimator in Section 3. However, at least on appearance it may not take ad-
vantage of the nature of the ML method. In addition, we show that the new
estimator below, can potentially be leveraged to remove the discretization bias.
We propose the following procedure. It should be remarked that it holds in
the particular context under study, but not for general Feynman-Kac models as

5

will be explained below. We have that for any l â‰¥ 2

(cid:17)

Î³l(1) = Î·0(G0) +

= Î·0(G0) +

= Î·0(G0) +

Î³p(1) âˆ’ Î³pâˆ’1(1)

(cid:0)Gpâˆ’2(Mpâˆ’1(Gpâˆ’1) âˆ’ 1)(cid:1)(cid:17)
(cid:0)Gpâˆ’2(Gpâˆ’1 âˆ’ 1)(cid:1)(cid:17)

.

Î³pâˆ’2

Î³pâˆ’2

(cid:16)
(cid:16)
(cid:16)

p=2

l(cid:88)
l(cid:88)
l(cid:88)

p=2

p=2

It is the ï¬nal line that we will approximate with our MLSMC sampler. It is
noted that the ï¬nal line holds in the speciï¬c case of interest, but is not generally
true for a given Feynman-Kac formula. The proposed approximation is

ËœÎ³N0:lâˆ’2
l

(1) = Î·N0

0 (G0) +

Î³N0:pâˆ’2
pâˆ’2

l(cid:88)

(cid:16)

(cid:0)Gpâˆ’2(Gpâˆ’1 âˆ’ 1)(cid:1)(cid:17)

where for any g âˆˆ Bb(E), p â‰¥ 2

Î³N0:pâˆ’2
pâˆ’2

(g) =

p=2

(cid:16) pâˆ’3(cid:89)

Note that for l â‰¥ 2, one has, almost surely,

k=0

pâˆ’2 (g).

Î·Nk

k (Gk)(cid:1)Î·Npâˆ’2
lâˆ’1(cid:89)

Î·Np
p (Gp).

ËœÎ³N0:lâˆ’1
l

(1) (cid:54)=

Using [8] it clearly follows that

p=0

Î³l(1) = E[ËœÎ³N0:lâˆ’2

l

(1)]

where E is the expectation w.r.t. the law of the SMC algorithm; the estimator
is unbiased.

2.4 Biased Estimator

Noting the estimator (3) another alternative estimator of Î³l(1) is

(cid:32)

lâˆ’1(cid:89)

p(cid:88)

(cid:110) Î·Nlâˆ’1

Î·N0
0 (G) +

p=0

l=1

lâˆ’1 (GpGlâˆ’1)
Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1)

âˆ’ Î·Nlâˆ’1

lâˆ’1 (Gp)

One can easily prove that this estimate is consistent, but biased, in the sense
that

(cid:34) lâˆ’1(cid:89)

(cid:32)

E

p(cid:88)

(cid:110) Î·Nlâˆ’1

Î·N0
0 (G) +

p=0

l=1

lâˆ’1 (GpGlâˆ’1)
Î·Nlâˆ’1
lâˆ’1 (Glâˆ’1)

âˆ’ Î·Nlâˆ’1

lâˆ’1 (Gp)

.

(cid:111)(cid:33)
(cid:111)(cid:33)(cid:105) (cid:54)= Î³l(1).

6

to the cost of computing this estimate. If (cid:80)lâˆ’1
(cid:80)lâˆ’1

However, the main reason why one may not want to consider its use is due
p=0 NpCp is the ordinary cost of
computing (4) (Cp is the cost per sample), then the cost of this estimator is
q=p Cq. Such a procedure is undesirable in general and this is not

(cid:80)lâˆ’1

p=0 Np

investigated further.

2.5 Estimator with no Discretization Bias
Let M âˆˆ {1, 2, . . .} be a random variable that is independent of the MLSMC
algorithm with PM (M â‰¥ m) > 0 âˆ€m > 0. Suppose further that one can prove
for N0, N1, . . . ï¬xed that

lim
pâ†’âˆ

Î³N0:pâˆ’2
pâˆ’2

E(cid:104)(cid:16)
E(cid:104)(cid:16)
E(cid:104)(cid:16)

lim
pâ†’âˆ

(cid:17)2(cid:105)1/2
(cid:0)Gpâˆ’2Gpâˆ’1) âˆ’ Î³âˆ(1)
(cid:17)2(cid:105)1/2
(cid:0)Gpâˆ’2) âˆ’ Î³âˆ(1)
(cid:17)2(cid:105)
(cid:0)Gpâˆ’2(Gpâˆ’1) âˆ’ Î³âˆ(1)

Î³N0:pâˆ’2
pâˆ’2

Î³N0:pâˆ’2
pâˆ’2

âˆ(cid:88)

p=2

1

PM (M â‰¥ p)

= 0

= 0

< +âˆ

(5)

(6)

(7)

then one can use the estimator from [20] to obtain an unbiased estimator for
Î³âˆ(1):

1

PM (M â‰¥ 1)

Î·N0
0 (G0) +

1

PM (M â‰¥ p)

Î³N0:pâˆ’2
pâˆ’2

M(cid:88)

p=2

(cid:16)

(cid:0)Gpâˆ’2(Gpâˆ’1 âˆ’ 1)(cid:1)(cid:17)

.

Note that, even if one can prove (5)-(7), one must be prepared to spend an
arbitrary amount of computational cost, which is not reasonable in the current
context. Hence we do not consider this further here. We further remark that
this particular approach is unlikely to work when estimating EÎ·âˆ[g(U )] (as in
(2)) as there is no unbiased property of the estimators (unbiased in the sense of
expectations and not associated to the discretization).

3 Theory

3.1 Relative Variance Bounds

Throughout E is compact. We make the following assumptions:
(A1) There exist 0 < C < C < +âˆ such that

sup
sup
uâˆˆE
0â‰¤l<âˆ
0â‰¤l<âˆ inf
inf
uâˆˆE

Îºl(u) â‰¤ C ;
Îºl(u) â‰¥ C .

7

(A2) There exists a Ï âˆˆ (0, 1) such that for any l â‰¥ 1, (u, v) âˆˆ E2, A âˆˆ E:

(cid:90)

(cid:90)

Ml(u, du(cid:48)) â‰¥ Ï

Ml(v, dv(cid:48)) .

A

A

These assumptions are almost identical to those in [4].

(A1) is diï¬€erent
but equivalent to (A1) in [4]. The proofs of the following Theorems are in
Appendices B and C respectively. It is remarked that there are other results in
the spirit of Theorem 1 below, (see [7, 22]) but the bounds are not sharp enough
for the purposes of this work.
Theorem 1. Assume (A1-2). Then there exists a c, C < +âˆ such that for any
L â‰¥ 2, N0 â‰¥ N1 â‰¥ Â·Â·Â· â‰¥ NLâˆ’1 > cL,

Lâˆ’1(cid:88)

p=0

C

(cid:16)(cid:16) Lâˆ’1(cid:88)

q=p

1
Np

(1)

âˆ’ 1

(cid:17)2

(cid:13)(cid:13)(cid:13)âˆ

âˆ’ 1

(cid:17)

.

(p + 1)

Np

+

Î·p(Gp)

Theorem 2. Assume (A1-2). Then there exists a c, C < +âˆ such that for any
L â‰¥ 2, N0 â‰¥ N1 â‰¥ Â·Â·Â· â‰¥ NLâˆ’2 > c(L âˆ’ 1),

(cid:17)2(cid:105) â‰¤
(cid:13)(cid:13)(cid:13) Gp
(cid:17)2(cid:105) â‰¤

âˆ’ 1

Î·q(Gq)

L
Î³L(1)

E(cid:104)(cid:16) Î³N0:Lâˆ’1
(cid:13)(cid:13)(cid:13)âˆ
(cid:13)(cid:13)(cid:13) Gq
E(cid:104)(cid:16) ËœÎ³N0:Lâˆ’2
L(cid:88)

L
Î³L(1)

(1)

âˆ’ 1

pâˆ’1(cid:88)

p=2

q=2

(cid:16) 1

N0

C

L(cid:88)

p=2

+

(p âˆ’ 1)
Npâˆ’2

3.2 Cost Analysis

(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)2âˆ +

(q âˆ’ 1)
Nqâˆ’2

(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)âˆ(cid:107)Gqâˆ’1 âˆ’ 1(cid:107)âˆ

In order to investigate the cost for a given level of error, we introduce the
following assumption.

(A3) (i) There exist Î±, Î¶ > 0, and a C > 0 such that for all p > 0

(cid:40)| Î³p(1)

Î³âˆ(1) âˆ’ 1| â‰¤ ChÎ±
p ;
â‰¤ Châˆ’Î¶
C(Gpâˆ’1)
p ,

(cid:17)

.

(8)

where C(Gpâˆ’1) denotes the cost to evaluate Gpâˆ’1.
(ii) There exist a Î² > 0 and a C > 0 such that for all p > 0

(cid:13)(cid:13)(cid:13) Gpâˆ’1

Î·pâˆ’1(Gpâˆ’1)

(cid:13)(cid:13)(cid:13)2

âˆ

âˆ’ 1

â‰¤ ChÎ²
p .

(iii) There exist a Î² > 0 and a C > 0 such that for all p > 0

(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)2âˆ â‰¤ ChÎ²
p .

8

Corollary 3.1. Assume (A1,2,3(i)(ii)) and 2Î± â‰¥ max{Î², Î¶}. Then for any
Îµ > 0, there exist L,{Nl}L

l=0 and C > 0 such that
(1) âˆ’ Î³âˆ(1)

(cid:17)2(cid:105) â‰¤ CÎµ2,

1

Î³âˆ(1)2

for the following cost

COST â‰¤ C

if Î² > Î¶,
if Î² = Î¶,

if Î² < Î¶.

Î± )| log(Îµ)|,

Corollary 3.2. Assume (A1,2,3(i)(iii)) and 2Î± â‰¥ max{Î², Î¶}. Then for any
Îµ > 0, there exist L,{Nl}L

l=0 and C > 0 such that
(1) âˆ’ Î³âˆ(1)

ËœÎ³N0:Lâˆ’2
L

(cid:17)2(cid:105) â‰¤ CÎµ2,

1

Î³âˆ(1)2

for the following cost

(9)

(10)

(11)

(12)

Îµâˆ’2| log(Îµ)|3,
Îµâˆ’(2+ Î¶âˆ’Î²

Î³N0:Lâˆ’2
L

E(cid:104)(cid:16)
ï£±ï£´ï£²ï£´ï£³Îµâˆ’2| log(Îµ)|,
E(cid:104)(cid:16)
ï£±ï£´ï£²ï£´ï£³Îµâˆ’2| log(Îµ)|,
(cid:13)(cid:13)(cid:13) Gp
Lâˆ’1(cid:88)

Îµâˆ’2| log(Îµ)|3,
Îµâˆ’(2+ Î¶âˆ’Î²

âˆ’ 1

COST â‰¤ C

if Î² > Î¶,
if Î² = Î¶,

if Î² < Î¶.

Î± )| log(Îµ)|,

We give the proof for Corollary 3.2 only. The proof of Corollary 3.1 is almost

identical. The only diï¬€erence is treating the term in the relative variance of

(p + 1)

(cid:13)(cid:13)(cid:13)âˆ

(cid:17)2(cid:105)

N 2
p
which is smaller than O(2), under our assumptions.

Î·p(Gp)

p=0

Proof of Corollary 3.2. The MSE can be bounded by
(1) âˆ’ Î³âˆ(1)

ËœÎ³N0:Lâˆ’2
L

1

E(cid:104)(cid:16)
E(cid:104)(cid:16) ËœÎ³N0:Lâˆ’2

Î³âˆ(1)2

(cid:17)2(cid:105) â‰¤
(cid:12)(cid:12)(cid:12)(cid:12)(cid:18) Î³L(1)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)2

Î³âˆ(1)

(cid:18) Î³L(1)
(cid:19)2
(cid:17)2
(cid:16) Î³L(1)
E(cid:104)(cid:16) ËœÎ³N0:Lâˆ’2

Î³âˆ(1)

L
Î³L(1)

(1)

âˆ’ 1

L
Î³L(1)

âˆ’ 1
Î³âˆ(1)
(cid:104) Îµ, and assuming
Following from (A3(i)), the second term requires that hÎ±
hL = Mâˆ’L for some M â‰¥ 2, this translates to L (cid:104) log Îµ. Notice that it also
L
= O(1). Now, deï¬ning Vp = (cid:107)Gpâˆ’1 âˆ’ 1(cid:107)2âˆ, Theorem 2
follows that
provides the following bound for the ï¬rst term

+

.

(cid:17)2(cid:105) â‰¤ V := C

(cid:32)

(1)

âˆ’ 1

Lâˆ’1(cid:88)

p=1

1
N0

+ L

Vp

Npâˆ’1

(cid:33)

.

9

To see this observe that

Lâˆ’1(cid:88)

p(cid:88)

q

Nqâˆ’1

V 1/2
p V 1/2

q =

Lâˆ’1(cid:88)

p=1

Lâˆ’1(cid:88)

q=p

p

Npâˆ’1

V 1/2
p

q â‰¤ CL
V 1/2

Lâˆ’1(cid:88)

p=1

Vp

Npâˆ’1

.

p=1

q=1

(cid:112)LVl/Cl (cid:104) L1/2h(Î²+Î¶)/2
where KL =(cid:80)Lâˆ’1

l

Optimizing the cost, given that the variance is O(Îµ2), dictates that Nl âˆ

. The constraint then requires that Nl âˆ LÎµâˆ’2KLh(Î²+Î¶)/2

,

l

l=1 h(Î²âˆ’Î¶)/2

. By assumption max{Î², Î¶} â‰¤ 2Î±, so (Î² + Î¶)/2Î± â‰¤ 2
and the requirement for all the Nl in Theorem 2 is guaranteed (as long as the
proportionality constant is greater than 1). Therefore, the MSE is controlled
by O(Îµ2) with a cost given by

l

L(cid:88)

NlCl (cid:104) LÎµâˆ’2K 2
L ,

and the result follows.

l=0

Remark 1. If one were able to perform i.i.d. sampling from Î·0 (denote the
samples u1, . . . , uN ), with estimator

N(cid:88)

i=1

1
N

Î³L(ui)
Î³0(ui)

âˆ’Î¶
for ZL/Z0 a computational eï¬€ort proportional to N h
L is used, with N the num-
ber of simulated samples. To make the overall error (bias squared plus variance)
of using i.i.d. sampling O(2) then one must take N âˆ O(âˆ’2), as the variance
of the MC estimate is O(Nâˆ’1), independently of L. This is a computational
âˆ’Î¶
cost of O(âˆ’2h
L ) is used which is far worse than MLSMC samplers in most
cases of practical interest.

4 Numerical Example

4.1 Setup

The performance of the proposed estimator will be demonstrated by a Bayesian
inverse problem example. The same example was also used in [4], which intro-
duced the MLSMC algorithm.
Introduce the Gelfand triple V := H 1(D) âŠ‚ L2(D) âŠ‚ Hâˆ’1(D) =: V âˆ—, where
the domain D will be understood. Let D âŠ‚ Rd with âˆ‚D âˆˆ C 1 convex. For
f âˆˆ V âˆ—, consider the following PDE on D:
âˆ’âˆ‡ Â· (Ë†uâˆ‡p) = f,
p = 0,

on âˆ‚D,

on D,

(14)

(13)

10

where

Ë†u(x) = Â¯u(x) +

K(cid:88)

k=1

ukÏƒkÏ†k(x).

(15)

Deï¬ne u = {uk}K
Assume that Â¯u, Ï†k âˆˆ Câˆ for all k and (cid:107)Ï†k(cid:107) = 1. In particular {Ïƒk}K
with k. In addition, the following property shall hold:

i.i.d.âˆ¼ U[âˆ’1, 1] (the uniform distribution on [-1,1]).
k=1 decay

k=1, with uk

Ë†u(x) â‰¥ inf

x

inf
x

Ïƒk â‰¥ uâˆ— > 0

(16)

Â¯u(x) âˆ’ K(cid:88)

k=1

so that the operator on the left-hand side of Equation (13) is uniformly elliptic.
Let p(Â·; u) denote the weak solution of Equation (13) for parameter u. Deï¬ne
the following vector-valued function

G(p) = [g1(p), . . . , gM (p)]T,

where gm are elements of the dual space V âˆ— for m = 1, . . . , M . It is assumed
that the data take the form

y = G(p) + Î¾,

Î¾ âˆ¼ N (0, Î),

(17)
where N (0, Î) denotes the Normal distribution with zero mean and covariance
Î.
The speciï¬c setting of the simulations are as the following. Let D = [0, 1]
and f (x) = 100x. Set K = 50, Â¯u(x) = 0.15 = const., Ïƒk = (2/5)4âˆ’k Ï†k(x) =
sin(kÏ€x) if k is odd and Ï†k(x) = cos(kÏ€x) if k is even. The forward problem at
resolution level l is solved using a ï¬nite element method with piecewise linear
shape functions on a uniform mesh of with hl = 2âˆ’(l+k), for some starting
k â‰¥ 1 (so that there are at least two grid-blocks in the coarsest, l = 0, case).
Thus, on level l the ï¬nite element basis functions are {Ïˆl
deï¬ned as (for
xi = i Â· 2âˆ’(l+k)):

i}2l+kâˆ’1

i=1

(cid:40)

Ïˆl

i(x) =

(1/hl)[x âˆ’ (xi âˆ’ hl)]
(1/hl)[(xi + hl) âˆ’ x]

if x âˆˆ [xi âˆ’ hl, xi],
if x âˆˆ [xi, xi + hl].

The function of interest g is taken as the solution of the forward problem at the
midpoint of the domain, that is g(u) = p(0.5; u). The observation operator is
G(u) = [p(0.25; u), p(0.75; u)]T, and the observational noise covariance is taken
to be Î = 0.252I.

Detailed error rates analysis of this example can be found in [4]. In partic-
ular, when the purpose of the study was to estimate Î·L(g), the variance rate
was Î² = 4 empirically. Later we will show that for estimating the normalizing
constant, the variance rate is very similar.

11

4.2 Veriï¬cation of Assumptions
Assumptions (A1) and (A3(i)(iii)) (for | Î³p(1)
Proposition 4.1 of [4]. For (A3(ii)) this follows directly from proving (A3(iii)).
It is natural to model the cost at level p by a power of the number degrees of
freedom, which is in turn related to hâˆ’1
p , verifying (A3(i)) (for C(Gpâˆ’1)). The
stiï¬€ness matrix of the ï¬nite element method is tridiagonal and thus the system
can be solved with cost O(2l+k), corresponding to a computational cost rate of
Î¶ = 1. Assumption (A2) is veriï¬ed for Gibbs sampler in section 4.2 of [4].

Î³âˆ(1)âˆ’1|), with Î² = 2Î± = 2, follow from

4.3 Experiments

We begin by using the theoretical rates Î² = 2Î± = 2 to estimate the MSE and
hence the cost ratio. Three cases are considered:

â€¢ A standard SMC algorithm, with the estimator Î³N0:lâˆ’1
â€¢ MLSMC sampler for Î³N0:lâˆ’1
â€¢ MLSMC sampler for ËœÎ³N0:lâˆ’2

(1).

(1).

l

l

l

(1).

The cost vs. MSE is plotted in Figure 2. The cost rates are âˆ’1.271, âˆ’0.967, and
âˆ’1.038 for the SMC, MLSMC with the standard estimator, and MLSMC with
the new estimator, respectively.
It is clear that the MLSMC algorithm with
both estimators provides superior performance when compared to the standard
SMC algorithm. It is interesting that for the given MLSMC ensemble, the per-
formance of the new estimator is comparable to that of the standard estimator,
as proven in Corollaries 3.1 and 3.2. It shall be noted that in practice, given the
same samples (U 1:N0
), the new estimator is capable of estimating
Î³L+1(1) while the standard one can only estimate the Î³L(1), which has a higher
bias.

, . . . , U 1:NLâˆ’1

Lâˆ’1

0

The variance rate Î² can also be estimated empirically by consider the vari-
ance of Î·l(Gl). The quantity, multiplied by the sample size, as a proxy of Vl
is plotted in Figure 3. The estimated empirical rate is Î² = 4.148. This is
consistent with the rate estimates in [4].

Acknowledgements

KJHL was supported in part by DARPA FORMULATE and in part by ORNL
LDRD Strategic Hire. AJ & YZ were supported by Ministry of Education AcRF
tier 2 grant, R-155-000-161-112.

12

Figure 2: Computational cost against mean squared error

Figure 3: Variance rate estimate

13

2152202252302âˆ’302âˆ’252âˆ’202âˆ’15MSE(ğœ€2)Runtimecostâˆâˆ‘ğ¿ğ‘™=0ğ‘ğ‘™â„âˆ’1ğ‘™AlgorithmMLSMC(Ìƒğ›¾ğ‘0âˆ¶ğ‘™âˆ’2ğ‘™(1))MLSMC(ğ›¾ğ‘0âˆ¶ğ‘™âˆ’1ğ‘™(1))SMC(ğ›¾ğ‘0âˆ¶ğ‘™âˆ’1ğ‘™(1))2âˆ’402âˆ’302âˆ’202âˆ’10202âˆ’122âˆ’102âˆ’82âˆ’62âˆ’4â„ğ‘™ğ‘‰ğ‘™â‰ˆğ‘ğ‘™var[ğœ‚ğ‘ğ‘™ğ‘™(ğºğ‘™)]A Notations
We give a collection of deï¬ntions which are used in the appendices. Let n â‰¥ 0,
F âˆˆ Bb(E Ã— E) and deï¬ne

(Î³N0:n

n

)âŠ—2(F ) =

Î·Np
p (Gp)

n )âŠ—2(F )
(Î·Nn

(cid:16) nâˆ’1(cid:89)

p=0

(cid:17)2

where for a ï¬nite (possibily signed) measure on E, Âµ, ÂµâŠ—2(d(u1, u2)) = Âµ(du1)Âµ(du2).
We recall the semi-group for p â‰¤ n (for p = n it is the identity operator):

(cid:90)

Enâˆ’pâˆ’1

Qp,n(xp, dxn) =

Qp+1(xp, dxp+1) . . . Qn(xnâˆ’1, dxn)

where for n â‰¥ 1, Qn(x, dy) = Gnâˆ’1(x)Mn(x, dy). We also deï¬ne the coalescent
operator for F âˆˆ Bb(E Ã— E), (x, y) âˆˆ E Ã— E:

C(F )(x, y) = F (x, x).

Then for 0 â‰¤ s â‰¤ (n + 1), 0 â‰¤ i1 < Â·Â·Â· < in â‰¤ n, F âˆˆ Bb(E Ã— E)

Î“i1:is

n

(F ) = Î³âŠ—2

i1

CQâŠ—2

i1,i2

CQâŠ—2

i2,i3

. . . CQâŠ—2

is,n(F )

and

i1:is
Î“
n

(F ) =

1

Î³n(1)2 Î“i1:is

n

(F ).

âˆ…
The conventions, for s = 0, Î“âˆ…
n(F ) = Î·âŠ—2
n (F ) and Î“
Recall the selection-mutation operator for any Âµ âˆˆ P(E), n â‰¥ 1

n(F ) = Î³âŠ—2

n (F ) are adopted.

Î¦n(Âµ)(dx) =

Âµ(Gnâˆ’1Mn(Â·, dx))

Âµ(Gnâˆ’1)

.

n

F N0:n
denotes the natural ï¬ltration generated by the particle system up-to
time n. For f1, f2 âˆˆ Bb(E) we write the tensor product of functions for every
(x, y) âˆˆ E Ã— E:

f1 âŠ— f2(x, y) = f1(x)f2(y).

B Proofs for Theorem 1
Lemma 1. Assume (A1-2). Then there exist a C < +âˆ such that for any

0 â‰¤ p â‰¤ n, x âˆˆ E:(cid:12)(cid:12)(cid:12) Qp,n(1)(x)

(cid:81)nâˆ’1

q=p Î·q(Gq)

(cid:12)(cid:12)(cid:12) â‰¤ C

nâˆ’1(cid:88)

q=p

(cid:13)(cid:13)(cid:13) Gq

Î·q(Gq)

(cid:13)(cid:13)(cid:13)âˆ

âˆ’ 1

âˆ’ 1

14

Proof. We ï¬x n, p and note that the case p = n is trivial, so we suppose p < n.
We prove the result by induction. We consider p = n âˆ’ 1 and thus

(cid:81)nâˆ’1

Qp,n(1)(x)
q=p Î·q(Gq)

âˆ’ 1 =

Gnâˆ’1(x)

Î·nâˆ’1(Gnâˆ’1)

âˆ’ 1

so the initialization is proved. Suppose the result holds at rank p and consider
the case p âˆ’ 1. We have

(cid:16) Gpâˆ’1(x)

Î·pâˆ’1(Gpâˆ’1)

(cid:17)

âˆ’ 1

Mp

By [7, Lemma 4.1]

âˆ’ 1 =

(x) + Mp

(cid:16) Qp,n(1)(x)
(cid:81)nâˆ’1

q=p Î·q(Gq)

(cid:17)

âˆ’ 1

(x).

â‰¤ C

(18)

(cid:17)

Qpâˆ’1,n(1)(x)
q=pâˆ’1 Î·q(Gq)

q=p Î·q(Gq)

(cid:81)nâˆ’1
(cid:16) Qp,n(1)(x)
(cid:81)nâˆ’1
(cid:81)nâˆ’1
(cid:12)(cid:12)(cid:12) â‰¤ C

âˆ’ 1

Qp,n(1)(x)
q=p Î·q(Gq)

where C does not depend upon p, n. Thus, by applying the induction hypothesis
and the above result it follows that:

(cid:12)(cid:12)(cid:12) Qpâˆ’1,n(1)(x)
(cid:81)nâˆ’1

q=pâˆ’1 Î·q(Gq)

nâˆ’1(cid:88)

q=pâˆ’1

(cid:13)(cid:13)(cid:13) Gq

Î·q(Gq)

(cid:13)(cid:13)(cid:13)âˆ

âˆ’ 1

and hence the proof is completed.

The result below follows one in [7].

Proposition 1. Assume (A1-2). Then there exists a C < +âˆ such that for
any n â‰¥ 0, F âˆˆ Bb(E Ã— E) and N0 â‰¥ Â·Â·Â· â‰¥ Nn > c(n + 1)

(cid:12)(cid:12)(cid:12)E(cid:104) (Î³N0:n

n

)âŠ—2(F )

Î³n(1)2

(cid:105) âˆ’ Î·âŠ—2

n (F )

(cid:12)(cid:12)(cid:12) â‰¤ 8c(cid:107)F(cid:107)âˆ

n(cid:88)

p=0

1
Np

.

Proof. The case with F constant essentially follows from the proofs of [7]. The
only diï¬€erence is the fact that we have a decreasing number of samples; this
does not change the calculations of that paper, so the case of F constant is in [7].
If F is a non-constant function, one has, from the equation above Proposition
3.4 (page 638) of [7]:

(cid:12)(cid:12)(cid:12)E(cid:104) (Î³N0:n
(cid:16) s(cid:89)

n

)âŠ—2(F )

Î³n(1)2

(cid:17)(cid:16) (cid:89)

1
Nik

(cid:105) âˆ’ Î·âŠ—2
(cid:0)1 âˆ’ 1

n (F )

(cid:12)(cid:12)(cid:12) =
(cid:1)(cid:17)

Nk

(cid:12)(cid:12)(cid:12).

(F âˆ’ Î·âŠ—2

n (F ))

Î“

i1:is
n

0â‰¤i1<Â·Â·Â·<isâ‰¤n

k=1

k /âˆˆ{i1,...,is}

(cid:88)

(cid:12)(cid:12)(cid:12) n+1(cid:88)

s=1

15

Following the proof of Theorem 5.1 of [7] and noting that one can allow the
function in that paper to be negative, it follows that

|Î“

i1:is
n

(F âˆ’ Î·âŠ—2

)âŠ—2(F )

Thus one has

n

(cid:12)(cid:12)(cid:12)E(cid:104) (Î³N0:n
n+1(cid:88)

Note that

Î³n(1)2

s=1

0â‰¤i1<Â·Â·Â·<isâ‰¤n

s=1

n (F )

n (F ))| â‰¤ (cid:107)F âˆ’ Î·âŠ—2

n (F )(cid:107)âˆ
(cid:12)(cid:12)(cid:12) â‰¤ 2(cid:107)F(cid:107)âˆ
(cid:105) âˆ’ Î·âŠ—2
n+1(cid:88)
(cid:16) s(cid:89)
(cid:17)s
(cid:17)(cid:16)
(cid:88)
(cid:17) âˆ’ 1 â‰¤ 2Ï
(cid:105) âˆ’ Î·âŠ—2

n(cid:89)
(cid:12)(cid:12)(cid:12)E(cid:104) (Î³N0:n

)âŠ—2(F )

1
Nik

n (F )

1
Ns

1 + Ï

(cid:16)

C
C

C
C

k=1

s=0

=

Ï

n

Î³n(1)2

and for N0 > C(n + 1), . . . , Nn > C(n + 1)

(cid:16)

Ï

C
C

.

Ï

(cid:17)s â‰¤ 2(cid:107)F(cid:107)âˆ
(cid:16) s(cid:89)
(cid:88)
(cid:16)

k=1

Ï

C
C

(cid:16)

(cid:17)s
(cid:17)(cid:16)
(cid:17) âˆ’ 1 ,

1
Nik

1 + Ï

C
C

1
Ns

n(cid:89)

s=0

0â‰¤i1<Â·Â·Â·<isâ‰¤n

(cid:17)s

.

C
C

1
Np

,

C
C

n(cid:88)
(cid:12)(cid:12)(cid:12) â‰¤ 8C(cid:107)F(cid:107)âˆ

p=0

n(cid:88)

p=0

1
Np

,

(see for instance the proofs of Theorem 5.1 and Corollary 5.2 of [7]). It follows
that

C ; the proof is concluded.

with C = Ï C
Proof of Theorem 1. Throughout the proof C < +âˆ is a constant whose value
may change from line-to-line. It will not depend on the level index. By [22,
Proposition 2.3]

(cid:17)2(cid:105)

(1)

âˆ’ 1

=

Lâˆ’1(cid:88)

p=0

1
Np

E[T N0:p
p,L ]

L
Î³L(1)

E(cid:104)(cid:16) Î³N0:Lâˆ’1
(cid:17)2(cid:16)

p (hp,L)2 + Î·Np

p (hp,L)Î·Np
p

(19)

âˆ’ 1(cid:1)(cid:17)

(cid:0) Gp

Î·p(Gp)

where

(cid:16) Î³N0:pâˆ’1

p

(1)

Î³p(1)

Î·Np
p (h2

T N0:p
p,L =
and we use the short-hand for 0 â‰¤ p â‰¤ n, x âˆˆ E:
Qp,n(1)(x)
q=p Î·q(Gq)

hp,n(x) =

.

p,L) âˆ’ Î·Np
(cid:81)nâˆ’1
(cid:16) Î³N0:pâˆ’1

p

T N0:p
p,L =

Now, one has almost surely that

(cid:17)2Ã—

(1)

Î³p(1)

16

(cid:16)

As

p ([hp,L âˆ’ 1]2) âˆ’ Î·Np
Î·Np

p (hp,L âˆ’ 1)2 + Î·Np

p (hp,L)Î·Np
p

(cid:34)(cid:16) Î³N0:pâˆ’1
(cid:34)(cid:16) Î³N0:pâˆ’1

p

p

Î³p(1)

(1)

(1)

(cid:17)2(cid:16)
(cid:17)2

E

Î³p(1)

p ([hp,L âˆ’ 1]2) âˆ’ Î·Np
Î·Np
(cid:0) Gp

Î·Np
p (hp,L)Î·Np
p

Î·p(Gp)

E[T N0:p

p,L ] = E

.

Î·p(Gp)

âˆ’ 1(cid:1)(cid:17)
(cid:0) Gp
p (hp,L âˆ’ 1)2(cid:17)(cid:35)
âˆ’ 1(cid:1)(cid:35)

+

(20)

we will consider controlling the two terms on the R.H.S. of (20) separately.
First term on the R.H.S. of (20).
We have, almost surely that
p ([hp,L âˆ’ 1]2) âˆ’ Î·Np
Î·Np

p (hp,L âˆ’ 1)2 â‰¤ C(cid:107)hp,L âˆ’ 1(cid:107)2âˆ

(cid:16) Lâˆ’1(cid:88)

q=p

(cid:13)(cid:13)(cid:13) Gq

Î·p(Gq)

(cid:17)2

(cid:13)(cid:13)(cid:13)âˆ

âˆ’ 1

â‰¤ C

where we have applied Lemma 1 to go to the second line. Then by Proposition
1 as N0 > cL, . . . , NLâˆ’1 > cL

â‰¤

(21)

(cid:17)2(cid:105) â‰¤ C.

(1)

Î³p(1)

p

E(cid:104)(cid:16) Î³N0:pâˆ’1
(cid:17)2(cid:16)
(cid:16) Lâˆ’1(cid:88)

C

.

q=p

âˆ’ 1

Î·p(Gq)

(cid:13)(cid:13)(cid:13) Gq

p (hp,L âˆ’ 1)2(cid:17)(cid:35)
p ([hp,L âˆ’ 1]2) âˆ’ Î·Np
Î·Np
(cid:13)(cid:13)(cid:13)âˆ
(cid:17)2
(cid:0) Gp
hp,L âŠ—(cid:16) Gp
(cid:17)(cid:17)

)âŠ—2(cid:16)
hp,L âŠ—(cid:16) Gp

âˆ’ 1(cid:1) =
(cid:17)(cid:17)

Î·Np
p (hp,L)Î·Np
p

(cid:17)2

Î·p(Gp)

Î·p(Gp)

âˆ’ 1

âˆ’ 1

= 0.

Î·p(Gp)

(cid:16) Î³N0:pâˆ’1

p

(1)

Î³p(1)

1

p

Î³p(1)2 (Î³N0:p
(cid:16)

Î·âŠ—2

p

17

So we have shown that

(cid:34)(cid:16) Î³N0:pâˆ’1

p

E

(1)

Î³p(1)

Second term on the R.H.S. of (20).
We have almost surely that

and note that,

E(cid:104)

(cid:105)

So by Proposition 1 as N0 > cL, . . . , NLâˆ’1 > cL and (18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34)(cid:16) Î³N0:pâˆ’1

Î³p(1)

p

(cid:17)2

(1)

Î·Np
p (hp,L)Î·Np
p

(cid:0) Gp

Î·p(Gp)

âˆ’ 1(cid:1)(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) â‰¤ C

(cid:13)(cid:13)(cid:13) Gp

Î·p(Gp)

(cid:13)(cid:13)(cid:13)âˆ

âˆ’ 1

(p + 1)

Np

.

(22)
Combining (19) with (20) and after applying the triangular inequality, the
bounds (21) and (22) complete the proof.

C Proofs for Theorem 2

Some of the proofs in this Section will use Proposition 1 in Appendix B.
Lemma 2. Let n â‰¥ 1 and f1, f2 âˆˆ Bb(E) then
= E[(Î³N0:n

(cid:105)
n âˆ’ Î³n](f2)

)âŠ—2(f1âŠ— f2)]âˆ’ Î³n(1)2Î·âŠ—2

n âˆ’ Î³n](f1)[Î³N0:n
[Î³N0:n

E(cid:104)

n

n (f1âŠ— f2).

Proof. We have

n âˆ’ Î³n](f1)[Î³N0:n
[Î³N0:n

n âˆ’ Î³n](f2)

=
(f1)]âˆ’Î³n(f1)E[Î³N0:n

n

E[Î³N0:n

n

(f1)Î³N0:n
E[Î³N0:n

n
(f1)Î³N0:n

n

n

(f2)]âˆ’Î³n(f2)E[Î³N0:n

n

(f2)] âˆ’ Î³n(f2)Î³n(f1) âˆ’ Î³n(f1)Î³n(f2) + Î³n(f1)Î³n(f2)

(f2)]+Î³n(f1)Î³n(f2) =

where the unbiased property of the normalizing constant has been used to go
to the last line. Then it follows that

E[Î³N0:n

n

(f1)Î³N0:n

n

(f2)] âˆ’ Î³n(f2)Î³n(f1) âˆ’ Î³n(f1)Î³n(f2) + Î³n(f1)Î³n(f2) =

E[(Î³N0:n

n

)âŠ—2(f1 âŠ— f2)] âˆ’ Î³n(1)2Î·âŠ—2

n (f1 âŠ— f2)

which concludes the proof.
Lemma 3. Assume (A1-2). Then there exists a C < +âˆ such that for any
2 â‰¤ q < p, N0 â‰¥ N1 â‰¥ Â·Â·Â· â‰¥ Nqâˆ’2 > C(q âˆ’ 1):
pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2
c(q âˆ’ 1)Î³qâˆ’2(1)2

(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1 âˆ’ 1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1 âˆ’ 1))
(cid:13)(cid:13)(cid:13)âˆ.

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))]| â‰¤

|E[[Î³N0:pâˆ’2

Nqâˆ’2

Proof. From [8, Proposition 7.4.1] we have

E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2
pâˆ’2(cid:88)

qâˆ’2(cid:88)

E(cid:104)

N0:s1âˆ’1
s1

Î³

(1)[Î·

Ns1

s1 âˆ’ Î¦s1(Î·

s1âˆ’1 )](Qs1,pâˆ’2(Gp))Ã—
Ns1âˆ’1

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))] =

s1=0

s2=0

18

N0:s2âˆ’1
s2

Î³

(1)[Î·

Ns2

s2 âˆ’ Î¦s2 (Î·

(cid:105)

Ns2âˆ’1
s2âˆ’1 )](Qs2,qâˆ’2(Gq))

where we have used the shorthand Gs = Gsâˆ’2(Gsâˆ’1 âˆ’ 1) for any s â‰¥ 2. For any
s â‰¥ 0, f âˆˆ Bb(E)

E[Î³N0:sâˆ’1

s

thus, it follows that

(1)[Î·Ns

s âˆ’ Î¦s(Î·Nsâˆ’1

sâˆ’1 )](f )|F N0:sâˆ’1

sâˆ’1

] = 0

E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2
qâˆ’2(cid:88)

E[Î³N0:sâˆ’1

s

(1)2[Î·Ns

s âˆ’ Î¦s(Î·Nsâˆ’1

sâˆ’1 )]âŠ—2(Qs,pâˆ’2(Gp) âŠ— Qs,qâˆ’2(Gq))].

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))] =

Now for any n â‰¥ 1, f1, f2 âˆˆ Bb(E), one can show, using almost the same
calculations as above, that the following holds

E[Î³N0:sâˆ’1

s

(1)2[Î·Ns

s âˆ’ Î¦s(Î·Nsâˆ’1

sâˆ’1 )]âŠ—2(Qs,n(f1) âŠ— Qs,n(f2))] =

E(cid:104)

(cid:105)
n âˆ’ Î³n](f2)

.

n âˆ’ Î³n](f1)[Î³N0:n
[Î³N0:n

s=0

n(cid:88)

s=0

E(cid:104)

Using this equality with n = q âˆ’ 2, and the fact that Qs,pâˆ’2 = Qs,qâˆ’2Qqâˆ’2,pâˆ’2,
ï¬nally

E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))] =

qâˆ’2 âˆ’ Î³qâˆ’2](Qqâˆ’2,pâˆ’2(Gp))[Î³N0:n
[Î³N0:n

(cid:105)
qâˆ’2 âˆ’ Î³qâˆ’2](Gq)

.

Then, by Lemma 2:

E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2

E[(Î³N0:qâˆ’2

qâˆ’2

)âŠ—2(Qqâˆ’2,pâˆ’2(Gp) âŠ— Gq)] âˆ’ Î³qâˆ’2(1)2Î·âŠ—2

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))] =
qâˆ’2(Qqâˆ’2,pâˆ’2(Gp) âŠ— Gq).

Then, one can apply Proposition 1 to obtain that

|E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gpâˆ’2(Gpâˆ’1 âˆ’ 1))[Î³N0:qâˆ’2
C(q âˆ’ 1)Î³qâˆ’2(1)2

(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1 âˆ’ 1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1 âˆ’ 1))
(cid:13)(cid:13)(cid:13)âˆ.

qâˆ’2 âˆ’ Î³qâˆ’2](Gqâˆ’2(Gqâˆ’1 âˆ’ 1))]| â‰¤

Nqâˆ’2

Proof of Theorem 2. Throughout the proof C < +âˆ is a constant whose value
may change from line-to-line. It will not depend on the level index. We have

E(cid:104)(cid:16) ËœÎ³N0:Lâˆ’2

L
Î³L(1)

(cid:17)2(cid:105) â‰¤

(1)

âˆ’ 1

19

1

Î³L(1)2

E[[Î·N0

0 âˆ’ Î·0](G0)2] +

1

Î³L(1)2

E[(

pâˆ’2 âˆ’ Î³pâˆ’2](Gp))2].
[Î³N0:pâˆ’2

L(cid:88)

p=2

As Î³L(1) = ZL/Z0 â‰¥ C/C it follows by standard results for i.i.d. random
variables that one has

1

Î³L(1)2

E[[Î·N0

.

0 âˆ’ Î·0](G0)2] â‰¤ C
N0
L(cid:88)

Î³pâˆ’2(1)2E(cid:104) Î³N0:pâˆ’2

pâˆ’2

p=2

Î³pâˆ’2(1)2 âˆ’ Î·pâˆ’2(Gp)2(cid:105)

(Gp)2

E[[Î³N0:pâˆ’2

pâˆ’2 âˆ’ Î³pâˆ’2](Gp)[Î³N0:qâˆ’2

qâˆ’2 âˆ’ Î³qâˆ’2](Gq)].

Now

L(cid:88)

E[(

pâˆ’2 âˆ’ Î³pâˆ’2](Gp))2] =
[Î³N0:pâˆ’2

p=2

+2

L(cid:88)

pâˆ’1(cid:88)

p=2

q=2

Applying Propositon 1 to the terms in the single sum and Lemma 3 to the terms
in the double sum, we have that

E[(

L(cid:88)
pâˆ’1(cid:88)

p=2

L(cid:88)

+

p=2

q=2

(cid:16) L(cid:88)

pâˆ’2 âˆ’ Î³pâˆ’2](Gp))2] â‰¤ C
[Î³N0:pâˆ’2

Î³pâˆ’2(1)2 (p âˆ’ 1)

(cid:107)Gp(cid:107)2âˆ

p=2

(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1 âˆ’ 1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1 âˆ’ 1))
(cid:13)(cid:13)(cid:13)âˆ

(cid:17)

.

Np

(q âˆ’ 1)Î³qâˆ’2(1)2

Nqâˆ’2

As Î³pâˆ’2(1) â‰¤ C/C, Î³L(1) â‰¥ C/C one has

L(cid:88)

1

Î³L(1)2

p=2

Î³pâˆ’2(1)2 (p âˆ’ 1)

Np

(cid:107)Gp(cid:107)2âˆ â‰¤ C

L(cid:88)

p=2

(p âˆ’ 1)
Npâˆ’2

(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)2âˆ.

We have

1

Î³L(1)2

L(cid:88)

pâˆ’1(cid:88)

(q âˆ’ 1)Î³qâˆ’2(1)2

p=2

q=2

Nqâˆ’2

(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1âˆ’1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1âˆ’1))
(cid:13)(cid:13)(cid:13)âˆ =

c(q âˆ’ 1)Î³qâˆ’2(1)

L(cid:88)
pâˆ’1(cid:88)
(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1 âˆ’ 1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1 âˆ’ 1))
(cid:13)(cid:13)(cid:13)âˆ.

Î·qâˆ’2(Qqâˆ’2,pâˆ’2(1))

Î³L(1)Nqâˆ’2

Zpâˆ’1
ZL

p=2

q=2

1

Ã—

Then as Î³qâˆ’2(1) â‰¤ C/C, Î³L(1) â‰¥ C/C, Zpâˆ’1 â‰¤ C, ZL â‰¥ C and by [7, Lemma
4.1]

Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1 âˆ’ 1))

â‰¤ C(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)âˆ

Î·qâˆ’2(Qqâˆ’2,pâˆ’2(1))

20

we have

1

Î³L(1)2

(cid:13)(cid:13)(cid:13)âˆ
(cid:13)(cid:13)(cid:13)Gqâˆ’2(Gqâˆ’1âˆ’1)Qqâˆ’2,pâˆ’2(Gpâˆ’2(Gpâˆ’1âˆ’1))

â‰¤

L(cid:88)

pâˆ’1(cid:88)

(q âˆ’ 1)Î³qâˆ’2(1)2

p=2

q=2

C

Nqâˆ’2

L(cid:88)

pâˆ’1(cid:88)

p=2

q=2

(q âˆ’ 1)
Nqâˆ’2

(cid:107)Gpâˆ’1 âˆ’ 1(cid:107)âˆ(cid:107)Gqâˆ’1 âˆ’ 1(cid:107)âˆ.

From here one can easily conclude.

References

[1] Andrieu, C., & Roberts, G. O. (2009). The pseudo-marginal approach for

eï¬ƒcient Monte Carlo computations. Ann. Statist., 37, 697-725.

[2] Bellman, R. E. (2015) Adaptive Control Processes: A Guided Tour. Prince-

ton university press.

[3] Beskos, A., Crisan, D. & Jasra, A. (2014). On the stability of sequential
Monte Carlo methods in high dimensions. Ann. Appl. Probab., 24, 1396â€“
1445.

[4] Beskos, A., Jasra, A., Law, K. J. H, Tempone, R. & Zhou,
Y. (2015). Multilevel sequential Monte Carlo samplers. arXiv preprint
arXiv:1503.07259.

[5] Bickel, P., Li, B. & Bengtsson, T. (2008). Sharp failure rates for the
bootstrap particle ï¬lter in high dimensions. In Pushing the Limits of Con-
temporary Statistics, B. Clarke & S. Ghosal, Eds, 318â€“329, IMS.

[6] Bungartz, H-J., & Griebel, M. (2004). Sparse grids. Acta numerica 13.1,

147-269.

[7] Cerou, F., Del Moral, P., & Guyader, A. (2011). A non-asymptotic
theorem for unnormalized Feynman-Kac particle models. Ann. Inst. Henri
Poincaire, 47, 629â€“649.

[8] Del Moral, P. (2004). Feynman-Kac Formulae: Genealogical and Inter-

acting Particle Systems with Applications. Springer: New York.

[9] Del Moral, P., Doucet, A. & Jasra, A. (2006). Sequential Monte Carlo

samplers. J. R. Statist. Soc. B, 68, 411â€“436.

[10] Giles, M. B. (2008). Multilevel Monte Carlo path simulation. Op. Res. 56,

607-617.

[11] Giles, M. B (2015). Multilevel Monte Carlo methods. Acta Numerica, 24,

259-328.

21

[12] Heinrich, S. (2001). Multilevel Monte Carlo methods. Large-scale scien-

tiï¬c computing. Springer Berlin Heidelberg, 2001. 58-67.

[13] Hoang, V., Schwab, C. & Stuart, A. (2013). Complexity analysis of ac-
celerated MCMC methods for Bayesian inversion. Inverse Prob., 29, 085010.

[14] Hoel, H., Law, K. J., & Tempone, R. (2015). Multilevel ensemble Kalman

ï¬ltering. arXiv preprint arXiv:1502.06069.

[15] Hoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T.

(1999). Bayesian model averaging: a tutorial. Statist. Sci., 14, 382-401.

[16] Jasra, A., Kamatani, K., Law, K. J., & Zhou, Y. (2015). Multilevel

particle ï¬lter. arXiv preprint arXiv:1510.04977.

[17] Ketelsen, C., Scheichl, R. & Teckentrup, A. L. (2013). A hierarchical
multilevel Markov chain Monte Carlo algorithm with applications to uncer-
tainty quantiï¬cation in subsurface ï¬‚ow. arXiv preprint arXiv:1303.7343.

[18] Le Matre, O. P., & Knio, O. M. (2010). Introduction: Uncertainty Quan-

tiï¬cation and Propagation. Springer Netherlands.

[19] Ã˜ksendal, B. (2003). Stochastic Diï¬€erential Equations. Springer Berlin

Heidelberg.

[20] Rhee, C. H., & Glynn, P. W. (2015). Unbiased estimation with square

root convergence for SDE models. Op. Res., 63, 1026â€“1043.

[21] Schwab, C., & Gittelson, C. J. (2011). Sparse tensor discretizations of
high-dimensional parametric and stochastic PDEs. Acta Numerica, 20, 291.

[22] Schweizer, N. (2012). Non-asymptotic error bounds for sequential MCMC
and stability of Feynman-Kac operators. arXiv preprint arXiv:1204.2382v1.

[23] Walsh, J. B. (1986) An Introduction to Stochastic Partial Diï¬€erential

Equations. Springer Berlin Heidelberg.

[24] Walstrom, J. E., Mueller, T. D. & McFarlane, R. C. (1967). Evalu-

ating uncertainty in engineering calculations. J. Pet. Tech. 19.12, 1-595.

[25] Wasserman, L. (2000). Bayesian model selection and model averaging. J.

Math. Psych., 44(1), 92-107.

[26] Zhou, Y., Johansen, A. M. & Aston, J. A. D. (2016). Towards automatic
model comparison: An adaptive sequential Monte Carlo approach. J. Comp.
Graph. Statist., (to appear).

22

