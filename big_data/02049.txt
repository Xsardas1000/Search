6
1
0
2

 
r
a

M
7

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
9
4
0
2
0

.

3
0
6
1
:
v
i
X
r
a

Prediction of functional ARMA processes with an
Taoran Weiâˆ—
Johannes Klepschâˆ—

application to traï¬ƒc data
Claudia KlÂ¨uppelbergâˆ—

March 8, 2016

We study the functional ARMA(p, q) and a corresponding approximating vector

Abstract

model, based on functional PCA. We investigate the structure of the multivariate
vector process and derive conditions for the existence of a stationary solution to both
the functional and the vector model equation. We then use the stationary vector
process to predict the functional process, and compare the resulting predictor to
the functional best linear predictor proposed by [3]. We derive bounds for the error
due to dimension reduction. We conclude by applying functional ARMA processes
for the modelling and prediction of highway traï¬ƒc data.

AMS 2010 Subject Classiï¬cations: primary: 62M20 secondary: 60G25
Keywords: FPCA, functional ARMA process, functional principal component analysis,
FTSA, functional time series analysis, prediction, projection, traï¬ƒc data

Introduction

1
A macroscopic highway traï¬ƒc model involves velocity, ï¬‚ow (number of vehicles passing a
reference point per unit of time) and density (number of vehicles on a given road segment).
The relation among these three variables can be depicted in diagrams of â€œvelocity-ï¬‚ow
relationâ€ and â€œï¬‚ow-density relationâ€. The diagram of â€œï¬‚ow-density relationâ€ is also called
fundamental diagram of traï¬ƒc ï¬‚ow and can be used to determine the capacity of a road
system and give guidance for inï¬‚ow regulations or speed limits. Figures 1 and 2 depict
the â€œvelocity-ï¬‚ow relationâ€ and â€œï¬‚ow-density relationâ€ for traï¬ƒc data provided by the
Autobahndirektion SÂ¨udbayern. At a critical traï¬ƒc density the state of ï¬‚ow will change
from stable to unstable. In Figure 2, the critical traï¬ƒc density for traï¬ƒc on highway A92
in southern Bavaria is depicted.

In this paper we develop a statistical model for traï¬ƒc data and apply it to the above
data. As can be seen from Figure 4 and 5 the data show a certain pattern over the day,
âˆ—Center for Mathematical Sciences, Technical University of Munich, 85748 Garching,

Boltzmannstrasse 3, Germany, e-mail: j.klepsch@tum.de , cklu@tum.de , taoran.wei@tum.de,
http://www.statistics.tum.de

1

Figure 1: Velocity-ï¬‚ow relation on highway A92 in Southern Bavaria. Depicted are average
velocities per 3 min versus number of vehicles within these 3 min during the period 01/01/2014
0:00 to 30/06/2014 23:59.

Figure 2: Flow-density relation for the data from Figure 1.

2

which we want to capture using tools from functional data analysis. The basic idea of
functional data analysis is to represent the very high-dimensional raw data by a random

function X(â‹…), which in our case describes the traï¬ƒc velocity over a day. In this paper

we do not focus on the transformation of discrete to functional data. For theoretical
considerations we work with data in functional form. For a sound introduction on the
transformation from vector observations to functions, we refer to [15]. We want to assess
temporal dependence between diï¬€erent days; i.e., our goal is a realistic time series model
for functional data, which captures the day to day dependence. We hope that our analysis
may support short term traï¬ƒc regulation realised in real-time by electronic devices during
the day, which could beneï¬t from a more precise day-to-day prediction.

tional ARMA(p, q) process for arbitrary orders p, q. In scalar and multivariate time series

From a statistical point of view we are mainly interested in the prediction of a func-

analysis there exist several prediction methods which can be easily implemented like the
Durbin-Levinson and the innovations algorithms (e.g see [6]). For functional time series,
[3] has proposed the functional best linear predictor for a general linear process. However,
implementation of the predictor is in general not feasible in practice, because explicit

formulas of the predictor can not be derived. The class of functional AR(p) models is
presented in [3], Chapter 8 and [14]. The AR(1) model has also been applied for the

an exception. Functional autoregressive model of ï¬nite order are well studied (e.g. [3],
Chapter 3) and allow for a elaborate prediction theory. Two well known approaches are

prediction of traï¬ƒc data in [2].

error are derived.

In [1] a prediction algorithm is proposed, which combines the idea of functional prin-
cipal component analysis (FPCA) and functional time series analysis. The basic idea is
to reduce the inï¬nite-dimensional functional data by FPCA to ï¬nite-dimensional vector
data. Thus, the prediction of the functional time series is transformed to the prediction
of a multivariate time series. In [1] this algorithm is used to predict linear functional time

series, with a focus on the functional AR(1) process for which bounds for the prediction
In this paper we focus on functional ARMA(p, q) processes. In a ï¬rst step we obtain
tions such that the projected process follows a vector ARMA(p, q). If these assumptions
ARMA(p, q) process and assess the quality of the approximation. We then present condi-
solution. This opens the way for prediction of functional ARMA(p, q) processes and we
sense under stationarity of the functional and the vector ARMA(p, q) process. We derive

a multivariate vector process by projection of the functional process X on the linear span
of the d most important eigenfunctions of the covariance operator of X. We derive condi-

tions such that both functional and multivariate vector process have a unique stationary

discuss relevant methods. The prediction algorithm of [1] can be applied, and makes

do not hold, we show that the projected process can at least be approximated by a vector

bounds for the prediction error based on the multivariate vector process in comparison to
the functional best linear predictor derived by [3].

An extended simulation study can be found in [17], Chapter 5. It shows in particu-
lar that approximating the projection of functional ARMA processes by vector ARMA
processes is reasonable. This is seen by comparing the model ï¬t based on AIC and BIC

3

criteria. The simulation study also yields a more detailed assessment of the quality of
the functional predictor obtained by an extension of the algorithm [1] for diï¬€erent linear
processes.

Our paper is organised as follows. In Section 2 we introduce the necessary Hilbert
space theory and notation, which we use throughout. Here we present the Karhunen-
LoÂ´eve Theorem and describe the FPCA based on the functional covariance operator. We
also introduce the CVP method, which is used for truncation of the functional data. In

Section 3 we turn to functional time series models with special emphasis on ARMA(p, q)
processes. Section 3.1 is devoted to stationarity conditions for the functional ARMA(p, q)

model. In Section 3.2 we study the multivariate vector process obtained by projection of
the functional process on the linear span of the d most important eigenfunctions of the
covariance operator of X. We investigate its stationarity and prove that the multivariate
vector ARMA process approximates the functional ARMA process in a natural way. Sec-

tion 4 investigates the prediction algorithm for functional ARMA(p, q) processes invoking

the multivariate vector process and compares it to the functional best linear predictor.
Finally, in Section 5 we apply our results to a traï¬ƒc dataset of velocity measurements
from 01/01/2014 to 30/06/2014 (obtained from the Autobahndirektion SÂ¨udbayern) on a
highway in Southern Bavaria, Germany.

We shall often use Parsevalâ€™s equality, which ensures that for a countable orthonormal

x(t)y(t)dt,
x, eiei, y,

2 Methodology
We summarize some concepts we shall use throughout. For details and more background

see e.g. the monographs [3], [11] and [13]. Let H= L2([0, 1]) be the real separable Hilbert
space of square integrable functions xâˆ¶[0, 1]â†’ R with normx=(âˆ« 1
0 x2(s)ds)1~2 gener-
ated by the inner productx, yâˆ¶=S 1
x, yâˆˆ L2([0, 1]) .
basis(ei)iâˆˆN,
x, yâˆˆ H.
x, y= âˆQ
We denote byL the space of bounded linear operators acting on H. If not stated diï¬€erently,
i=1
we take the standard operator norm deï¬ned for a bounded operator Î¨âˆˆL byÎ¨Lâˆ¶=
supxâ‰¤1Î¨(x).
every orthonormal basis(ei)iâˆˆN of H,âˆQ
We denote byS the space of Hilbert-Schmidt operators acting on H, which is again a

A bounded linear operator Î¨ is a Hilbert-Schmidt operator, if it is compact and for

separable Hilbert space equipped with the following inner product and corresponding

(2.1)

0

Î¨(ei)2<âˆ.

i=1

4

i=1

(2.2)

Hilbert-Schmidt norm,

and Î¨Sâˆ¶=Î¨, Î¨S=Â¿``(cid:192)âˆQ

integrable random functions L2

Î¨(ei)2<âˆ.

Î¨1(ei), Î¨2(ei)

Î¨1, Î¨2Sâˆ¶= âˆQ
i=1
If Î¨ is a Hilbert-Schmidt operator, thenÎ¨Lâ‰¤Î¨S.
LetBH be the Borel Ïƒ-algebra of subsets of H. All random functions are deï¬ned on
some probability space(â„¦,A, P) and areAâˆ’BH-measurable. Then the space of square
(â„¦,A, P) is a Hilbert space with inner product
= L2
EX, Y= Eâˆ« 1
0 X(s)Y(s)ds for X, Y âˆˆ L2
For Xâˆˆ L2
tâˆˆ[0, 1].
Âµ(t)âˆ¶= E[X(t)],
W.l.o.g. we will assume throughout that Âµâ‰¡ 0.
xâˆˆ H.
CXâˆ¶ x E[X, xX] ,
E[X(t)X(s)] x(s)ds
X(s)x(s)ds X(t)=S 1

Deï¬nition 2.1. The covariance operator CX of X acts on H and is deï¬ned as

H. We call such X an H-valued random function.

CX(x)(t)= ES 1

H the functional mean of X is deï¬ned as

More precisely,

(2.3)

H

H

(2.4)

(2.5)

(2.6)

0

0

resentation

xâˆˆ H,

CX(x)= âˆQ
j=1

the covariance operator CX. This is known as the Karhunen-LoÂ´eve representation.

CX is a symmetric, non-negative deï¬nite Hilbert-Schmidt operator with spectral rep-

Î»jx, Î½jÎ½j,
for eigenpairs (Î»j, Î½j)jâˆˆN, where (Î½j)jâˆˆN is an orthonormal basis of H and (Î»j)jâˆˆN is a
j=1 Î»j<âˆ. When considering eigendecom-
sequence of positive real numbers such thatâˆ‘âˆ
positions, we will assume that the Î»j are ordered decreasingly, hence Î»iâ‰¥ Î»k for i< k.
H can be represented as a linear combination of the eigenfunctions(Î½i)iâˆˆZ of
Every Xâˆˆ L2
Theorem 2.2 (Karhunen-LoÂ´eve Theorem). Suppose Xâˆˆ L2
where(Î½i)iâˆˆZ are the orthonormal eigenfunctions of the covariance operator C deï¬ned in
(2.5). The scalar products(X, Î½i)iâˆˆZ have mean-zero, variance Î»i and are uncorrelated;
i.e., for all i, jâˆˆ Z, iâ‰  j,
where(Î»i)iâˆˆZ are the eigenvalues of the covariance operator CX.

EX, Î½i= 0, E(X, Î½iX, Î½j)= 0 and EX, Î½i2= Î»i,

H with EX= 0, then

X, Î½i Î½i,

X= âˆQ
i=1

(2.8)

(2.7)

5

The scalar products(X, Î½i)iâˆˆZ deï¬ned in (2.7) are called the scores of X. By the last
equation in (2.8), we haveâˆQ
j=1
Remark 2.3. [The CVP method] For any integer dâˆˆ N, we consider the largest d eigen-

Combining (2.8) and (2.9), every Î»j represents some proportion of the total variability of
X.

EX, Î½j2= EX2<âˆ, Xâˆˆ L2

Î»j= âˆQ
j=1

values Î»1, . . . , Î»d of CX. The cumulative percentage of total variance CPV(d) is deï¬ned
as

If we choose d âˆˆ N such that the CPV(d) exceeds a predetermined high value, then

CPV(d)âˆ¶= dQ
j=1

Î»j âˆQ
j=1

(2.10)

(2.9)

Î»j.

H.

X, Î½i Î½i,

denote by sp{Î½1, . . . , Î½d}

Î»1, . . . , Î»d or the corresponding Î½1, . . . , Î½d explain most of the variability of X. In this
context Î½1, . . . , Î½d are called the functional principal components (FPCâ€™s). If we project
the H-valued random function X on the ï¬nite dimensional subspace of H, spanned by
the d most important eigenfunctions Î½1, . . . , Î½d of the covariance operator of X, which we

Xdâˆ¶= dQ
i=1
then it contains most of the variability of X.

3 Functional ARMA processes


In this section, we ï¬rst introduce the functional ARMA(p, q) equations and derive suf-
H. We approximate this ï¬nite dimensional process by a suitable vector ARMA(p, q) pro-

ï¬cient conditions for the equations to have a stationary and causal solution, which we
present. We then project the functional linear process on a ï¬nite dimensional subspace of

(2.11)

cess, and give conditions for the stationarity of this multivariate approximation. We also
give conditions on the functional ARMA model such that the projection of the functional
process on a ï¬nite dimensional space still follows an ARMA structure.

ables.

paper.

We start by deï¬ning functional white noise, which will be needed throughout the

Deï¬nition 3.1. [[3], Deï¬nition 3.1] Let(Îµn)nâˆˆZ be a sequence of H-valued random vari-
= CÎµ,
<âˆ, CÎµn
(i) (Îµn)nâˆˆZ is H-white noise (WN) if for all nâˆˆ Z, EÎµn= 0, 0< EÎµn2= Ïƒ2
(ii)(Îµn)nâˆˆZ is H-strong white noise (SWN), if for all nâˆˆ Z, EÎµn= 0, 0< EÎµn2= Ïƒ2
<âˆ
and(Îµn)nâˆˆZ is i.i.d.

<âˆ. When
= Ïƒ2
We assume throughout that(Îµn)nâˆˆZ is WN with zero mean and EÎµ2

(â‹…)âˆ¶= E[Îµm,â‹… Îµn]= 0 for all nâ‰  m.

and if CÎµn,Îµm

Îµ

Îµ

SWN is required, this will be speciï¬ed.

n

Îµ

6

We will derive conditions such that (3.1) has a stationary solution. We begin with the

3.1 Stationary functional ARMA processes
Formally we can deï¬ne a functional ARMA process of arbitrary order.

Theorem 3.4. Under Assumption 3.3 there exists a unique stationary and causal solution

Deï¬nition 3.2. Let(Îµn)nâˆˆZ be WN as in Deï¬nition 3.1(i). Let furthermore Ï†1, . . . , Ï†p,
Î¸1, . . . , Î¸qâˆˆL. Then a solution of
Xn= pQ
Ï†i(Xnâˆ’i)+ qQ
Î¸j(Îµnâˆ’j)+ Îµn, nâˆˆ Z,
i=1
j=1
is called a functional ARMA(p, q) process.
functional ARMA(1, q) process, and need the following assumption.
Assumption 3.3. There exists some j0âˆˆ N such thatÏ†j0L< 1.
to (3.1) for p= 1 given by
Xn= Îµn+(Ï†+ Î¸1)Îµnâˆ’1+(Ï†2+ Ï†Î¸1+ Î¸2)Îµnâˆ’2
++(Ï†qâˆ’1+ Ï†qâˆ’2Î¸1++ Î¸qâˆ’1)Îµnâˆ’(qâˆ’1)
Ï†jâˆ’q(Ï†q+ Ï†qâˆ’1Î¸1++ Î¸q)Îµnâˆ’j
+ âˆQ
j=q
Ï†qâˆ’kÎ¸k)Îµtâˆ’j,
Ï†jâˆ’q( qQ
Ï†jâˆ’kÎ¸k)Îµtâˆ’j+ âˆQ
( jQ
= qâˆ’1Q
j=q
j=0
k=0
k=0

where Ï†0= I denotes the identity operator in H. Furthermore, the series in (3.2) converges
Lemma 3.5 ([3], Lemma 3.1). For every Ï†âˆˆL the following two conditions are equivalent:
(i) There exists an integer j0 such thatÏ†j0L< 1.
(ii) There exist a> 0 and 0< b< 1 such that for every jâ‰¥ 0,Ï†jL< abj.
in [3]. First we prove the mean square convergence of the series in (3.2). Take mâ€²> mâ‰¥ q

Proof of Theorem 3.4. We follow the lines of the proof of Prop. 3.1.1 of [6] and Theorem 3.1

almost surely and in L2
H.

For the proof we need the following lemma.

and consider the truncated series

(3.1)



(3.2)

(3.3)

X

Deï¬ne

(m)

n

âˆ¶= Îµn+(Ï†+ Î¸1)Îµnâˆ’1+(Ï†2+ Ï†Î¸1+ Î¸2)Îµnâˆ’2
++(Ï†qâˆ’1+ Ï†qâˆ’2Î¸1++ Î¸qâˆ’1)Îµnâˆ’(qâˆ’1)
+ mQ
Ï†jâˆ’q(Ï†q+ Ï†qâˆ’1Î¸1++ Î¸q)Îµnâˆ’j.
j=q
Î²(Ï†, Î¸)âˆ¶= Ï†q+ Ï†qâˆ’1Î¸1++ Ï†Î¸qâˆ’1Î¸qâˆˆL.

7

Then for all mâ€²> mâ‰¥ q, using that(Îµn)nâˆˆZ is WN,

(3.4)

âˆ’ X

(m)

n

(mâ€²)

n

mâ€²Q
j=m

EX

Using (3.4) we get

Since Ï†âˆˆL satisï¬es Lemma 3.5(i), and equivalently (ii), we obtain

Ï†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j)2
2= E mâ€²Q
j=m
EÏ†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j), Ï†kâˆ’q(Î²(Ï†, Î¸)(Îµnâˆ’k)
= mâ€²Q
mâ€²Q
j=m
k=m
EÏ†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j)2
= mâ€²Q
j=m
Ï†jâˆ’qÎ²(Ï†, Î¸)2L
â‰¤ EÎµ02
mâ€²Q
j=m
Ï†jâˆ’q2LÎ²(Ï†, Î¸)2L.
â‰¤ Ïƒ2
mâ€²Q
j=m
<âˆ.
Ï†j2L< âˆQ
a2b2j= a2
âˆQ
1âˆ’ b2
j=0
j=0
Ï†jâˆ’q2LÎ²(Ï†, Î¸)2L Ïƒ2
b2(jâˆ’q)â†’ 0,
â‰¤Î²(Ï†, Î¸)2L Ïƒ2
mâ€²Q
j=m
Ï†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j)<âˆ a.s.
sure convergence we verify thatâˆQ
j=1
Ï†jâˆ’qLÎ²(Ï†, Î¸)L2
Ï†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j)2â‰¤ âˆQ
EÎµ02= Ïƒ2
E âˆQ
Since
j=1
j=1
then by(3.4), we have
abjâˆ’q2= Ïƒ2
Î²(Ï†, Î¸)2L âˆQ
Ï†jâˆ’q2L2= Ïƒ2
Î²(Ï†, Î¸)2L âˆQ
j=1
j=1
Ï†jâˆ’qÎ²(Ï†, Î¸)(Îµnâˆ’j)2<âˆ.
E âˆQ
j=1
its second order stucture only depends on(Îµn)nâˆˆZ, which is WN.
In order to prove that (3.2) is a solution of (3.1) with p= 1, we plug (3.2) into (3.1),
and obtain for nâˆˆ Z,
Ï†qâˆ’kÎ¸k)Îµnâˆ’j

Thus we obtain a.s. convergence of the series in (3.2). Note that (3.2) is stationary, since

By the Cauchy criterion, the series in (3.2) converges in mean square. To prove almost

Îµ

Î²(Ï†, Î¸)2L âˆQ
j=1
Î²(Ï†, Î¸)2L
(1âˆ’ b)2

a2

Ï†jâˆ’q2L ,
<âˆ.

as m, mâ€²â†’âˆ.

Xnâˆ’ Ï†(Xnâˆ’1)= qâˆ’1Q
( jQ
j=0
k=0

Ï†jâˆ’kÎ¸k)Îµnâˆ’j+ âˆQ
j=q

Ï†jâˆ’q( qQ
k=0

Ïƒ2

Îµ

Hence

Îµ

Îµ

Îµ

Îµ

Îµ a2

8

(3.5)

Ï†qâˆ’kÎ¸k)Îµnâˆ’1âˆ’j.

Now notice that the second term of the right-hand side can be written as

âˆ’ Ï† qâˆ’1Q
Ï†jâˆ’kÎ¸k)Îµnâˆ’1âˆ’j+ âˆQ
( jQ
Ï†jâˆ’q( qQ
j=0
j=q
k=0
k=0
Ï† qâˆ’1Q
( jQ
Ï†jâˆ’kÎ¸k)Îµnâˆ’1âˆ’j+ âˆQ
Ï†qâˆ’kÎ¸k)Îµnâˆ’1âˆ’j
Ï†jâˆ’q( qQ
j=0
j=q
k=0
k=0
= qâˆ’1Q
( jQ
Ï†j+1âˆ’kÎ¸k)Îµnâˆ’1âˆ’j+ âˆQ
Ï†j+1âˆ’q( qQ
Ï†qâˆ’kÎ¸k)Îµnâˆ’1âˆ’j
j=0
j=q
k=0
k=0
= qQ
(jâ€²âˆ’1Q
Ï†jâ€²âˆ’kÎ¸k)Îµnâˆ’jâ€²+ âˆQ
Ï†jâ€²âˆ’q( qQ
Ï†qâˆ’kÎ¸k)Îµnâˆ’jâ€²
jâ€²=1
jâ€²=q+1
k=0
k=0
= qQ
Î¸jâ€²)Îµnâˆ’jâ€²+ âˆQ
Ï†jâ€²âˆ’kÎ¸kâˆ’ Ï†jâ€²âˆ’jâ€²
( jâ€²Q
Ï†qâˆ’kÎ¸k)Îµnâˆ’jâ€²
Ï†jâ€²âˆ’q( qQ
jâ€²=1
jâ€²=q+1
k=0
k=0
Ï†qâˆ’kÎ¸k)Îµnâˆ’jâ€²âˆ’ qQ
Ï†jâ€²âˆ’q( qQ
Ï†jâ€²âˆ’kÎ¸k)Îµnâˆ’jâ€²+ âˆQ
= qQ
( jâ€²Q
Î¸jâ€²Îµnâˆ’jâ€².
jâ€²=1
jâ€²=1
jâ€²=q+1
k=0
k=0
Ï†qâˆ’kÎ¸kÎµnâˆ’q+ qQ
Î¸jâ€²Îµnâˆ’jâ€²+ qQ
Xnâˆ’ Ï†(Xnâˆ’1)= Îµnâˆ’ qQ
Hence, comparing the sums in (3.5), the only remaining terms are
jâ€²=1
k=0
k=0
Î¸jâ€²Îµnâˆ’jâ€², nâˆˆ Z,
= Îµn+ qQ
jâ€²=1

Ï†qâˆ’kÎ¸kÎµnâˆ’q

n of (3.1).

which shows that (3.2) is a solution of equation (3.1) with p= 1. Finally we prove the
uniqueness of the solution. Assume that there is another stationary solution Xâ€²
Iteration gives (cf. [16], eq. (4)) for all r> q,
Ï†jâˆ’kÎ¸k)Îµnâˆ’j+ râˆ’1Q
= qâˆ’1Q
( jQ
Ï†jâˆ’q( qQ
Ï†qâˆ’kÎ¸k)Îµnâˆ’j
j=q
j=0
k=0
k=0
Ï†r+jâˆ’q( qQ
+ qâˆ’1Q
Ï†qâˆ’kÎ¸k)Îµnâˆ’(r+j)+ Ï†rXâ€²
nâˆ’r
j=0
k=j+1
Therefore, with X(r) as in (3.3), for r> q,
n 2= E qâˆ’1Q
Ï†qâˆ’kÎ¸k)Îµnâˆ’(r+j)+ Ï†rXâ€²
Ï†r+jâˆ’q( qQ
2
(r)
nâˆ’r
j=0
k=j+1
Ï†qâˆ’kÎ¸k)Îµnâˆ’(r+j)2+ 2 EÏ†r(Xâ€²
Ï†r+jâˆ’q( qQ
â‰¤ 2E qâˆ’1Q
nâˆ’r
j=0
k=j+1
Since(Îµn)nâˆˆZ is WN, and using the linearity of the operators
Ï†qâˆ’kÎ¸kL2
Ï†jL2( qQ
n 2â‰¤ 2Ï†râˆ’q2L qâˆ’1Q
EXâ€²
(r)
j=0
k=j+1

)2
EÎµnâˆ’(r+j)2+ 2Ï†r2LE(Xâ€²
nâˆ’r

EXâ€²

âˆ’ X

âˆ’ X

)2

Xâ€²

n

n

n

9

+ qQ
j=0

p.

Xn

Yn

I

0

0

I

0

0

0

0

condition (ii) of Lemma 3.5,

EXâ€²

n

âˆ’ X

n is equal in L2

H to the limit of X

He thus extended known results considerably.

Remark 3.6. Spangenberg [16] derived a strictly stationary, not necessarily causal so-

râ†’âˆ.

n 2â†’ 0,
(r)
(r)
n , hence to Xn, which proves uniqueness.

where Î¸0= I, and I and 0 in (3.6) denote the indentity and zero operators, respectively.

By stationarity of(Xn)nâˆˆZ and(Îµn)nâˆˆZ, the boundedness of Ï† and Î¸j, j= 1, . . . , q and by

Thus Xâ€²
lution of a functional ARMA(p, q) equation in Banach spaces under minimal conditions.

For a functional ARMA(p, q) process we use the state space representation
Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯ ,
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Îµnâˆ’j
Xnâˆ’1â‹®
0â‹®
Â·â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€šâ€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€Â¶
Â·â€â€â€â€â€â€â€â€â€â€â€šâ€â€â€â€â€â€â€â€â€â€Â¶
Xnâˆ’p+1
0
Î´nâˆ’j

Ï†1  Ï†pâˆ’1 Ï†p
Î¸j 0  0
Ã¯Ã¯Ã¯Ã¯Ã¯
=Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
â‹®
Xnâˆ’1

â‹®
Xnâˆ’2â‹®
â‹®

Â·â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€šâ€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€Â¶
Â·â€â€â€â€â€â€â€â€â€â€â€šâ€â€â€â€â€â€â€â€â€â€â€Â¶
Â·â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€šâ€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€Â¶
Xnâˆ’p
ÌƒÏ†
ÌƒÎ¸j
Ynâˆ’1
ÌƒÎ¸j(Î´nâˆ’1), nâˆˆ Z.
Yn=ÌƒÏ†(Ynâˆ’1)+ qQ
j=0
Hpâˆ¶=(L2([0, 1]))p with inner product and norm given by
and xpâˆ¶=x, y
We denote byL(H p) the space of bounded linear operators acting on H p, the operator
norm ofÌƒÏ†âˆˆL(H p) is deï¬ned byÌƒÏ†Lâˆ¶= supxpâ‰¤1ÌƒÏ†(x)p.
(Î´n)nâˆˆZ is WN in Hp. Let P1 be the projection of H p on H deï¬ned as
Assumption 3.7. There exists some j0âˆˆ N such that ÌƒÏ† as in (3.6) satisï¬esÌƒÏ†j0L< 1.
representation of a functional ARMA(p, q) in H as a functional ARMA(1, q) in H p, we
to the functional ARMA(p, q) equations (3.1). The solution can be written as Xn= P1Yn,

Since the Xn and Îµn take values in H, Yn and Î´n take values in the product Hilbert space

Theorem 3.8. Under Assumption 3.7 there exists a unique stationary and causal solution

Since the proof of Theorem 3.4 works in arbitrary Hilbert spaces, using the state space

get the following theorem as a consequence of Theorem 3.4.

P1(x1, . . . , xn)= x1,

(x1, . . . , xn)âˆˆ H p.

x, y

p

âˆ¶= pQ
j=1

xj, yj ,

(3.7)

(3.8)

We summarize this as

(3.6)

(3.9)

where Yn is the solution to the state space equation (3.7), given by

Yn= Î´n+(ÌƒÏ†+ÌƒÎ¸1)Î´nâˆ’1+(ÌƒÏ† 2+ÌƒÏ†ÌƒÎ¸1+ÌƒÎ¸2)Î´nâˆ’2

10

(3.10)

Furthermore, the series converges almost surely and in L2
H.

++(ÌƒÏ† qâˆ’1+ÌƒÏ† qâˆ’2ÌƒÎ¸1++ÌƒÎ¸qâˆ’1)Î´nâˆ’(qâˆ’1)
ÌƒÏ† jâˆ’q(ÌƒÏ† q+ÌƒÏ† qâˆ’1ÌƒÎ¸1++ÌƒÎ¸j)Î´nâˆ’j,
+ âˆQ
j=q
ÌƒÏ†jâˆ’kÌƒÎ¸k)Î´tâˆ’j+ âˆQ
ÌƒÏ†jâˆ’q( qQ
ÌƒÏ†qâˆ’kÌƒÎ¸k)Î´tâˆ’j
= qâˆ’1Q
( jQ
j=0
j=q
k=0
k=0
where ÌƒÏ† 0 denotes the identity operator in H p and Yn, Î´n, ÌƒÏ† and ÌƒÎ¸j are deï¬ned in (3.7).
3.2 The multivariate vector ARMA(p, q) process
We project the functional ARMA(p, q) process on a ï¬nite dimensional subspace of H,
X, which we denote by sp{Î½1, . . . , Î½d}. With the CPV-method from Remark 2.3 we choose
dâˆˆ N such that most of the variability of the stationary functional time series variables
of (2.11) we consider the projection on sp{Î½1, . . . , Î½d}
Xn, Î½iÎ½i.
Xn,d= Psp{Î½1,...,Î½d}Xn= dQ
i=1
Xnâˆ¶=(Xn, Î½1 , . . . ,Xn, Î½d)

can be explained by Î½1, . . . , Î½d. Recalling the concept of functional principal components

spanned by the d most important eigenfunctions Î½1, . . . , Î½d of the covariance operator of

In what follows, we are interested in

(3.11)

.

(3.12)

operators and their eigenelements in the case of dependent data we refer to [10].

Due to its ï¬nite dimensionality Xn is isomorph to Xn,d.
Remark 3.9. We will in the following assume that the eigenfunctions of the covariance
operators are known. In practice, this is of course not the case, and the eigenfunctions
that show up in the following have to replaced by their empirical counterpart. Our results
remain unchanged, except that we need stronger assumptions on the innovation process

(Îµn)nâˆˆZ to ensure consistency of the estimators. For details on the estimation of covariance

A ï¬rst result concerns the projection of the WN(Îµn)nâˆˆZ on sp{Î½1, . . . , Î½d}, which we
Lemma 3.10. Let(ei)iâˆˆZ be an arbitrary orthonormal basis of H. For dâˆˆ N, we deï¬ne
If(Îµn)nâˆˆZ is WN as in Deï¬nition 3.1(i), then(Zn)nâˆˆN is d-dimensional WN.
If(Îµn)nâˆˆZ is SWN as in Deï¬nition 3.1(ii), then(Zn)nâˆˆN is d-dimensional SWN.

Znâˆ¶=(Îµn, e1 , . . . ,Îµn, ed), nâˆˆ Z.

the d-dimensional vector process

will need throughout.

(i)
(ii)

(3.13)

11

(3.14)

Î¸j(Îµnâˆ’j), Î½l ,

Xn, Î½l=Ï†(Xnâˆ’1), Î½l+ qQ
j=0

Ï†(Î½lâ€²), Î½lXnâˆ’1, Î½lâ€² ,
Î¸j(Î½lâ€²), Î½lÎµnâˆ’j, Î½lâ€² .

Xnâˆ’1, Î½lâ€² Î½lâ€², Î½l= âˆQ
lâ€²=1
Îµnâˆ’j, Î½lâ€² Î½lâ€², Î½l= âˆQ
lâ€²=1

As in Section 3.1 we start with the functional ARMA(1, q) process for qâˆˆ N and are
interested in the dynamics of(Xn,d)nâˆˆZ of (3.11) for ï¬xed dâˆˆ N. For every lâˆˆ Z, using the
model equation (3.1) with p= 1, we get
lâˆˆ Z.
For every l we expandÏ†(Xnâˆ’1), Î½l, using that(Î½l)lâˆˆZ is a ONB of H
Ï†(Xnâˆ’1), Î½l=Ï† âˆQ
lâ€²=1
andÎ¸j(Îµnâˆ’j), Î½l for j= 1, . . . , q as
Î¸j(Îµnâˆ’j), Î½l=Î¸j âˆQ
lâ€²=1
In order to derive the d-dimensional vector process(Xn)nâˆˆZ, for notational ease, we restrict
a precise presentation to the ARMA(1, 1) model. The presentation of the ARMA(1, q)
For q= 1, with Î¸0= I and Î¸1= Î¸, in matrix form (3.14) is given by
Ï†(Î½d+1) , Î½1
Xnâˆ’1, Î½1
Xn, Î½1

â‹®
â‹®
Ï†(Î½d+1) , Î½d
Xnâˆ’1, Î½d
Xn, Î½d
Xnâˆ’1, Î½d+1
Xn, Î½d+1
Ï†(Î½d+1) , Î½d+1
â‹®
â‹®

Î¸(Î½d+1) , Î½1
Îµn, Î½1
Îµnâˆ’1, Î½1

â‹®
â‹®
Î¸(Î½d+1) , Î½d
Îµn, Î½d
Îµnâˆ’1, Î½d
Îµn, Î½d+1
Îµnâˆ’1, Î½d+1
Î¸(Î½d+1) , Î½d+1
â‹®
â‹®


Ï†(Î½1) , Î½1
â‹®
Ï†(Î½1) , Î½d
Ï†(Î½1) , Î½d+1
â‹®
Î¸(Î½1) , Î½1
Î¸(Î½1) , Î½d
Î¸(Î½1) , Î½d+1

Ï†(Î½d) , Î½1
â‹®
Ï†(Î½d) , Î½d
Ï†(Î½d) , Î½d+1
â‹®
Î¸(Î½d) , Î½1
Î¸(Î½d) , Î½d
Î¸(Î½d) , Î½d+1

Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

model is an obvious extension.






Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯







Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

+

â‹®
â‹®
â‹®
â‹®

Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

=

+

â‹®
â‹®

We simplify the notation in (3.15) by summarizing vectors and matrices to

(3.15)

(3.16)

â‹®
â‹®

12

where

Enâˆ’1
Eâˆ
nâˆ’1

 ,

Xn
Xâˆ

n

+ Î˜ Î˜âˆ
â‹®

â‹®

Xnâˆ’1
= Î¦ Î¦âˆ
+En
â‹®
â‹®
Xâˆ
Eâˆ
nâˆ’1
Enâˆ¶=(Îµn, Î½1 , . . . ,Îµn, Î½d)
âˆ¶=(Xn, Î½d+1 , . . .)
Xâˆ
âˆ¶=(Îµn, Î½d+1 , . . .)
Eâˆ

n

n

,

.

,

n

The operators Î¦ and Î˜ in (3.16) are dÃ— d matrices with entriesÏ†(Î½lâ€²), Î½l andÎ¸(Î½lâ€²), Î½l
in the l-th row and lâ€²-th column, respectively. Î¦âˆ and Î˜âˆ are dÃ—âˆ matrices with llâ€²-th
entriesÏ†(Î½lâ€²+d), Î½l andÎ¸(Î½lâ€²+d), Î½l, respectively.

By (3.16), we write the d-dimensional vector equation

Xn= Î¦Xnâˆ’1+ En+ Î˜Enâˆ’1+ âˆ†nâˆ’1, nâˆˆ Z,

(3.17)

where

nâˆˆZ is d-dimensional WN. Note that âˆ†nâˆ’1 in (3.18) is a d-dimensional
= âˆQ
vector with l-th component
lâ€²=d+1

nâˆ’1+ Î˜âˆEâˆ
âˆ†nâˆ’1âˆ¶= Î¦âˆXâˆ
nâˆ’1.
Ï†(Î½lâ€²), Î½lXnâˆ’1, Î½lâ€²+ âˆQ
Î¸(Î½lâ€²) , Î½lÎµnâˆ’1, Î½lâ€² .
lâ€²=d+1

By Lemma 3.10(En)
(âˆ†nâˆ’1)

(3.18)

(3.19)

l

Eâˆ†nâˆ’12

Proof. Using (3.18) we obtain

arbitrarily small by increasing the dimension d.

Thus, the â€œerror termâ€ âˆ†nâˆ’1 depends on Xnâˆ’1, and the vector process(Xn)nâˆˆZ in (3.17)
is in general not a vector ARMA(1, 1) process with innovations(En)nâˆˆZ. However, we can
use a vector ARMA model as an approximation to(Xn)nâˆˆZ, where we can make âˆ†nâˆ’1
Lemma 3.11. Letâ‹…2 denote the Euclidean norm in Rd, and let the d-dimensional vector
âˆ†nâˆ’1 be deï¬ned as in (3.18). Then Eâˆ†nâˆ’12
2 is bounded and tends to 0 as dâ†’âˆ.
2â‰¤ 2EÎ¦âˆXâˆ
2 .
nâˆ’12
2+ EÎ˜âˆEâˆ
nâˆ’12
nâˆ’12
nâˆ’1+ Î˜âˆEâˆ
2= EÎ¦âˆXâˆ
We estimate the two parts EÎ¦âˆXâˆ
nâˆ’12
2 and EÎ˜âˆEâˆ
nâˆ’12
(using Parsevalâ€™s identity in the third line),
nâˆ’12
EÎ¦âˆXâˆ
 âˆQ
2= E dQ
Ï†(Î½lâ€²)Xnâˆ’1, Î½lâ€², Î½l2
l=1
lâ€²=d+1
Ï†(Î½lâ€²)Xnâˆ’1, Î½lâ€², Î½l2
 âˆQ
â‰¤ E âˆQ
lâ€²=d+1
l=1
Xnâˆ’1, Î½lâ€²Ï†(Î½lâ€²)2
= E âˆQ
lâ€²=d+1
= E âˆQ
Xnâˆ’1, Î½lÏ†(Î½l),
Xnâˆ’1, Î½lâ€²Ï†(Î½lâ€²).
âˆQ
l=d+1
lâ€²=d+1
Xnâˆ’1, Î½lâ€²2Ï†(Î½lâ€²)2= âˆQ
nâˆ’12
EÎ¦âˆXâˆ
2â‰¤ E âˆQ
lâ€²=d+1
lâ€²=d+1

By the Karhunen-LoÂ´eve Theorem (Theorem 2.2) the scores(Xnâˆ’1,l, Î½l)lâˆˆZ are uncorre-

E(Xnâˆ’1, Î½lâ€²)2Ï†(Î½lâ€²)2

2 separately. By (3.19), we have

lated. Thus,

(3.20)

.

13

2 is bounded and tends

(3.21)

(3.22)

2 can be obtained in exactly the same way, and we get

Î»lâ€²Ï†2LÎ½lâ€²2â‰¤Ï†2L âˆQ
lâ€²=d+1

where CÎµ is the covariance operator of the WN. As a covariance operator, it has ï¬nite

Î»lâ€².
EÎµnâˆ’1, Î½lâ€²Îµnâˆ’1, Î½lâ€²

Since by (2.8) we have EXnâˆ’1, Î½lâ€²2= Î»lâ€², we get
E(Xnâˆ’1, Î½lâ€²)2Ï†(Î½lâ€²)2= âˆQ
âˆQ
lâ€²=d+1
lâ€²=d+1
The bound for EÎ˜âˆEâˆ
nâˆ’12
EÎ˜âˆEâˆ
nâˆ’12
2â‰¤ âˆQ
EÎµnâˆ’1, Î½lâ€²2Î¸(Î½lâ€²)2â‰¤Î¸2L âˆQ
lâ€²=d+1
lâ€²=d+1
=Î¸2L âˆQ
CÎµ(Î½lâ€²), Î½lâ€²,
lâ€²=d+1
lâ€²=d+1CÎµ(Î½lâ€²), Î½lâ€²â†’ 0 for
lâ€²=1CÎµ(Î½lâ€²), Î½lâ€²<âˆ. Hence,âˆ‘âˆ
nuclear operator normCÎµN âˆ¶=âˆ‘âˆ
dâ†’âˆ. Combining (3.20), (3.21) and (3.22) we ï¬nd that Eâˆ†nâˆ’12
to 0 as dâ†’âˆ.
2 is analogous in the ARMA(1, q) case. We now sum-
The proof of bounding Eâˆ†nâˆ’12
marize our ï¬ndings in the case of a functional ARMA(1, q) process.
Theorem 3.12. Consider a functional ARMA(1, q) process such that Assumption 3.3
holds. For dâˆˆ N the vector process Xnâˆ¶=(Xn, Î½1 , . . . ,Xn, Î½d) has the representation
Î˜qEnâˆ’1+ âˆ†nâˆ’1, nâˆˆ Z,
Xn= Î¦Xnâˆ’1+ En+ qQ
j=1
nâˆ’1+ qQ
âˆ†nâˆ’1âˆ¶= Î¦âˆXâˆ
Î˜âˆ
j Enâˆ’j
j=1
Î˜jEnâˆ’j, nâˆˆ Z.
Ë‡Xn= Î¦ Ë‡Xnâˆ’1+ En+ qQ
j=1
2 is bounded and tends to 0 as dâ†’âˆ.
Ï†(Î½d) , Î½1
Ï†(Î½1) , Î½1
Î¦=Ã¯Ã¯Ã¯Ã¯
Ã¯Ã¯Ã¯ .
Ï†(Î½1) , Î½d
Ï†(Î½d) , Î½d

Then both the functional ARMA(1, q) process (Xn)nâˆˆZ in (3.1) and the d-dimensional
vector process( Ë‡Xn)nâˆˆZ in (3.24) have a unique stationary and causal solution.
Moreover Eâˆ†nâˆ’12
Proof. Recall that the dÃ— d matrix Î¦ of the vector process (3.24) (see (3.15) and (3.16))

and all quantities are deï¬ned analogously to (3.12), (3.17), and (3.18). Deï¬ne

In order to show that (3.24) has a stationary solution, by Theorem 11.3.1 of [6], it suï¬ƒces

to prove that every eigenvalue Î»k of Î¦ with corresponding eigenvector ak=(ak,1, . . . , ak,d),
k = 1, . . . , d of Î¦ satisï¬es  Î»k < 1. Note that  Î»k < 1 is equivalent to  Î»j0
 < 1, for all

k

(3.23)

(3.24)

â‹®

â‹®



. . .

. . .

14

where

has representation

which ï¬nishes the proof.

k,l

=ak2= 1 for all
2=âˆ‘d
l=1ak,Î½l
j0âˆˆ N. Let ak= ak,1Î½1+â‹…â‹…â‹…+ ak,dÎ½d, andak=âˆ‘d
lâ€²=1Ï†Î½lâ€², Î½lak,l2 and,
l=1âˆ‘d
2 =âˆ‘d
1â‰¤ k â‰¤ d. With the orthogonality of Î½1, . . . , Î½d, Î¦ak2
l=1 a2
deï¬ning Ad={Î½1, . . . , Î½d}, we calculate
PAdÏ†PAdak2= dQ
Ï†PAdak, Î½lÎ½l2
l=1
Ï†( dQ
ak,lâ€²Î½lâ€²), Î½l2Î½l2
= dQ
l=1
lâ€²=1
 dQ
ak,lâ€²Ï†Î½lâ€², Î½l2=Î¦ak2
= dQ
lâ€²=1
l=1
j0(ak)â‰¤PAdÏ†PAd
k ak2=Î¦j0ak2=PAdÏ†PAd

Hence, for j0 as in Assumption 3.3,

2

k

ÌƒÎ¸j(Î´nâˆ’j), nâˆˆ Z,

Yn=ÌƒÏ†(Ynâˆ’1)+ qQ
j=0

j0Lakâ‰¤Ï†j0L< 1,
 Î»j0
 =Î»j0
In order to extend approximation (3.24) of a functional ARMA(1, q) process to a
functional ARMA(p, q) process we use again the state space representation (3.7) given by
where ÌƒÎ¸0 = I, Yn, ÌƒÏ†, ÌƒÎ¸ and Î´n are deï¬ned as in Theorem 3.8 and take values in Hp =
(L2([0, 1]))p; see (3.8).
Theorem 3.13. Consider the functional ARMA(p, q) process as deï¬ned in (3.1) such
that Assumption 3.7 holds. Then for dâˆˆ N the vector process
Xnâˆ¶=(Xn, Î½1 , . . . ,Xn, Î½d)
Î˜qEnâˆ’j+ âˆ†nâˆ’1, nâˆˆ Z,
Î¦iXnâˆ’i+ En+ qQ
j=1
+ qQ
âˆ†nâˆ’1âˆ¶= pQ
Î˜âˆ
i Xâˆ
Î¦âˆ
j Enâˆ’j
nâˆ’i
j=1
i=1
Ë‡Xn= pQ
Î˜qEnâˆ’1, nâˆˆ Z.
Î¦i Ë‡Xnâˆ’i+ En+ qQ
i=1
j=1
2 is bounded and tends to 0 as dâ†’âˆ.

Then both the functional ARMA(p, q) process (Xn)nâˆˆZ in (3.1) and the d-dimensional
vector process( Ë‡Xn)nâˆˆZ in (3.27) have a unique stationary and causal solution.
Moreover Eâˆ†nâˆ’12

and all quantities are deï¬ned analogously to (3.12), (3.17), and (3.18). Deï¬ne

Xn= pQ
i=1

(3.25)

(3.26)

(3.27)

has representation

where

15

d

d

that

= PAâŠ¥

d

Î¸jPAâŠ¥

d

= 0

d

d

(3.28)

d its orthogonal com-

d, the orthogonal com-

However, as we show next, assumptions on the moving average parameters are actually
not required. We start with a well known result that characterises vector MA processes.

We are now interested in conditions for(Xn)nâˆˆZ actually following a vector ARMA(p, q)
model. A trivial condition is that the projection of Ï†i and Î¸j on AâŠ¥
plement of Ad= sp{Î½1, . . . , Î½d}, satisï¬es
PAâŠ¥
Ï†iPAâŠ¥
for all i= 1, . . . , p and j= 1, . . . , q. Then the vector process Ë‡Xnâ‰¡ Xn for all nâˆˆ Z.
Lemma 3.14 ([6], Proposition 3.2.1). If(Xn)nâˆˆZ is a stationary vector process with au-
tocovariance function CXh,X0= E[XhX
0] with CXq,X0â‰  0 and CXh,X0= 0 for h > q, then
(Xn)nâˆˆZ is a vector MA(q).
Proposition 3.15. Denote again by Adâˆ¶= sp{Î½1, . . . , Î½d}, and by AâŠ¥
= 0 for all i= 1, . . . , p, then the d-dimensional process (Xn)nâˆˆZ in
(3.26) is a vector ARMA(p, q) process.
plement. If PAâŠ¥
Ï†iPAâŠ¥
Proof. Since Ï†i for i= 1, . . . , p only acts on Ad, from (3.26) we get
Î¦iXnâˆ’i+ En+ qQ
Xn= pQ
j=1
i=1
Î¦iXnâˆ’i+ En+ qQ
= pQ
j=1
i=1
Î˜jEnâˆ’j+ qQ
Ynâˆ¶= En+ qQ
j=1
j=1

Hence, in order to show that(Xn)nâˆˆZ follows an ARMA(p, q) process, we have to show
follows an vector MA(q) model. According to Lemma 3.14, it is suï¬ƒcient to show that
(Yn)nâˆˆZ is stationary and has an appropriate autocovariance structure. Deï¬ning (with
Î¸0= I)
observe that Yn = (Yn, Î½1, . . . ,Yn, Î½d) is isomorph to PAdYn = âˆ‘d
j=1Yn, Î½jÎ½j for all
nâˆˆ Z. Hence, stationarity of(Yn)nâˆˆZ immediately follows from the stationarity of(Yn)nâˆˆZ.
= PAdCYh,Y0PAd.
But since(Yn)nâˆˆZ is a functional MA(q) process, CYh,Y0= 0 for h > q. Due to the relation
between PAdYn and Yn, we also have CYh,Y0= 0 for h > q and, hence,(Yn)nâˆˆZ is a vector
MA(q).

Î˜jEnâˆ’j+ âˆ†nâˆ’1
Î˜jEnâˆ’j+ qQ
nâˆ’j, nâˆˆ Z.
j Eâˆ
Î˜âˆ
j=1
nâˆ’j, nâˆˆ Z,
j Eâˆ
Î˜âˆ

E[PAdYhPAdY0,â‹…]= PAdE[YhY0,â‹…]PAd

Î¸j(Îµnâˆ’j), nâˆˆ Z,

Ynâˆ¶= qQ
j=0

Furthermore,

16

4 Prediction of functional ARMA process

We derive the best linear predictor of a functional ARMA(p, q) process (Xn)nâˆˆZ based

on X1, . . . , Xn, deï¬ned as in (3.26). We then compare the vector best linear predictor to
the functional best linear predictor based on X1, . . . , Xn and show that, under regularity
conditions, the diï¬€erence is bounded and tends to 0 as d tends to inï¬nity.

by

4.1 Prediction based on the vector process
In ï¬nite dimensions, the concept of a best linear predictor is well studied. For a d-

dimensional stationary time series(Xn)nâˆˆZ we denote the â€œmatrix linear spanâ€ of X1, . . . , Xn

M1âˆ¶= nQ
i=1

AniXiâˆ¶ Ani are real dÃ— d matrices, i= 1, . . . , n.

(4.1)
Then the vector best linear predictor Ë†Xn+1 of Xn+1 based on X1, . . . , Xn is deï¬ned as the
projection of Xn+1 on M1; i.e.,

Ì‚Xn+1âˆ¶= PM1Xn+1.

(4.2)

(4.3)

sponding scalar product.

Its properties are given by the projection theorem (e.g. Theorem 2.3.1 of [6]) and are
summarized as follows.

Remark 4.1. Recall thatâ‹…2 denotes the Euclidean norm in Rd and ,Rd the corre-
(i) EXn+1âˆ’ Ë†Xn+1, YRd= 0 for all Yâˆˆ M1.
2= infYâˆˆM1 EYn+1âˆ’Y2
(ii) Ë†Xn+1 is the unique element in M1 such that EXn+1âˆ’ Ë†Xn+12

(iii) M1 is a linear subspace of Rd.
linear predictor of Xn+1 based on X1, . . . , Xn is the following:
Algorithm1:
(1) Select the number d of FPCâ€™s by the CPV-method (Remark 2.3) such that most of

the data variability can be explained by Î½1, . . . , Î½d. Compute the FPC scoresXk, Î½l
for l= 1, . . . , d and k= 1, . . . , n by projecting each Xk for k= 1, . . . , n on Î½1, . . . , Î½d.

In analogy to the prediction algorithm suggested in [1], a method for ï¬nding the best

2.

We summarize the scores in the vector

k= 1, . . . n.
best vector linear predictor of Xn+1 that we denote by
Xn+1, Î½d).

Xkâˆ¶=(Xk, Î½1 , . . . ,Xk, Î½d),
Ì‚Xn+1=( Xn+1, Î½1, . . . ,

(2) Now we consider the d-dimensional vectors X1, . . . , Xn. Using (4.2), we compute the

1The ï¬rst and the third step in the algorithm can be implemented in R with the package fda, and the

second step can be achieved with the R package mts.

17

(3) Finally, we re-transform the best vector linear predictor Ì‚Xn+1 into a functional form
Ì‚Xn+1 by the truncated Karhunen-LoÂ´eve representation:

Ì‚Xn+1âˆ¶= Xn+1, Î½1Î½1+â‹…â‹…â‹…+ Xn+1, Î½dÎ½d=(Î½1, . . . , Î½d)Ì‚Xn+1.

(4.4)

In [1] the resulting predictor (3) is compared to the functional best linear predictor for

functional AR(1) processes.
Our goal is to extend these results to functional ARMA(p, q) processes. However, when

moving away from autoregressive models, the best linear predictor is no longer directly
given by the process. Therefore, we start by recalling the notion of best linear predictors
in Hilbert spaces.

We introduce to our setting the functional best linear predictor Ì‚Xn+1 of Xn+1 based on
4.2 Functional best linear predictor
X1, . . . , Xn proposed in [5], whose notation we also use. It is the projection of Xn+1 on a
L-closed subspaces as in Deï¬nition 1.1 in [3].
H containing X1, . . . , Xn. More formally, we use the concept of
large enough subspace of L2
Deï¬nition 4.2. Recall thatL denotes the space of bounded linear operators acting on
H. We call G anL-closed subspace (LCS) of L2

(2) If Xâˆˆ G and gâˆˆL, then gXâˆˆ G.

(1) G is a Hilbertian subspace of L2
H.

H, if

We deï¬ne

X(n)âˆ¶=(Xn, . . . , X1).

where

The functional best linear predictor Ë†X G
G, which we write as

X(n)âˆ¶=gn X(n)âˆ¶ gnâˆˆL(H n, H).
Gâ€²

By Theorem 1.8 of [3] the LCS Gâˆ¶= GX(n) generated by X(n) is the closure of Gâ€²
(4.5)
X(n),
(4.6)
n+1 of Xn+1 is deï¬ned as the projection of Xn+1 on
n+1âˆ¶= PGXn+1âˆˆ G.
n+1, Y= 0 for all Y âˆˆ G.
n+1 is the unique element in G such that EXn+1âˆ’ Ë†X G
âˆ¶= EXn+1âˆ’ Ë†X G
n+12.

(ii) Ë†X G
(iii) The mean squared error of the functional best linear predictor Ë†X G
by

(4.7)
Its properties are given by the projection theorem (e.g. Theorem 2.3.1 of [6]) and are

n+12= inf YâˆˆG EXn+1âˆ’ Y2.
n+1 will be denoted
(4.8)

summarized as follows.

Remark 4.3. (i) EXn+1âˆ’ Ë†X G

Ë†X G

Ïƒ2

n

18

Note that, since Gâ€²
n+1= gn X(n) for some gnâˆˆL(H n, H), whereL(H n, H) denotes
n+1 is not
X(n) is not closed in general (cf. [5], Proposition 2.1), Ë†X G
necessarily of the form Ë†X G
the space of bounded linear operators from H n to H. However, the following Proposition
n+1 to be represented in terms of bounded
gives necessary and suï¬ƒcient conditions for Ë†X G
linear operators.

n+1= sn X(n) for

We now formulate conditions for Ë†X G

Proposition 4.5. The following statements are equivalent:

n+1 to have the representation Ë†X G

Proposition 4.4 (Proposition 2.2, [5]). The following statements are equivalent:

(i) There exists some g0âˆˆL(H n, H) such that CX(n),Xn+1
= g0CX(n).
(ii) PGXn+1= g0 X(n) for some g0âˆˆL(H n, H).
some Hilbert-Schmidt operator sn from H n to H (snâˆˆS(H n, H)).
(i) There exists some s0âˆˆS(H n, H) such that CX(n),Xn+1
= s0CX(n).
(ii) PGXn+1= s0 X(n) for some s0âˆˆS(H n, H).
= s0CX(n).
s0âˆˆS(H n, H), such that CX(n),Xn+1
Then, since CX(n),s0 X(n)= E[s0X(n)X(n),â‹…]= s0CX(n), we have
CX(n),Xn+1âˆ’s0 X(n)= 0.
Therefore Xn+1âˆ’ s0(X(n))âŠ¥ X(n) and hence Xn+1âˆ’ s0(X(n))âŠ¥ G which gives (ii).

Proof. The proof is similar to the proof of Proposition 4.4. Assume there exists some

For the reverse, note that (ii) implies

CX(n),Xn+1âˆ’s0 X(n)= CX(n),Xn+1âˆ’PGXn+1

= 0.
= CX(n),s0 X(n)= s0CX(n), which ï¬nishes the proof.

that

applies.

We will proceed with examples of processes where Proposition 4.4 or Proposition 4.5

Hence CX(n),Xn+1
Example 4.6. Let(Xn)nâˆˆZ be a stationary and invertible functional linear process, such
Xn= Îµn+ âˆQ
Ï€j(Xnâˆ’j), nâˆˆ Z,
j=1
j=1Ï€jL<âˆ. Then there exists some l0âˆˆL(H n, H)
where(Îµn)nâˆˆZ is WN and Ï€jâˆˆL withâˆ‘âˆ
Proof. By Lemma 2.1 in [4] it suï¬ƒces to show that there exists some Î±> 0 such that
such that CX(n),Xn+1
CX(n),Xn+1
Lâ‰¤ Î±CX(n)L nâˆˆ Z.
L(H n, H), and on the left-hand side, it is the operator norm on L(H n, H n). To ease

In the above equation, the norm used for the right-hand side is the operator norm on

= l0CX(n).

the representation we use the same notation being conï¬dent that this does not lead to

19

H, by repeatedly applying the Cauchy-Schwarz inequality,

Ï€jXn+1âˆ’jX(n),â‹…(cid:6)L
Ï€jEXn+1âˆ’jX(n),â‹…(cid:6)L
=Ï€1, . . . , Ï€nEX(n)X(n),â‹…(cid:6)+Q
j>n
â‰¤Ï€1, . . . , Ï€nLCX(n)L+Q
Ï€jLCX(n)1~2L CXn+1âˆ’j
1~2L
j>n
â‰¤Ï€1, . . . , Ï€nLCX(n)L+Q
Ï€jLCX(n)1~2L CX01~2L .
j>n

By stationarity CX0 = CXk for all k = 1, . . . , n. But CX0 is the projection of CX(n) âˆˆ
L(H n, H n) on the ï¬rst component in the following sense: with P1 deï¬ned as in (3.9),
CX0 = P1CX(n)P1. Hence, using Theorem 4.2.7 of [13], one can show that Î»j(CX0) â‰¤
Î»j(CX(n)) for jâˆˆ N, where Î»j(CX0) and Î»j(CX(n)) denote the j-th eigenvalues of CX0 and
CX(n), respectively. Since furthermore CX0L = Î»1(CX0) (e.g. [7], Theorem 4.9.8) and
CX(n)L= Î»1(CX(n)) we getCX0Lâ‰¤CX(n)L, and
Ï€jLCX(n)L.
EXn+1X(n),â‹…(cid:6)â‰¤Ï€1, . . . , Ï€nL+Q
j>n
The invertibility of(Xn)nâˆˆZ assures the boundedness of(Ï€1, . . . , Ï€n)+âˆ‘j>n
Example 4.7. Let(Xn)nâˆˆZ be a stationary functional AR(p) process with representation
Ï†j(Xnâˆ’j), nâˆˆ Z,
where(Îµn)nâˆˆZ is WN and Ï†jâˆˆS are Hilbert Schmidt operators. Then for nâ‰¥ p Proposi-
tion 4.5 applies, giving PGXn+1= s0 X(n) for some s0âˆˆS.

Note that an obvious special case of Example 4.6 is a functional autoregressive process

of ï¬nite order. In this case we can also apply Proposition 4.5 in an obvious way.

Xn= Îµn+ pQ
j=1

Ï€j.

Proof. We immediately get

CX(n),Xn+1

(â‹…)= EXn+1X(n),â‹…(cid:6)= E pQ
j=1
= E(Ï†1, . . . , Ï†p, 0, . . . , 0)X(n)X(n),â‹…(cid:6)
= Ï†CX(n)(â‹…),

Ï†jXn+1âˆ’jX(n),â‹…(cid:6)

misunderstandings. For Y, Zâˆˆ L2
CY,ZLâ‰¤CY1~2L CZ1~2L . Hence, we have
EXn+1X(n),â‹…(cid:6)L=E âˆQ
j=1

where Ï†=(Ï†1, . . . , Ï†p, 0, . . . , 0)âˆˆL(H n, H). Now let(ei)iâˆˆN be an orthonormal basis of H.
Then(fj)jâˆˆN with f1=(e1, 0, . . . , 0), f2=(0, e1, 0, . . . , 0), . . . , fn=(0, . . . , 0, e1), fn+1=
(e2, 0, . . . , 0), fn+2 =(0, e2, 0, . . . , 0), . . . , f2n =(0, . . . , 0, e2), f2n+1 =(e3, 0, . . . , 0), . . . is
an orthonormal basis of H n and, by orthogonality of(ei)iâˆˆN, we get
Ï†j(ei)2= pQ
j=1
since Ï†jâˆˆS for every j= 1, . . . , p, which implies that Ï†âˆˆS(H n, H).

Ï†j(ei)2= pQ
j=1

Ï†(fj)2=Q
iâˆˆN

Ï†1~2S =Q
jâˆˆN

Ï†j2L<âˆ,

pQ
j=1

Q
iâˆˆN

20

represented as an autoregressive process of ï¬nite order, where the operators in the inverse
representation are still Hilbert-Schmidt operators. Then the statement follows from the
arguments of the proof of Example 4.7.

Example 4.8. Let(Xn)nâˆˆZ be a stationary functional MA(1) process
Xn= Îµn+ Î¸(Îµnâˆ’1) nâˆˆ Z,
where(Îµn)nâˆˆZ is WN,Î¸L< 1, Î¸âˆˆS and Î¸ nilpotent, such thatÎ¸jL= 0 for j> j0 for
some j0âˆˆ N. Then for n> j0 Proposition 4.5 applies.
Proof. Since Î¸L < 1, (Xn)nâˆˆZ is invertible, and since Î¸ is nilpotent, (Xn)nâˆˆZ can be
Example 4.9. Let(Xn)nâˆˆZ be a stationary functional MA(1) process
where(Îµn)nâˆˆZ is WN, and denote by CÎµ the covariance operator of Îµ0. Assume thatÎ¸L<
1. If Î¸ and CÎµ commute, then there exists an s0âˆˆS such that CXn,Xn+1= s0CXn.
Proof. Stationarity of(Xn)nâˆˆZ ensures that CXn,Xn+1= CX0,X1. Let Î¸âˆ— denote the adjoint
operator of Î¸. Since Î¸CÎµ= CÎµÎ¸, we have that CX1,X0= CX0,X1 which implies Î¸CÎµ= CÎµÎ¸âˆ—=
CÎµÎ¸. Hence, CÎµ= CX0âˆ’ Î¸CÎµÎ¸âˆ—= CX0âˆ’ Î¸CÎµÎ¸= CX0âˆ’ Î¸2CÎµ, and we get by iteration
CX1,X0= Î¸CÎµ= Î¸(CX0âˆ’ Î¸CÎµÎ¸)=Q
(âˆ’Î¸2)j(Î¸CX0)=(Id+ Î¸2)âˆ’1Î¸CX0,
jâ‰¥0
where Id+ Î¸2 is invertible, since Î¸L < 1. Furthermore, since the space S of Hilbert-
operators and Î¸âˆˆS, also(Id+ Î¸2)âˆ’1Î¸âˆˆS.

Schmidt operators forms an ideal in the space of bounded linear (e.g. [8], Theorem VI.5.4.)

Xn= Îµn+ Î¸(Îµnâˆ’1) nâˆˆ Z,

4.3 Bounds for the error of the vector predictor
We are now ready to derive bounds for the prediction error caused by the dimension
reduction. More precisely, we want to compare the predictor

as deï¬ned in (4.4), and based on the vector process, with the functional best linear pre-
dictor

Xk, Î½jÎ½j=(Î½1, . . . , Î½d)Ì‚Xn+1
n+1= PGXn+1,
as deï¬ned in (4.7). We ï¬rst compare them on sp{Î½1, . . . , Î½d} where the vector representa-
Ì‚Xn+1=( Xn+1, Î½1, . . . ,
and Ì‚XG
We give Assumptions under which for dâ†’âˆ the mean squared distance between the
vector best linear predictor Ë†Xn+1 and the vector Ë†XG

n+1, Î½d
n+1âˆ¶= Ë†X G
n+1 becomes arbitrarily small.

n+1, Î½1 , . . . , Ë†X G

tions are

.

(4.9)

Ì‚Xn+1âˆ¶= dQ
j=1

Ë†X G

Xn+1, Î½d)

21

Ë†XG

Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

ni

(4.10)

Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯Ã¯

(4.11)

(4.12)

â‹®

. . .

. . .

i ,

such that

â‹®

. . .

. . .

Using a vector representation, we can write

n+1 is given by
Xi, Î½lâ€² gni(Î½lâ€²) , Î½l= nQ
âˆQ
i=1
lâ€²=1
gni(Î½d+1) , Î½1
gni(Î½d+1) , Î½d

âˆQ
lâ€²=1
gni(Î½d) , Î½1
gni(Î½d) , Î½d

â‹®

â‹®

gni(Xi), Î½l= nQ
i=1

For l= 1, . . . , d, the l-th component of Ì‚XG
 Ë†X G
n+1, Î½l= nQ
Xi, Î½lâ€²gni(Î½lâ€²) , Î½l .
i=1
Xi, Î½1


â‹®
gni(Î½1) , Î½1
Xi, Î½d
n+1= nQ
â‹®
gni(Î½1) , Î½d
Xi, Î½d+1
i=1
â‹®
=âˆ¶ nQ
GniXi+ dQ
Gâˆ
niXâˆ
i=1
i=1
where Gni is a dÃ— d matrix with llâ€²-th componentgni(Î½lâ€²), Î½l and Gâˆ
ni is a dÃ—âˆ matrix
with llâ€²-th componentgni(Î½d+lâ€²), Î½l.
Moreover, for all Y âˆˆ G there exist (possibly unbounded) linear operators tn1, . . . , tn,n
Similar to (4.10), we project Y âˆˆ G on Î½1, . . . , Î½d, which results in
tni(Xi), Î½d

tni(Xi).
Y = nQ
i=1
Yâˆ¶=(Y, Î½1 , . . . ,Y, Î½d)
= nQ
tni(Xi), Î½1, . . . , nQ
i=1
i=1
âˆ¶= nQ
TniXi+ nQ
Tâˆ
niXâˆ
i=1
i=1
The dÃ— d matrix Tni and the dÃ—âˆ matrix Tâˆ
Gni and Gâˆ
Mâˆ¶=Y=(Y, Î½1 , . . . ,Y, Î½d)âˆ¶ Y âˆˆ G
Observing that for all Y1âˆˆ M1 there exist dÃ— d matrices An1, . . . , Ann such that Y1=
i=1 AniXi, one can ï¬nd operators tni such that Tni= Ani, and Tâˆ
= 0, which then gives
âˆ‘n
Y1âˆˆ M. Hence M1âŠ† M.
the mean squared distance E Ë†Xn+1âˆ’ Ë†XG
n+12
Theorem 4.10. Suppose(Xn)nâˆˆZ is a functional ARMA(p, q) process such that Assump-
n+1 be the functional best linear predictor of Xn+1 as deï¬ned in (4.7)
tion 3.7 holds. Let Ë†X G
n+1 be as deï¬ned in (4.9). Let furthermore Ë†Xn+1 be the vector best linear predictor
âˆš
and let Ë†XG
Î»l<âˆ, for all dâˆˆ N,
(i) In the framework of Proposition 4.4, and ifâˆ‘âˆ
of Xn+1 based on X1, . . . , Xn as in (4.2).

l=1
E Ë†Xn+1âˆ’ Ë†XG
n+12
gniL2 âˆQ
Î»l2<âˆ.
â‰¤ 4 nQ
i=1
l=d+1
22

Now that we have introduced the notation and the setting, we are ready to compute

ni in (4.11). We denote by M the space of all Y:

ni in (4.12) are deï¬ned in the same way as

(4.13)

(4.14)

i .

2.

2

(4.15)

(4.16)

(4.17)

 âˆQ
l=d+1

22 âˆQ
l=d+1

Î»l<âˆ.

We start with a technical lemma, which we shall need for the proof of the above

n+1, Î½lY, Î½j(cid:6)= 0,
EXn+1âˆ’ Ë†X G
Y âˆˆ G.
sl,j(x)=x, Î½l Î½jâ‰¤xâ‰¤ 1,

(ii) In the framework of Proposition 4.5, for all dâˆˆ N,
E Ë†Xn+1âˆ’ Ë†XG
n+12
â‰¤ 4 nQ
gni(Î½l)2 1
i=1
In both cases, E Ë†Xn+1âˆ’ Ë†XG
n+12
2 tends to 0 as dâ†’âˆ.
Lemma 4.11. Suppose(Xn)nâˆˆZ is a stationary and causal functional ARMA(p, q) process
and(Î½l)lâˆˆZ are the eigenfunctions of its covariance operator C. Then for all j, lâˆˆ Z
Proof. For all j, lâˆˆ Z we set sl,j(â‹…)âˆ¶=â‹…, Î½l Î½j. We ï¬rst show that sl,j âˆˆL. Since for all
xâˆˆ H withxâ‰¤ 1,
hence sl,jâˆˆL. Since G is anL-closed subspace, Y âˆˆ G implies sl,j(Y)âˆˆ G and we get with
Remark 4.3(i) for all j, lâˆˆ Z,
n+1, Î½lY, Î½j(cid:6)= 0.
n+1, sl,j(Y)= EXn+1âˆ’ Ë†X G
EXn+1âˆ’ Ë†X G
Proof of Theorem 4.10. First, notice that both under (i) and (ii) there exist gni âˆˆL
i=1 gniXn+1âˆ’i, and thatS âŠ‚L. Furthermore, recall thatâ‹…2 denotes
n+1=âˆ‘n
the Euclidean norm in Rd and , Rd the corresponding scalar product. Now, using the
EY, Î½jXn+1âˆ’ Ë†X G
dQ
j=1

= EY, Xn+1âˆ’ nQ
niXâˆ
Gâˆ
i=1
Y=(Y, Î½1, . . . ,Y, Î½d)âˆˆ M.
GniXiâˆ’ nQ
EY1, Xn+1âˆ’ nQ
Gâˆ
niXâˆ
i=1
i=1
= EY1,
GniXi
EY1, Ë†Xn+1âˆ’ nQ
nQ
Combining (4.20) and Remark 4.3(i), we have
i=1
i=1

Since (4.18) holds for all Yâˆˆ M and M1âŠ† M, it especially holds for all Y1âˆˆ M1; i.e.,

= 0, Y1âˆˆ M1.
, Y1âˆˆ M1.


n+1,Y, Î½l Î½j= EXn+1âˆ’ Ë†X G

n+1 in (4.11) and Lemma 4.11, we obtain
n+1, Î½j= EY, Xn+1âˆ’ Ë†XG
n+1
= 0, Y âˆˆ G,
GniXiâˆ’ nQ
i=1

Rd

Rd

i

matrix representation of Ë†XG

Gâˆ
niXâˆ

i

Rd

where we have set

(4.18)

(4.19)

(4.20)

such that Ë†X G



i

Rd

Rd

23

2

Theorem.

(4.21)

and for the right hand side of (4.23) we get by the Cauchy-Schwarz inequality applied
twice,

Dividing the right hand side of (4.24) by the ï¬rst square root on the right hand side of
(4.25) we obtain

(4.22)



Rd

.

(4.23)

,

(4.24)

i

i

2

 1

2

2

2

. (4.25)

(4.26)

(4.27)

Since both Ë†Xn+1 and nâˆ‘
i=1

Rd

Rd

2

2



Rd

2

i

GniXi,

GniXi,

Gâˆ
niXâˆ

i

GniXi are in M1, (4.21) especially holds, when

E Ë†Xn+1âˆ’ nQ
i=1

We plug Y1 as deï¬ned in (4.22) in (4.21) and obtain

GniXi, Ë†Xn+1âˆ’ nQ
E Ë†Xn+1âˆ’ nQ
i=1
i=1
E Ë†Xn+1âˆ’ nQ
i=1
nQ
i=1

GniXiâˆˆ M.
Y1= Ë†Xn+1âˆ’ nQ
i=1
= E Ë†Xn+1âˆ’ nQ
GniXi
nQ
Gâˆ
niXâˆ
i=1
i=1
GniXi2
= E Ë†Xn+1âˆ’ nQ
GniXi
GniXi, Ë†Xn+1âˆ’ nQ
From the left hand side of (4.23) we read oï¬€
i=1
i=1
GniXi
â‰¤ E Ë†Xn+1âˆ’ nQ
i=1
GniXi2
â‰¤E Ë†Xn+1âˆ’ nQ
i=1
â‰¤ E nQ
GniXi2
E Ë†Xn+1âˆ’ nQ
Gâˆ
niXâˆ
i=1
i=1
= E Ë†Xn+1âˆ’ nQ
GniXiâˆ’ nQ
E Ë†Xn+1âˆ’ Ë†XG
n+12
Hence, for the mean squared distance we get
Gâˆ
niXâˆ
i=1
i=1
â‰¤ 2E Ë†Xn+1âˆ’ nQ
GniXi2
+ 2E nQ
i=1
i=1
â‰¤ 4E nQ
2
Gâˆ
niXâˆ
i=1
What remains to do is to bound nâˆ‘
Gâˆ
niXâˆ
i=1
Xi, Î½lâ€²gni(Î½â€²
), Î½l= nQ
i=1
âˆQ
lâ€²=d+1

 nQ


Gâˆ
niXâˆ
i=1
2E nQ
 1
Gâˆ
niXâˆ
i=1
2

nQ
âˆQ
with l-th component
i=1
lâ€²=d+1
E nQ
i=1

calculate

âˆQ
lâ€²=d+1
xi,lâ€²gni(Î½â€²

2
Gâˆ
niXâˆ

2

i

xi,lâ€²gni(Î½â€²

= E dQ
l=1

), Î½l2

Gâˆ
niXâˆ

i

), Î½l.

2

2

2

2

i

.

2

i

.

2

i

2

2

 nQ
i=1
24

2

l

i , which, by (4.11), is a d-dimensional vector

(i) First we consider the framework of Proposition 4.4. We abbreviate xi,lâ€²âˆ¶=Xi, Î½â€²

l

l

 and

l

by Parsevalâ€™s identity. Then we proceed using the linearity and orthogonality of Î½l and
the Cauchy-Schwarz inequality

(4.28)

(4.29)

âˆš
Î»l<
i=1 gniXn+1âˆ’i.

gn,j(Î½lâ€²)2 1
2

(4.30)

l

l

l

xi,lâ€²gni(Î½â€²
xi,lâ€²gni(Î½â€²
)2

l

), Î½lÎ½l2
), Î½lÎ½l2

 nQ
= E dQ
âˆQ
i=1
lâ€²=d+1
l=1
â‰¤ E âˆQ
 nQ
âˆQ
i=1
l=1
lâ€²=d+1
=E nQ
xi,lâ€²gni(Î½â€²
âˆQ
i=1
lâ€²=d+1
= E nQ
xi,lgni(Î½l),
xj,lâ€²gn,j(Î½â€²
)
âˆQ
âˆQ
nQ
i=1
j=1
l=d+1
lâ€²=d+1
= nQ
E(xi,lxj,lâ€²)gni(Î½l), gn,j(Î½lâ€²)
âˆQ


i,j=1
l,lâ€²=d+1
E(xj,lâ€²)2gn,j(Î½lâ€²)
E(xi,l)2gni(Î½l) nQ
â‰¤ nQ
âˆQ
âˆQ


j=1
i=1
lâ€²=d+1
l=d+1
Î»lgni(Î½l) nQ
Î»lâ€²gn,j(Î½lâ€²),
= nQ
âˆQ
âˆQ
j=1
i=1
lâ€²=d+1
l=d+1
since EXi, Î½l2= Î»l by (2.8). Then using the linearity of the operators


Î»lgniLÎ½l nQ
â‰¤ nQ
Î»lâ€²gniLÎ½lâ€²
âˆQ
âˆQ


i=1
i=1
lâ€²=d+1
l=d+1
Î»lâ€²gniL
Î»lgniL nQ
= nQ
âˆQ
âˆQ

i=1
i=1
lâ€²=d+1
l=d+1
Î»l2
= nQ
gniL2 âˆQ
i=1
l=d+1
sinceÎ½l= 1. Now since gniâˆˆL, we haveâˆ‘n
i=1gniL<âˆ for all nâˆˆ N and withâˆ‘âˆ
âˆ, the right hand side tends to 0 as dâ†’âˆ.
l=1
n+1=âˆ‘n
(ii) In the framework of Proposition 4.5 there exist gniâˆˆS such that Ë†X G


Î»lâ€²gn,j(Î½lâ€²)
Î»lgni(Î½l) nQ
E nQ
âˆQ
âˆQ
j=1
i=1
l=d+1
lâ€²=d+1
Î»l 1
 âˆQ
Î»lâ€² 1
 âˆQ
2 nQ
gni(Î½l)2 1
2 âˆQ
2 âˆQ
j=1
l=d+1
lâ€²=d+1
l=d+1
lâ€²=d+1
22 âˆQ
gni(Î½l)2 1
 âˆQ
l=d+1
l=d+1
gni(Î½l)2â‰¤gniS<âˆ.
âˆQ
l=d+1
gni 1
22 âˆQ
gni(Î½l)2 1
2S2 âˆQ
l=d+1
l=d+1
25

 âˆQ
Thus, (4.30) is bounded by
l=d+1

Then, similarly as before, using the Cauchy-Schwarz inequality, we calculate

â‰¤ nQ
i=1
â‰¤ nQ
i=1
= nQ
i=1

Î»lâ‰¤ nQ
i=1

Î»l<âˆ,

Gâˆ
niXâˆ

i

2

2

Now note that

 nQ
i=1

,

Î»l

n

(4.31)

(3.1). Then for Ïƒ2

n as deï¬ned in (4.8),

Proof. First note that by orthogonality of Î½1, . . . , Î½d,

+ Î³d;n.
âˆš
Î»l<âˆ, for all dâˆˆ N,
Î»l2+ âˆQ
l=d+1

Î»l.


such that (4.30) tends to 0 as dâ†’âˆ.
We are now ready to provide bounds of the mean squared prediction error EXn+1âˆ’ Ë†Xn+12.
Theorem 4.12. Consider a stationary and causal functional ARMA(p, q) process as in
EXn+1âˆ’ Ë†Xn+12â‰¤ Ïƒ2
(i) In the framework of Proposition 4.4, and ifâˆ‘âˆ

l=1
Î³d;n= 4 nQ
gniL2 âˆQ
i=1
l=d+1
(ii) In the framework of Proposition 4.5, for all dâˆˆ N,
Î»l4 gn;d+ 1
Î³d;n= âˆQ
l=d+1
gni(Î½l)21~2â‰¤ nQ
 âˆQ
gn;d= nQ
i=1
i=1
l=d+1
In both cases, EXn+1âˆ’ Ë†Xn+12
n as dâ†’âˆ.
2 tends to Ïƒ2
EXn+1âˆ’ Ë†Xn+12= E dQ
Xn+1âˆ’ Ë†Xn+1, Î½lÎ½l+ âˆQ
Xn+1, Î½lÎ½l2
l=1
l=d+1
EXn+1âˆ’ Ë†Xn+1, Î½lÎ½l2+ âˆQ
= dQ
EXn+1, Î½lÎ½l2
l=1
l=d+1
= dQ
EXn+1âˆ’ Ë†Xn+1, Î½l2+ âˆQ
l=1
l=d+1
EXn+1âˆ’ Ë†Xn+1, Î½l2= EXn+1âˆ’ Ë†Xn+12
n+1, Y= 0 for all Y âˆˆ G. Observing that Ë†X G
EXn+1âˆ’ Ë†X G
n+1âˆ’ Ë†Xn+1= 0,
n+1, Ë†X G
n+1âˆ’ Ë†Xn+1, Î½lâ€²= 0,
n+1, Î½l Ë†X G
n+12
2= EXn+1âˆ’ Ë†XG
2+ E Ë†XG

by (2.8) and the fact thatÎ½l= 1 for all lâˆˆ N. Now recall that similarly as in the ï¬rst
Furthermore, by Deï¬nition 4.2 of L-closed subspaces and Remark 4.3(i) we know that
EXn+1âˆ’ Ë†X G
n+1âˆ’ Ë†Xn+1âˆˆ G, we conclude that

EXn+1âˆ’ Ë†X G
EXn+1âˆ’ Ë†Xn+12

l, lâ€²âˆˆ N.
n+1âˆ’ Ë†Xn+12

2,

and, by Lemma 4.11,

Hence,

equation of (4.28)

dQ
l=1

gni2S.

2.

Î»l

(4.32)

26

(4.33)

n+12
EXn+1âˆ’ Ë†XG
2= E

n+1, Î½l2â‰¤ âˆQ
where for the ï¬rst term of the right hand side,
l=1

Xn+1âˆ’ Ë†X G
dQ
l=1

n,
(4.34)
and the last equality holds by Remark 4.3(iii). For the second term of the right hand side
of (4.33) we use Theorem 4.10. We ï¬nish the proof of both (i) and (ii) by plugging (4.33)
and (4.34) into (4.32).

Xn+1âˆ’ Ë†X G

n+1, Î½l2= EXn+1âˆ’ Ë†X G

n+12= Ïƒ2

Figure 3: Functional velocity data (black) and raw data (grey) on the last ten working days in
June 2014 (June 19th 2014 was a catholic holiday).

5 Real data analysis
In this section we apply the functional time series prediction theory to highway traï¬ƒc
data provided by the Autobahndirektion SÂ¨udbayern, thus extending previous work by [2].
Our dataset consists of measurements at a ï¬xed point on a highway (A92) in Southern
Bavaria, Germany. Recorded is the average velocity per minute from 1/1/2014 00:00 to
30/06/2014 23:59 on three lanes. After taking care of missing values and data outliers,
we merge the three lanes (using the weighted average velocity per minute). Finally, we
smooth the cleaned daily high-dimensional data, using a Fourier basis to obtain functional
data. In Figure 3 we depict the outcome on the working days of two weeks in June 2014.
For a precise description we refer to Wei [17], Chapter 6.

As can be seen in Figure 4, diï¬€erent weekdays have diï¬€erent mean velocity functions.
To account for the diï¬€erence between weekdays, we subtract the empirical individual
daily mean from all daily data (Monday mean from Monday data, etc.). The eï¬€ect is
clearly visible in Figure 5. However, even after deduction of the daily mean, functional
stationarity tests [12] reject stationarity of time series. This is due to traï¬ƒc ï¬‚ow on
weekends: Saturday and Sunday traï¬ƒc show diï¬€erent patterns than weekdays, even after
mean correction. Consequently, we restrict our investigation to working days (Monday-

Friday), resulting in a functional time series Xn for n= 1, . . . , N= 119.

27

40608010012006âˆ’16(M)06âˆ’17(Tu)06âˆ’18(W)06âˆ’20(F)06âˆ’23(M)06âˆ’24(Tu)06âˆ’25(W)06âˆ’26(Th)06âˆ’27(F)06âˆ’30(M)Velocity (km/h)raw datafunctional dataFigure 4: Empirical functional mean velocity on the 7 days of the week

Figure 5: Functional velocity data for 30 working days smoothed by a Fourier basis before and
after substracting the weekday mean

28

9095105115MTuWThFSaSu0:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity(km/h).60801001200:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity (km/h)âˆ’60âˆ’40âˆ’200200:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity (km/h)A Portmanteau test applied to Xn for n= 1, . . . , N with N = 119 working days (cf.
[9]) rejects (with a p-value as small as 10âˆ’6) that the daily functional data are uncorre-
data on working days, hence the empirical version of E[(X(t)âˆ’ Âµ(t))(X(s)âˆ’ Âµ(s))], for
0â‰¤ t, sâ‰¤ 1, based on 119 working days.

lated. Furthermore, the stationarity tests suggested in [12] do not reject the stationarity
assumption.

Figure 6 shows the empirical covariance kernel for the highway functional velocity

Figure 6: Empirical covariance kernel of functional velocity data on 119 working days.

As indicated by the arrows, the point(t, s)=(0, 0) is at the bottom right corner and

estimates the variance at midnight. The empirical variance over the day is represented
along the diagonal from the bottom right to the top left corner. The valleys and peaks
along the diagonal represent phases of low and high traï¬ƒc density: for instance, the ï¬rst
peak represents the variance at around 05:00 a.m., where traï¬ƒc becomes denser, since
commuting to work starts. Peaks away from the diagonal represent high dependencies
between diï¬€erent time points during the day. For instance, high traï¬ƒc density in the
early morning correlates with high traï¬ƒc density in the late afternoon, again due to
commuting.
Remark 5.1. We want to emphasize that we have developed the prediction theory in its
natural framework of a Hilbert space, which follows from the projection theorem. This
requires only second order stationarity of all processes involved. Second order stationarity

follows from the fact that we used WN as driving process of the functional ARMA(p, q)
as driving process of the functional ARMA(p, q) equations.

equations. Consistency of the empirical estimators of e.g. the covariance operator, how-
ever, hold under strict stationarity (see e.g. [10]). Strict stationarity of our models (both
functional and vector models) follows immediately from using SWN (cf. Deï¬nition 3.1 (ii))

29

HoursHoursAll our results remain valid under this more restrictive condition of SWN driving process
with the obvious modiï¬cations.



Figure 7: Four empirical eigenfunctions of the N = 119 working days functional velocity data.

The criterion is 80%; i.e., Î½1, Î½2, Î½3, Î½4 explain together 80% of the total data variability.

Based on the empirical covariance operator with kernel represented in Figure 6 we
compute its empirical eigenvalues and eigenfunctions (cf. Figure 7) that we denote by Î»e
and Î½e
functional velocity data and implement the following steps.

j , for j= 1, . . . , N. We are then ready to apply the Algorithm of Section 4.1 to the
â€œCPV(d) vs. dâ€ plot we read oï¬€ that d= 4 functional principal componens explain 80% of
the variability of the data. Now for each day nâˆˆ{1, . . . , N}, we use the Karhunen-LoÂ´eve

(1) We apply the CPV method to the highway functional velocity data. From a

j

Theorem 2.2 and truncate the daily functional velocity curve Xn. This yields

In Figure 8 we show the (centered) functional velocity data and the corresponding trun-
cation.

 Î½e
j , n= 1, . . . , N= 119.
We store the d= 4 scores in the vector Xn,
1 , . . . ,Xn, Î½e
(2) We now ï¬t diï¬€erent vector ARMA(p, q) models to the multivariate vector data and

4), n= 1, . . . , N= 119.

Xn=(Xn, Î½e

Xn,dâˆ¶= dQ
j=1

Xn, Î½e

j

compare the goodness of ï¬t of the models by their prediction error. We summarize root
mean squared errors (RMSE) and mean absolute errors (MAE) for the diï¬€erent models in
Table 5.1. To be able to evaluate the performance of the diï¬€erent models, we use standard
non-parametric prediction methods from the literature in comparison. All of the linear
models signiï¬cantly outperform methods like exponential smoothing or naively predicting
with the mean of the time series. Again details are given in [17]. We ï¬nd minimal errors

for VAR(2) and VMA(1) models, where both prediction errors are equal in case of the
MAE, and the RMSE for the VAR(2) model is slightly smaller than that for the VMA(1)
model. Since we opt for a parsimonious model, we choose the VMA(1) model, which we

ï¬t to the data.

30

âˆ’10120:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00n1n2n3n4Figure 8: Functional velocity raw data on 5 consecutive working days (black) versus the truncated
data by the Karhunen-LoÂ´eve representation (grey). The criterion is 80% and the resulting number
d of FPCâ€™s is 4.

Using the model ï¬t of the VMA(1) model, we compute the best linear predictor Ì‚Xn+1

as in (4.2).

Model ï¬t AR(1) AR(2) MA(1) MA(2) ARMA(1, 1)

RMSE
MAE

4.05
3.19

3.87
3.06

3.89
3.06

4.78
3.77

4.50
3.59

Table 5.1: Average prediction errors of the predictors for the last 10 observations for all working
days

(3) We re-transform the vector best linear predictorÌ‚Xn+1 into its functional form Ì‚Xn+1,

which is depicted in Figure 9.

Figure 9: Functional velocity data in black and one-step functional predictor based on VMA(1)

in grey for the last working days in June 2014

Acknowledgement: We thank the Autobahndirektion SÂ¨udbayern and especially J.
GrÂ¨otsch for their support und for providing the traï¬ƒc data. J. Klepsch furthermore ac-
knowledges ï¬nancial support from the Munich Center for Technology and Society based

31

âˆ’20âˆ’100102014âˆ’04âˆ’14(M)2014âˆ’04âˆ’15(Tu)2014âˆ’04âˆ’16(W)2014âˆ’04âˆ’17(Th)2014âˆ’04âˆ’18(F)2014âˆ’04âˆ’19(Sa)Velocity(km/h)functionaltruncated70809010012006âˆ’16(M)06âˆ’17(Tu)06âˆ’18(W)06âˆ’20(F)06âˆ’23(M)06âˆ’24(Tu)06âˆ’25(W)06âˆ’26(Th)06âˆ’27(F)06âˆ’30(M)Velocity (km/h)functional dataVMA(1) predictorproject ASHAD.

References
[1] A. Aue, D. Norinho, and S. HÂ¨ormann. On the prediction of stationary functional

time series. Journal of the American Statistical Association, 110:378â€“392, 2015.

[2] P. Besse and H. Cardot. Approximation spline de la prÂ´evision dâ€™un processus func-
tionnel autorÂ´egressive dâ€™ordre 1. Canadian Journal of Statistics, 24:467â€“487, 1996.

[3] D. Bosq. Linear Processes in Function Spaces: Theory and Applications. Springer

New York, 2000.

[4] D. Bosq. General linear processes in Hilbert spaces and prediction. Journal of

Statistical Planning and Inference, 137:879â€“894, 2007.

[5] D. Bosq. Computing the best linear predictor in a Hilbert space. Applications to

general ARMA processes. Journal of Multivariate Analysis, 124:436â€“450, 2014.

[6] P.J. Brockwell and R.A. Davis. Time Series: Theory and Methods (2nd Ed.).

Springer, New York, 1991.

[7] L. Debnath and P. MikusiÂ´nski.

Academic Press, 1999.

Introduction to Hilbert Spaces with Applications.

[8] N. Dunford and J.T. Schwartz. Linear Operators Part 1 General Theory. Wiley,

1988.

[9] R. Gabrys and P. Kokoszka. Portmanteau test of independence for functional obser-

vations. Journal of the American Statistical Association, Vol. 102(No. 480), 2007.

[10] S. HÂ¨ormann and P. Kokoszka. Weakly dependent functional data. The Annals of

Statistics, 38(3):1845â€“1884, 2010.

[11] L. HorvÂ´ath and P. Kokoszka.

Springer, New York, 2012.

Inference for Functional Data with Applications.

[12] L. HorvÂ´ath, P. Kokoszka, and G. Rice. Testing stationarity of functional time series.

Journal of Econometrics, 2013.

[13] T. Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with

an Introduction to Linear Operators. Wiley, 2015.

[14] V. Kargin and A. Onatski. Curve forecasting by functional autoregression. Journal

of Multivariate Analysis, 99:2508â€“2526, 2008.

[15] J.O. Ramsay and B.W. Silverman. Functional Data Analysis. Springer, New York,

2005.

32

[16] F. Spangenberg. Strictly stationary solutions of ARMA equations in Banach spaces.

Journal of Multivariate Analysis, 121:127â€“138, 2013.

[17] T. Wei. Time series in functional data analysis. Masterâ€™s thesis, Technical University

of Munich, 12 2015.

33

