A LEARNING-BASED FRAME POOLING MODEL FOR EVENT DETECTION

Jiang Liu(cid:63)

Chenqiang Gao(cid:63)

Lan Wang(cid:63)

Deyu Mengâ€ 

(cid:63) Chongqing Key Laboratory of Signal and Information Processing,

Chongqing University of Posts and Telecommunications, Chongqing, China

â€  School of Mathematics and Statistics,
Xiâ€™an Jiaotong University, Xiâ€™an, China

6
1
0
2

 
r
a

M
7

 

 
 
]

V
C
.
s
c
[
 
 

1
v
8
7
0
2
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT

Detecting complex events in a large video collection crawled
from video websites is a challenging task. When applying
directly good image-based feature representation, e.g., HOG,
SIFT, to videos, we have to face the problem of how to pool
multiple frame feature representations into one feature repre-
sentation. In this paper, we propose a novel learning-based
frame pooling method. We formulate the pooling weight
learning as an optimization problem and thus our method
can automatically learn the best pooling weight conï¬gura-
tion for each speciï¬c event category. Experimental results
conducted on TRECVID MED 2011 reveal that our method
outperforms the commonly used average pooling and max
pooling strategies on both high-level and low-level 2D image
features.

Index Termsâ€” event detection, feature pooling, alterna-

tive optimization

1. INTRODUCTION

Complex event detection aims to detect events, such as â€œmar-
riage proposalâ€, â€œrenovating a homeâ€, in a large video col-
lection crawled from video websites, like Youtube. This
technique can be extensively applied to Internet video re-
trieval, content-based video analysis and machine intelli-
gence ï¬elds and thus has recently attracted much research
attention[1, 2, 3, 4, 5]. Nevertheless, the complex event de-
tection encounters lots of challenges, mostly because events
are usually more complicated and undeï¬nable, possessing
great intra-class variations and variable video durations, as
compared with traditional concept analysis in constrained
video clips, e.g., action recognition. These factors make this
technique far from being applicable to practical use with
robust performance.

A large number of methods have been proposed to han-
dle this challenging task[6, 7, 8, 9]. Generally speaking, the
video representation is one of the most important compo-
nents. For many techniques to extract the video representa-
tion, namely feature descriptors, have to be carefully designed
or selected for good detection performance. Different from

images, video clips can be treated as spatial-temporal 3D
cuboids. Lots of spatial-temporal oriented feature descrip-
tors have been proposed and been proved effective, such as
HOG3D[10], MoSIFT[11], 3DSIFT[12] and the state-of-the-
art improved Dense Trajectory(IDT)[13]. Although these
spatial-temporal descriptors can intrinsically describe videos,
the 2D image descriptors are still very important for describ-
ing videos in the complex event detection community due to
two aspects. On one hand, compared with 2D image descrip-
tors, the spatial-temporal feature descriptors usually require
larger data storage and higher computational complexity to be
extracted and processed. This problem becomes more serious
for large scale datasets. On the other hand, the TRECVID
Multimedia Event Detection (MED) evaluation track[14] of
each year, held by NIST, reveals that combining kinds of
feature descriptors, including 2D and 3D features, usually
outperforms those of using a single feature descriptor[15].

Proï¬ting from the research development in image repre-
sentations, a number of good handcrafted features, including
low-level ones of such HOG[16], SIFT[17], and high-level
features of such Object-bank[18] along with the recently most
successful Convolutional Neural Network(CNN) feature[19]
can be directly applied to describe the video. The commonly-
used strategy is to extract the feature representation for each
frame or selected key frame of the video (we will use frame
hereinafter) and then pool all feature representations into
one representation with average pooling or max pooling[20].
While the max pooling just uses the maximum response of all
frames for each feature component, the average pooling uses
their average value. It is hard to say which one of two pool-
ing strategies is better. Sometimes, average pooling is better
than max pooling and vice versa. The performance heavily
depends on the practical application or datasets. The actual
strategy is manually choosing the better one through exper-
iments conducted on validation set. Therefore, intuitively,
here comes two questions: 1) can we automatically choose
the better one between the two previous pooling strategies? 2)
is there any pooling method superior to these two strategies?
To answer these two questions mentioned above, we pro-
pose a novel learning-based frame pooling method. We no-

tice that when human beings observe different events, they
usually have different attention on various frames, i.e., the
pooling weight for a particular event is inconsistent with the
others.This pheneomenon inspires us to adaptively learn the
optimal pooling way from data. In other words, our approach
can automatically derive the best pooling weight conï¬gura-
tion for each speciï¬c event category. To this end, we design
an alternative search strategy, which embeds the optimiza-
tion process for frame pooling weight and classiï¬er param-
eters into an unifying optimization problem. In this way, for
a given test video clip and its pre-processed frame-level fea-
ture matrix, our optimum pooling parameters can represent
it with a discriminative ï¬nal feature z and a classiï¬er is em-
ployed to present the classiï¬cation result, as shown in Figure
1. Experimental results conducted on TRECVID MED 2011
reveal that our learning-based frame pooling method outper-
forms the commonly used average pooling and max pooling
strategies on both high-level and low-level 2D image features.

The rest part of this paper is organized as following. In
Section 2, we present our proposed methodology for video
description task. Section 3 shows the experimental results
with various low-level and high-level features. The conclu-
sion is ï¬nally given in Section 4.

2. THE PROPOSED METHOD

2.1. Interpolation

Our goal is to learn an uniform pooling weight setting for
each speciï¬c event. However, the number of frames extracted
from videos containing events are different due to different
video durations or frame sampling methods. To address this
problem, the interpolation operation is adopted.
Given a video clip Vi with Ti frames, we can get Ti
encoded feature vectors y(i,j)(t), t âˆˆ (1, 2, 3,Â·Â·Â· , Ti), j âˆˆ
(1, 2, 3,Â·Â·Â· , m). Here m is the dimension of the feature
in each frame. First, we construct a Lagrange interpolation
function Ëœfi,j(u) for the jth feature component as following:

Ti(cid:88)

t=1

(cid:81)tâˆ’1
k=1 (u âˆ’ k)(cid:81)Ti
(cid:81)tâˆ’1
k=1 (t âˆ’ k)(cid:81)Ti

k=t+1 (u âˆ’ k)
k=t+1 (t âˆ’ k)

Ëœfi,j (u) =

yi,j (t) ,

(1)

where Ëœfi,j(u) can ï¬t all the responses at each time (frame)
u in the original video clip. With the interpolated functions
for all feature components, we can re-sample a ï¬xed number
of the feature representations. Thus, the videos with various
durations are eventually able to re-normalized into ones with
the same number T of feature representations. However, we
would encounter the â€œover-ï¬ttingâ€ problem if directly con-
ducting interpolating operation on the original encoded fea-
tures. This is due to the fact that the original feature com-
ponents may varies greatly even between consecutive frames
and hence will cause the corresponding interpolation function
to vary dramatically in the feature space. This would produce
potential noise data. For the sake of alleviating this prob-
lem, we sort independently all features for each component
in descent order before constructing the Lagrange interpola-
tion function. In this way, the interpolation function will tend
to gradual decreasing in the feature space, we denote it as
fi,j (u). Later, we sample along the temporal axis for the jth
feature component with fi,j (u), denoted as xi,j:

xi,j =(cid:8)fi.j

(cid:0)ti

k

(cid:1)(cid:9) , k âˆˆ (1, 2, 3,Â·Â·Â· , T ),

(2)

Ti âˆ’ 1
T âˆ’ 1

k = 1 + (k âˆ’ 1)

where ti
, are the re-sampling points on
the interpolated function. For a given video clip, we combine
all sampled feature vectors together into a new feature matrix,
denoted as Xi = (xi,1, xi,2, xi,3...xi,m)T âˆˆ RmÃ—T .

Fig. 1. The test stage of our proposed method: after the pre-
processing to the original encoded frame features, the video
frames could be represented with a feature matrix Xi. We
further pool the feature matrix with our learning-based opti-
mal pooling weights Î¸, while the detection result is given by
a classiï¬er using the pooled feature z.

2.2. Formulation
Given n training samples (Xi, yi)(i = 1, 2,Â·Â·Â· , n), where
the Xi is the feature matrix obtained by Section 2.1 and yi
is the sample label, our goal is to learn a weight parame-
ter to compress the feature matrix Xi into a single feature
vector. Actually, for both average and max pooling meth-
ods, the pooling operation is done independently for each fea-
ture component. Intuitively, we should learn an independent

Î¸Î¸â€¦â€¦.â€¦â€¦â€¦â€¦â€¦.â€¦.mÃ—TTÃ—1Ã—mÃ—1video frames:ğ‘‰ğ‘–encoded frame feature:ğ‘¦ğ‘–,ğ‘—ğ‘¡Sort, Interpolation and Samplingğ‘‹ğ‘–in ğ‘…ğ‘šÃ—ğ‘‡Î¸ğ‘§ğ‘–,ğ‘—Pre-processingClassificationFeature pooling classificationresultâ€¦â€¦SVMclassificationOptimalLearningweight vector Î¸j(j = 1,Â·Â·Â· , m) for each component. How-
ever, this would make the model too complex to be learned
effectively. Instead, we learn a single weight vector Î¸ for all
components. Namely, we pool the features using the same
weight vector for all feature components as XiÎ¸. Because
our interpolation function fi,j will perform a decreasing prop-
erty in feature space, we can easily know that the cases of
Î¸ = (1/T,Â·Â·Â· , 1/T ) and Î¸ = (1, 0, 0,Â·Â·Â· , 0) approximately
correspond to average and max pooling strategies, respec-
tively. Furthermore, the medium and min pooling strategies
can also be approximately viewed as the speciï¬c cases, where
Î¸ = (0,Â·Â·Â· , 1,Â·Â·Â· , 0)(1 is located in the middle position of
the vector) and Î¸ = (0, 0,Â·Â·Â· , 1), respectively. Nevertheless,
our goal is to learn an optimal pooling strategy for each event.
To this end, the problem of pooling parameter Î¸ learning is
formulated as the following optimization problem:

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

min
w,b,Î¸

n(cid:88)

(cid:0)1 âˆ’ yi

(cid:0)wT XiÎ¸ + b(cid:1)(cid:1)

i=1

s.t Î¸ â‰¥ 0,

1
2

wT w,

Î¸k = 1,

(3)

+ +

T(cid:88)

k=1

where (Â·)+ = max(0,Â·) means the hinge-loss in the loss
function. Our model intends to minimize the objective func-
tion over w, b, which are the parameters of the hyperplane in
the SVM classiï¬er, along with our additional pooling param-
eter Î¸.

2.3. Solution

In order to solve the parameters of w, b, Î¸ in the model (3)
above, an alternative search strategy is employed. In general,
our alternative search strategy can be viewed as an iteration
approach with two steps in each round. The ï¬rst step in each
iteration is to update w, b with ï¬xed Î¸ by solving the following
sub-optimization problem:

n(cid:88)

(cid:0)1 âˆ’ yi

(cid:0)wT XiÎ¸ + b(cid:1)(cid:1)

+ +

1
2

wT w.

(w, b) = arg min

w,b

i=1

Î¸ â‰¥ 0,(cid:80)T

(4)
Here, we initialize Î¸ with random values with constraint that
k=1 Î¸k = 1. Equation (4) is the standard formula-
tion of a linear SVM problem and therefore can be solved via
off-the-shelf tools like libsvm[21].

The second step in an iteration is to search Î¸ by ï¬xing the
w, b obtained by the ï¬rst step. This step actually iteratively
updates an optimal pooling manner under current model pa-
rameter w, b:

n(cid:88)

(cid:0)1 âˆ’ yi

(cid:0)wT XiÎ¸ + b(cid:1)(cid:1)

+ .

(5)

Î¸ = arg min

Î¸

i=1

Because the operation between Xi and Î¸ is a linear inner prod-
uct, the second step in one iteration can also be solved via
Gradient Descent algorithm. In this degree, the overall objec-
tive function can be minimized with expected convergence by
iteratively searching for w, b and Î¸, respectively. The overall
algorithm is illustrated in Algorithm 1.

Algorithm 1 Alternative search strategy to obtain optimum
w, b, Î¸
Input: Xi, yi(the training set feature matrices and labels),
Output: learned parameter w, b, Î¸

1. Initialize Î¸ with random values, s.t. Î¸ â‰¥ 0,(cid:80)T

j=1 Î¸j =

1;

2. for k:=1 to N

(a) Fixing Î¸ and updating w,b:

(wk,bk)=arg minw,b
1
2

wT w;

(cid:80)n

(b) Fixing w, b and updating Î¸:

Î¸k=arg minÎ¸

i=1

k XiÎ¸ + bk

i=1

(cid:80)n
(cid:0)1 âˆ’ yi

(cid:0)1 âˆ’ yi
(cid:0)wT

(cid:1)(cid:1)

+.

(cid:0)wT XiÎ¸kâˆ’1 + b(cid:1)(cid:1)

++

end for

3. Return wN , bN and Î¸N ;

3. EXPERIMENTS

We evaluate our proposed model on the public large scale
TRECVID MED2011 dataset[14] with both low-level fea-
tures: HOG, SIFT, and high-level features: Object Bank-
based feature and CNN-based feature. We adopt the most
popular pooling methods of the max and average poolings as
the baseline methods for comparison.

3.1. Dataset and evaluation metric

The TRECVID MED 2011 development set is used to assess
our method. It contains more than 13,000 video clips over
18 different kinds of events and background class, which pro-
vides us with real life web video instances consisting of com-
plex events under different scenes lasting from a few seconds
to several minutes. We follow the original evaluation metric
along with the pre-deï¬ned training/test splits of MED 2011
development set. In the pre-processing stage, we empirically
interpolate each video clips into T = 20 frames. Besides,
each learning-based frame pooling model for individual event
class is trained with 100 times of iteration, which enables the
objective function to be minimized to convergent. Finally,
the average precision(AP) value is used to evaluate different
pooling approaches.

3.2. Results on low-level features

We use the off-the-shelf toolkit VLFeat[22] to extract HOG
and SIFT features with standard conï¬gurations for each
frame. It is worth noting that the SIFT descriptors are densely
extracted. Then the Bag-of-Words method is employed to en-
code the raw features from each frame into a 100 dimensional
vector. The results are listed in Table 1.

Event ID

Method
E001
E002
E003
E004
E005
E006
E007
E008
E009
E010
E011
E012
E013
E014
E015
P001
P002
P003
mAP

HOG
Max
0.435
0.320
0.511
0.307
0.217
0.175
0.112
0.269
0.357
0.136
0.080
0.144
0.126
0.177
0.104
0.162
0.379
0.066
0.226

Average
0.407
0.302
0.527
0.279
0.184
0.179
0.083
0.162
0.327
0.151
0.082
0.107
0.110
0.192
0.097
0.123
0.350
0.057
0.207

SIFT
Max
0.275
0.217
0.252
0.158
0.185
0.145
0.076
0.181
0.149
0.151
0.071
0.206
0.091
0.177
0.180
0.129
0.344
0.044
0.168

Average
0.270
0.207
0.290
0.140
0.142
0.098
0.081
0.197
0.103
0.113
0.085
0.141
0.107
0.150
0.185
0.105
0.362
0.058
0.158

Ours
0.457
0.369
0.586
0.285
0.189
0.220
0.102
0.325
0.362
0.180
0.096
0.153
0.130
0.233
0.157
0.147
0.424
0.117
0.252

Ours
0.298
0.223
0.294
0.130
0.165
0.138
0.082
0.201
0.180
0.125
0.112
0.216
0.104
0.154
0.195
0.130
0.362
0.065
0.176

Table 1. The AP comparison among average pooling, max
pooling and our optimal pooling method for low-level fea-
tures on TRECVID MED11 dataset.

From Table 1,

it can be obviously observed that our
method is effective on most events for both HOG and SIFT
features. For the HOG descriptor, our model leads to apparent
AP improvements on 14 out of 18 events, and our learning-
based method outperforms the max and average pooling
strategies by 0.026 and 0.045 in mAP, respectively. As to
the SIFT descriptor, the APs of overall 13 out of 18 events
are improved by our method and our method outperforms the
max and average pooling strategies by 0.008 and 0.018 in
mAP, respectively. It is worth noting that it is very hard to
improve mAP, even by 0.01 since the TRECVID MED11 is a
very challenging dataset.

3.3. Results on high-level features

We test two kinds of high-level features: CNN-based fea-
ture and Object Bank-based feature. When it comes to
the CNN-based feature, we directly employ the vgg-m-128
network[23], pre-trained on ILSVRC2012 dataset, to extract
feature on each single frame. In detail, we use the 128 di-
mensional fully connected layer feature as the ï¬nal feature
descriptor, denoted as â€œCNN 128dâ€. The Object Bank-based
descriptor is a combination of several independent â€œobject
conceptâ€ ï¬lter responses, where We pre-train 1,000 Object
ï¬lters on the ImageNet dataset [24]. For each video frame,
we employ the maximum response value for each ï¬lter as the

Event ID

Method
E001
E002
E003
E004
E005
E006
E007
E008
E009
E010
E011
E012
E013
E014
E015
P001
P002
P003
mAP

Max-OB
Max
0.445
0.338
0.184
0.129
0.151
0.368
0.075
0.121
0.320
0.127
0.243
0.211
0.110
0.246
0.191
0.172
0.198
0.133
0.209

Average
0.443
0.321
0.191
0.128
0.153
0.370
0.077
0.120
0.318
0.124
0.186
0.178
0.123
0.175
0.210
0.201
0.211
0.118
0.203

Ours
0.436
0.403
0.216
0.168
0.131
0.384
0.132
0.244
0.362
0.119
0.268
0.183
0.125
0.169
0.219
0.203
0.224
0.144
0.229

CNN 128d

Average
0.645
0.394
0.746
0.820
0.502
0.387
0.333
0.423
0.632
0.214
0.250
0.371
0.309
0.384
0.410
0.426
0.851
0.224
0.484

Max
0.653
0.388
0.745
0.818
0.590
0.389
0.323
0.446
0.627
0.269
0.249
0.425
0.327
0.381
0.410
0.453
0.956
0.219
0.481

Ours
0.654
0.394
0.747
0.813
0.581
0.389
0.337
0.461
0.636
0.303
0.252
0.425
0.326
0.384
0.422
0.447
0.949
0.227
0.486

Table 2. Comparisons among different methods for high-
level features on TRECVID MED11 dataset.

image-level ï¬lter response. Thus, each frame is represented
with a 1,000 dimensional descriptor, denoted as â€œMax-OBâ€.
The experiment results are listed shown in Table 2.

Basically, consistent with the low-level feature descrip-
tors, our learning-based pooling method is also effective for
both two high-level features on most events. For some spe-
ciï¬c events, the improvements are large using our method.
For example, on the event of E008 for object bank-based
feature, our method improves the AP by more than 0.12 com-
pared with average and max pooling methods. Averagely,
our method has an improvement of around 0.02 in mAP
compared to baseline methods for object bank-based feature,
while around 0.002 in mAP for CNN-based feature.

From Table 1 and 2, we can see that it is hard to deter-
mine which one of the baseline methods is better. Their per-
formances rely heavily on the feature descriptors and event
types.
In contrast, our method performs the best in most
cases(and in average).

4. CONCLUSION

In this paper, we propose a learning-based frame pooling
model to address the complex event detection task. Com-
pared with commonly used average pooling and max pooling
approaches, our method can automatically derive the pool-
ing weight among frames for each event category. Experi-
mental results conducted on TRECVID MED 2011 dataset
reveal that our approach is more effective and robust for both
low-level and high-level image descriptors compared with
traditional pooling methods.

5. REFERENCES

[1] Kevin Tang, Li Fei-Fei, and Daphne Koller, â€œLearning latent
temporal structure for complex event detection,â€ in Computer

ACM International Workshop on Multimedia Information Re-
trieval, New York, NY, USA, 2006, pp. 321â€“330, ACM Press.
[15] Zhen-Zhong Lan, Lu Jiang, Shoou-I Yu, Chenqiang Gao,
Shourabh Rawat, Yang Cai, Shicheng Xu, Haoquan Shen, Xu-
anchong Li, Yipei Wang, et al., â€œInformedia e-lamp@ trecvid
2013: Multimedia event detection and recounting (med and
mer),â€ 2013.

[16] Navneet Dalal and Bill Triggs, â€œHistograms of oriented gra-
dients for human detection,â€ in Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society Con-
ference on. IEEE, 2005, vol. 1, pp. 886â€“893.

[17] David G Lowe, â€œObject recognition from local scale-invariant
features,â€ in Computer vision, 1999. The proceedings of the
seventh IEEE international conference on. Ieee, 1999, vol. 2,
pp. 1150â€“1157.

[18] Li-Jia Li, Hao Su, Li Fei-Fei, and Eric P Xing, â€œObject bank:
A high-level image representation for scene classiï¬cation &
semantic feature sparsiï¬cation,â€ in Advances in neural infor-
mation processing systems, 2010, pp. 1378â€“1386.

[19] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell, â€œDecaf: A deep
convolutional activation feature for generic visual recognition,â€
arXiv preprint arXiv:1310.1531, 2013.

[20] Y-Lan Boureau, Jean Ponce, and Yann LeCun, â€œA theoretical
analysis of feature pooling in visual recognition,â€ in Proceed-
ings of the 27th International Conference on Machine Learn-
ing (ICML-10), 2010, pp. 111â€“118.

[21] Chih-Chung Chang and Chih-Jen Lin, â€œLibsvm: A library for
support vector machines,â€ ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 2, no. 3, pp. 27, 2011.

[22] Andrea Vedaldi and Brian Fulkerson, â€œVlfeat: An open and
portable library of computer vision algorithms,â€ in Proceed-
ings of the international conference on Multimedia. ACM,
2010, pp. 1469â€“1472.

[23] K. Chatï¬eld, K. Simonyan, A. Vedaldi, and A. Zisserman, â€œRe-
turn of the devil in the details: Delving deep into convolutional
nets,â€ in British Machine Vision Conference, 2014.

[24] Jia Deng, Alexander C Berg, Sanjeev Satheesh, Hao Su, Aditya
Khosla, and Li Fei-Fei, â€œImagenet large scale visual recogni-
tion challenge (ilsvrc) 2012,â€ 2012.

Vision and Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on. IEEE, 2012, pp. 1250â€“1257.

[2] Zhen-Zhong Lan, Lu Jiang, Shoou-I Yu, Shourabh Rawat,
Yang Cai, Chenqiang Gao, Shicheng Xu, Haoquan Shen, Xu-
anchong Li, Yipei Wang, et al., â€œCmu-informedia at trecvid
2013 multimedia event detection,â€ in TRECVID 2013 Work-
shop, 2013, vol. 1, p. 5.

[3] Chenqiang Gao, Deyu Meng, Wei Tong, Yi Yang, Yang Cai,
Haoquan Shen, Gaowen Liu, Shicheng Xu, and Alexander G
Hauptmann, â€œInteractive surveillance event detection through
mid-level discriminative representation,â€ in Proceedings of In-
ternational Conference on Multimedia Retrieval. ACM, 2014,
p. 305.

[4] Zhongwen Xu, Yi Yang, and Alexander G Hauptmann, â€œA
discriminative cnn video representation for event detection,â€
arXiv preprint arXiv:1411.4006, 2014.

[5] Luyu Yang, Chenqiang Gao, Deyu Meng, and Lu Jiang,
â€œA novel group-sparsity-optimization-based feature selection
in Computer
model for complex interaction recognition,â€
Visionâ€“ACCV 2014, pp. 508â€“521. Springer, 2015.

[6] Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang,
â€œDynamic concept compo-
arXiv preprint

and Alexander G Hauptmann,
sition for zero-example event detection,â€
arXiv:1601.03679, 2016.

[7] Yan Yan, Haoquan Shen, Gaowen Liu, Zhigang Ma, Chen-
qiang Gao, and Nicu Sebe, â€œGlocal tells you more: Coupling
glocal structural for feature selection with sparsity for image
and video classiï¬cation,â€ Computer Vision and Image Under-
standing, vol. 124, pp. 99â€“109, 2014.

[8] Yan Yan, Yi Yang, Deyu Meng, Gaowen Liu, Wei Tong,
Alexander G Hauptmann, and Nicu Sebe,
â€œEvent oriented
dictionary learning for complex event detection,â€ Image Pro-
cessing, IEEE Transactions on, vol. 24, no. 6, pp. 1867â€“1878,
2015.

[9] Zhigang Ma, Yi Yang, Nicu Sebe, and Alexander G Haupt-
mann, â€œKnowledge adaptation with partiallyshared features
for event detectionusing few exemplars,â€ Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 36, no. 9, pp.
1789â€“1802, 2014.

[10] Alexander Klaser, Marcin MarszaÅ‚ek, and Cordelia Schmid, â€œA
spatio-temporal descriptor based on 3d-gradients,â€ in BMVC
2008-19th British Machine Vision Conference. British Ma-
chine Vision Association, 2008, pp. 275â€“1.

[11] Ming-yu Chen and Alexander Hauptmann, â€œMosift: Recog-

nizing human actions in surveillance videos,â€ 2009.
[12] Paul Scovanner, Saad Ali, and Mubarak Shah,

â€œA 3-
dimensional sift descriptor and its application to action recog-
nition,â€ in Proceedings of the 15th international conference on
Multimedia. ACM, 2007, pp. 357â€“360.

[13] Heng Wang, Alexander KlÂ¨aser, Cordelia Schmid, and Cheng-
Lin Liu, â€œDense trajectories and motion boundary descriptors
for action recognition,â€ International journal of computer vi-
sion, vol. 103, no. 1, pp. 60â€“79, 2013.

[14] Alan F. Smeaton, Paul Over, and Wessel Kraaij, â€œEvaluation
campaigns and trecvid,â€ in MIR â€™06: Proceedings of the 8th

