Optimized Sensor Collaboration for Estimation of

Temporally Correlated Parameters

Sijia Liu Student Member, IEEE, Swarnendu Kar, Member, IEEE, Makan Fardad, Member, IEEE,

and Pramod K. Varshney Fellow, IEEE

1

6
1
0
2

 
r
a

 

M
0
1

 
 
]
T
I
.
s
c
[
 
 

1
v
8
4
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstractâ€”In this paper, we aim to design the optimal sen-
sor collaboration strategy for the estimation of time-varying
parameters, where collaboration refers to the act of sharing
measurements with neighboring sensors prior to transmission to
a fusion center. We begin by addressing the sensor collaboration
problem for the estimation of uncorrelated parameters. We
show that the resulting collaboration problem can be trans-
formed into a special nonconvex optimization problem, where
a difference of convex functions carries all the nonconvexity.
This speciï¬c problem structure enables the use of a convex-
concave procedure to obtain a near-optimal solution. When the
parameters of interest are temporally correlated, a penalized
version of convex-concave procedure becomes well suited for
designing the optimal collaboration scheme. In order to improve
computational efï¬ciency, we further propose a fast algorithm
that scales gracefully with problem size via the alternating
direction method of multipliers. Numerical results are provided to
demonstrate the effectiveness of our approach and the impact of
parameter correlation and temporal dynamics of sensor networks
on estimation performance.

Index Termsâ€”Distributed estimation, sensor collaboration,
temporal correlation, convex-concave procedure, semideï¬nite
programming, alternating direction method of multipliers, wire-
less sensor networks.

I. INTRODUCTION

Wireless sensor networks (WSNs) consist of a large number
of spatially distributed sensors that often cooperate to perform
parameter estimation; example applications include environ-
ment monitoring, source localization and target tracking [1]â€“
[3]. Under limited resources such as limited communication
bandwidth and sensor battery power, it is important to de-
sign an energy-efï¬cient architecture for distributed estimation.
In this paper, we employ a WSN to estimate time-varying
parameters in the presence of inter-sensor communication
that is referred to as sensor collaboration. Here sensors are
allowed to update their measurements by taking a linear
combination of the measurements of those they interact with
prior to transmission to a fusion center (FC). The presence
of sensor collaboration smooths out the observation noise,

Copyright (c) 2015 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to pubs-permissions@ieee.org.
S. Liu, M. Fardad and P. K. Varshney are with the Department of Electrical
Engineering and Computer Science, Syracuse University, Syracuse, NY, 13244
USA e-mail: {sliu17, makan, varshney}@syr.edu.

S. Kar is with New Devices Group, Intel Corporation, Hillsboro, Oregon,

97124 USA email: swarnendu.kar@intel.com.

The work of S. Liu and P. K. Varshney was supported by the U.S. Air Force
Ofï¬ce of Scientiï¬c Research under grants FA9550-10-1-0458. The work of
M. Fardad was supported by the National Science Foundation under awards
EAGER ECCS-1545270 and CNS-1329885.

thereby improving the quality of the signal and the eventual
estimation performance.

Early research efforts [4]â€“[9] focused on the problem of
distributed estimation in the absence of sensor collaboration,
where an amplify-and-forward transmission strategy is com-
monly used. The key problem in this setting is to design
optimal power amplifying factors under certain performance
criteria, such as estimation distortion and energy cost. In [4],
the optimal power allocation scheme was proposed for dis-
tributed estimation over an orthogonal multiple access channel
(MAC). In [5], the problem of power allocation was studied
when MAC is coherent, where sensors coherently form a
beam into a common channel received at the FC. In [6] and
[7], quantized measurements were transmitted to the FC for
distributed estimation. In [8] and [9], optimal power allocation
was studied in sensor networks with energy harvesting nodes.
Recently, the problem of distributed estimation with sensor
collaboration has attracted attention [10]â€“[18]. In [10], the op-
timal power allocation strategy was found for a fully connected
network, where all the sensors are allowed to collaborate,
namely, share their measurements with the other sensors.
It was shown that sensor collaboration results in signiï¬-
cant improvement of estimation performance compared with
the conventional amplify-and-forward transmission scheme.
In [11] and [12], optimal power allocation schemes were
found for star, branch and linear network topologies. In [13],
the sensor collaboration problem was studied for parameter
estimation via the best linear unbiased estimator. In [14]â€“
[16], the problem of sensor collaboration was studied given
an arbitrary collaboration topology. It was observed that even
a partially connected network can yield performance close to
that of a fully connected network. In [17] and [18], nonzero
collaboration costs were taken into account, and a sparsity
inducing optimization framework was proposed to jointly
design both sensor selection and sensor collaboration schemes.
In the aforementioned literature [10]â€“[18], sensor collabora-
tion was studied in static networks, where sensors take a single
snapshot of the static parameter, and then initiate sensor col-
laboration protocols designed in the setting of single-snapshot
estimation. In contrast, here we study the problem of sensor
collaboration for the estimation of time-varying parameters
in dynamic networks that involve, for example, time-varying
observation and channel gains. Solving such a problem is
also motivated by real-life applications, in which the physical
phenomenon to be monitored such as daily temperature and
precipitation [19]â€“[21] is temporally correlated. Due to the
presence of temporal dynamics and parameter correlation, the

design of optimal sensor collaboration schemes at multiple
time steps are coupled with each other, and thus poses
many challenges in problem formulation and optimization
compared to the previous work [10]â€“[18]. For example, when
parameters of interest are temporally correlated, expressing the
estimation distortion in a succinct closed form (with respect
to collaboration variables) becomes intractable. It should be
pointed out that even for uncorrelated parameters, ï¬nding the
optimal collaboration scheme for each time step is nontrivial
since energy constraints are temporally inseparable. In this
paper, we seek the optimal sensor collaboration scheme by
minimizing the estimation distortion subject
to individual
energy constraints of sensors in the presence of (a) temporal
dynamics in system, (b) temporal correlation of parameter, and
(c) energy constraints in time.

In our work, design of the optimal collaboration scheme
is studied under two scenarios: a) parameters are temporally
uncorrelated or the prior knowledge about temporal correlation
is not available, and b) parameters are temporally correlated.
When parameters are uncorrelated, we derive the closed form
of the estimation distortion with respect to sensor collaboration
variables, which is in the form of a sum of quadratic ratios.
We show that the resulting sensor collaboration problem is
equivalent to a nonconvex quadratically constrained problem,
in which the difference of convex functions carries all the
nonconvexity. This speciï¬c problem structure enables the use
of convex-concave procedure (CCP) [22] to solve the sensor
collaboration problem in a numerically efï¬cient manner.

When parameters of interest are temporally correlated,
expressing the estimation error as an explicit function of
the collaboration variables becomes more difï¬cult. In this
case, we show that the sensor collaboration problem can be
converted into a semideï¬nite program together with a rank-
one (nonconvex) constraint. After convexiï¬cation, the method
of penalty CCP [23] becomes well suited for seeking the
optimal sensor collaboration scheme. However, the proposed
algorithm is computationally intensive for large-scale prob-
lems. To improve computational efï¬ciency, we develop a fast
algorithm that scales gracefully with problem size by using
the alternating direction method of multipliers (ADMM) [24].

We summarize our contributions as follows.
â€¢ We propose a tractable optimization framework for the
design of the optimal collaboration scheme that accounts
for parameter correlation and temporal dynamics of sen-
sor networks.

â€¢ We show that the problem of sensor collaboration for the
estimation of temporally uncorrelated parameters can be
solved as a special nonconvex problem, where the only
source of nonconvexity can be isolated to a constraint
that contains the difference of convex functions.

â€¢ We provide valuable insights into the problem structure
of sensor collaboration with correlated parameters, and
propose an ADMM-based algorithm for improving the
computational efï¬ciency.

The rest of the paper is organized as follows. In Section II,
we introduce the collaborative estimation system, and present
the general formulation of the optimal sensor collaboration
problem. In Section III, we discuss two types of sensor collab-

2

oration problems for the estimation of temporally uncorrelated
and correlated parameters. In Section IV, we study the sensor
collaboration problem with uncorrelated parameters. In Sec-
tion V, we propose efï¬cient optimization methods to solve the
sensor collaboration problem with correlated parameters. In
Section VI, we demonstrate the effectiveness of our approach
through numerical examples. Finally, in Section VII we sum-
marize our work and discuss future research directions.

II. SYSTEM MODEL

In this section, we introduce the collaborative estimation
system and formulate the sensor collaboration problem consid-
ered in this work. The task here is to estimate a time-varying
parameter Î¸k over a time horizon of length K. In the esti-
mation system, sensors ï¬rst accquire their raw measurements
via a linear sensing model, and then update their observations
through spatial collaboration, where collaboration refers to the
act of sharing measurements with neighboring sensors. The
collaborative signals are then transmitted through a coherent
MAC to the FC, which ï¬nally determines a global estimate of
Î¸k for k âˆˆ [K]. The overall architecture of the collaborative
estimation system is shown in Fig. 1.

Fig. 1: Collaborative estimation architecture.

The vector of measurements from N sensors at time k is

given by the linear sensing model

xk = hkÎ¸k + k, k âˆˆ [K],

(1)

where for notational simplicity, let [K] denote the integer set
{1, 2, . . . , K}, xk = [xk,1, . . . , xk,N ]T is the vector of mea-
surements, hk = [hk,1, . . . , hk,N ]T is the vector of observation
gains, Î¸k is a random process with zero mean and variance
Î¸, k = [k,1, . . . , k,N ]T is the vector of Gaussian noises
Ïƒ2
with i.i.d variables k,n âˆ¼ N (0, Ïƒ2
 ) for k âˆˆ [K] and n âˆˆ [N ].
After linear sensing, each sensor may pass its observation
to another sensor for collaboration prior to transmission to the
FC. With a relabelling of sensors, we assume that the ï¬rst
M sensors (out of a total of N sensor nodes) communicate
with the FC. Collaboration among sensors is represented by a
known adjacency matrix A âˆˆ RMÃ—N with zero-one entries,
namely, Amn âˆˆ {0, 1} for m âˆˆ [M ] and n âˆˆ [N ]. Here
Amn = 1 signiï¬es that the nth sensor shares its observation
with the mth sensor. Conversely, Amn = 0 indicates the
absence of a collaboration link between the nth and mth
sensors.

random signalLinear sensingâ„Žð‘˜,1ðœ–ð‘˜,1{ðœƒð‘˜}ð‘¥ð‘˜,1linear spatial collaborationð‘”ð‘˜,1ð‘§ð‘˜,ð‘€ð‘§ð‘˜,2ð‘§ð‘˜,1ð‘”ð‘˜,2ð‘”ð‘˜,ð‘€ð‘Šð‘˜LMMSEestimateðœð‘˜ð‘¦ð‘˜coherent-MACFC{ ðœƒð‘˜}â„Žð‘˜,2ðœ–ð‘˜,2ð‘¥ð‘˜,2ðœ–ð‘˜,ð‘ð‘¥ð‘˜,ð‘â„Žð‘˜,ð‘Based on the adjacency matrix, the sensor collaboration

process at time k is given by

zk = Wkxk, k âˆˆ [K]
Wk â—¦ (1M 1T

N âˆ’ A) = 0,

(2)

where zk = [zk,1, zk,2, . . . , zk,M ]T , zk,m is the signal after
collaboration at sensor m and time k, Wk âˆˆ RMÃ—N is the
collaboration matrix that contains collaboration weights (based
on the energy allocated) used to combine sensor measurements
time k, â—¦ denotes the elementwise product, 1N is the
at
N Ã— 1 vector of all ones, and 0 is the M Ã— N matrix of all
zeros. In what follows, while refering to vectors of all ones
and all zeros, their dimensions will be omitted for simplicity
but can be inferred from the context. In (2), we assume that
sharing of an observation is realized through an ideal (noise-
less and cost-free) communication link. The proposed ideal
collaboration model enables us to obtain explicit expressions
for transmission cost and estimation distortion.

After sensor collaboration, the message zk is transmitted
through a coherent MAC so that the received signal yk at the
FC is a coherent sum [5]

yk = gT

k zk + Ï‚k, k âˆˆ [K],

(3)

where gk = [gk,1,, gk,2, . . . , gk,M ]T is the vector of channel
gains, and Ï‚k is temporally white Gaussian noise with zero
Ï‚ .
mean and variance Ïƒ2

We next deï¬ne the transmission cost of the mth sensor at
time k, which refers to the energy consumption of transmitting
the collaborative message zk to the FC. That is,

Tm(Wk) = EÎ¸k,k [z2
mWk(Ïƒ2

= eT

k,m]
Î¸ hkhT

k + Ïƒ2

 IN )WT

(4)
for m âˆˆ [M ] and k âˆˆ [K], where em âˆˆ RM is a basis vector
with 1 at the mth coordinate and 0s elsewhere, and IN is
the N Ã— N identity matrix. In what follows, while refering
to basis vector and identity matrix, their dimensions will be
omitted for simplicity but can be inferred from the context.

k em,

From (1) â€“ (3), the vector of received signals at the FC can
be compactly expressed as a linear function of parameters Î¸ =
[Î¸1, Î¸2, . . . , Î¸K]T ,

y = DW DhÎ¸ + Î½, DW := blkdiag{gT

k Wk}K

k=1,

(5)

where y = [y1, y2, . . . , yK]T , Î½ = [Î½1, Î½2, . . . , Î½K]T , Î½k :=
k=1, and blkdiag{Xi}n
k Wkk + Ï‚k, Dh := blkdiag{hk}K
gT
i=1
denotes the block-diagonal matrix with diagonal blocks
X1, X2, . . . , Xn.

At the FC, we employ a linear minimum mean squared-
error estimator (LMMSE) [25] to estimate Î¸, where we assume
that the FC knows the observation gains, channel gains, and
the second-order statistics of the parameters of interest and
additive noises. The corresponding estimator and estimation
error covariance are given by [25, Theorem 10.3]

Î¸ + DT
Î¸ + DT

h DT
h DT

W Dâˆ’1
W Dâˆ’1

Î½ DW Dh)âˆ’1DT
Î½ DW Dh)âˆ’1,

W Dâˆ’1
Ï… y

h DT

(6)

(cid:26) Ë†Î¸W = (Î£âˆ’1

PW = (Î£âˆ’1

where Î£Î¸ represents the prior knowledge about the parameter
Î¸ I for temporally uncorrelated
correlation, particularly Î£Î¸ = Ïƒ2

3

W + Ïƒ2

 DW DT

parameters, and DÎ½ := Ïƒ2
Ï‚ I. It is clear from
(6) that both the LMMSE and the estimation error covariance
matrix are functions of collaboration matrices {Wk}, and their
dependence on {Wk} is through DW . This dependency does
not lend itself to easy optimization of scalar-valued functions
of PW for design of the optimal sensor collaboration scheme.
More insights into the LMMSE (6) will be provided in Sec. III.
We now state the main optimization problem considered in

this work for sensor collaboration

minimize

subject to

tr (PW )

K(cid:88)

Tm(Wk) â‰¤ Em,

k=1

Wk â—¦ (1M 1T

N âˆ’ A) = 0,

m âˆˆ [M ]
k âˆˆ [K],

(7)

where Wk is the optimization variable for k âˆˆ [K], tr(PW )
denotes the estimation distortion of using the LMMSE, which
has a special form (shown in Sec. III) if parameters are
uncorrelated or the correlation prior is not available, Tm(Wk)
is the transmission cost given by (4), Em is a prescribed
energy budget of the mth sensor, and A is the adjacency
matrix to characterize the network topology. We remark that
although sensor collaboration is performed with respect to a
time-invariant (ï¬xed) topology matrix A, energy allocation
in terms of the magnitude of nonzero entries in Wt is time
varying in the presence of temporal dynamics of the sensor
network (e.g., time-varying observation and channel gains).
As will be evident later, the proposed sensor collaboration
approach is also applicable to the problem with time-varying
topologies. The problem structure and the solution of (7) will
be elaborated on in the rest of the paper.

III. REFORMULATION AND SIMPLIFICATION USING

MATRIX VECTORIZATION

In this section, we simplify problem (7) by exploiting the
sparsity structure of the adjacency matrix and concatenating
the nonzero entries of a collaboration matrix into a collabora-
tion vector. We show that the resulting optimization problem
involves special types of nonconvexities.

Fig. 2: Example of vectorization of Wk.

In problem (7), the only unknowns are the nonzero entries
of collaboration matrices. Motivated by that, we concatenate
these nonzero entries (columnwise) into a collaboration vector

wk = [wk,1, wk,2, . . . , wk,L]T ,

(8)

123FCð‘¨100011011ð‘¾ð‘˜â†’ð’˜ð‘˜ð‘¤ð‘˜,1000ð‘¤ð‘˜,2ð‘¤ð‘˜,40ð‘¤ð‘˜,3ð‘¤ð‘˜,5ð‘™ð‘šð‘™ð‘›ð‘™111222332423533where wk,l denotes the lth entry of wk, and L is the number
of nonzero entries of the adjacency matrix A. We note that
given wk,l, there exists a row index ml and a column index
nl such that wk,l = [Wk]mlnl, where [X]mn (or Xmn)
denotes the (m, n)th entry of a matrix X. We demonstrate
the vectorization of Wk through an example in Fig. 2, where
we consider N = 3 sensor nodes, M = 3 communicating
nodes, and 2 collaboration links.

A. Collaboration problem for the estimation of uncorrelated
parameters

When the parameters of interest are uncorrelated, the esti-

mation error covariance matrix (6) simpliï¬es to

PW =(cid:0)Ïƒâˆ’2
(cid:32)
(cid:26)

Ïƒâˆ’2
Î¸ I + diag

=

= diag

Ïƒ2
Î¸ gT

(cid:26) gT

Î¸ I + DT

w(Ïƒ2

w + Ïƒ2

h DT
k WkhkhT
 gT
t WkWT
Ïƒ2
k WkWT
 gT
Î¸ Ïƒ2
Ïƒ2
k WkhkhT
k WT
k gk+Ïƒ2

 DwDT
k WT
k gk
k gt + Ïƒ2
Ï‚
k gk + Ïƒ2
 gT

(cid:33)âˆ’1
(cid:27)K
Ï‚ I)âˆ’1DwDh

k=1

Î¸ Ïƒ2
Ï‚

k WkWT

k gk+Ïƒ2
Ï‚

(cid:1)âˆ’1
(cid:27)K

,

k=1
(9)

where diag{ak}K
entries a1, a2, . . . , aK.

k=1 denotes a diagonal matrix with diagonal

It

is clear from (9) and (4) both the estimation error
covariance matrix and the transmission cost contain quadratic
matrix functions [26], which can be converted into quadratic
vector functions according to the relationship between Wk
and wk stated in Proposition 1.
Proposition 1: Let w âˆˆ RL be the vector of stacking the
nonzero entries of W âˆˆ RMÃ—N columnwise, the expression
bT W can be written as a function of w,

bT W = wT B,

(10)
where b âˆˆ RN is a coefï¬cient vector, B is an L Ã— N matrix
whose (l, n)th entry is given by

(cid:26) bml n = nl

Bln =

(11)
and the indices ml and nl satisfy that wl = Wmlnl for l âˆˆ [L].
(cid:4)
Proof: The proof follows from [14, Sec. III-A].
From (9) and Proposition 1, the objective function of prob-

otherwise,

0

lem (7) can be rewritten as

K(cid:88)

k=1

where we use the fact that gT
L Ã— N matrix deï¬ned as (11), Rk := GkGT
Gk(Ïƒ2
 I)GT
semideï¬nite matrices.

k Gk, Gk is an
k , and Sk :=
k . Clearly, both Rk and Sk are positive

k Wk = wT

Î¸ hkhT

k + Ïƒ2

Moreover, the transmission cost (4) can be rewritten as

Tm(wk) := wT

k Qk,mwk,

(13)

where Qk,m := Em(Ïƒ2
as (11) such that eT
positive semideï¬nite for k âˆˆ [K] and m âˆˆ [M ].

m, and Em is deï¬ned
k + Ïƒ2
k Em. We remark that Qk,m is

Î¸,khkhT
mWk = wT

 I)ET

Ï†(w) := tr(PW ) =

Ïƒ2
Î¸ Ïƒ2

 wT
wT

k Rkwk + Ïƒ2
k Skwk + Ïƒ2
Ï‚

Î¸ Ïƒ2
Ï‚

,

(12)

where Gk has been introduced in (12).

Combining (15) and (16), we can rewrite the estimation

error covariance as a function of the collaboration vector

4

From (12) and (13), the sensor collaboration problem for
the estimation of temporally uncorrelated parameters becomes

minimize Ï†(w)

K(cid:88)

subject to

k Qk,mwk â‰¤ Em, m âˆˆ [M ],
wT

(P1)

k=1
1 , wT

2 , . . . , wT

where w = [wT
K] is the optimization variable,
and Ï†(w) is the estimation distortion given by (12). Note
that (P1) cannot be decomposed in time since sensor energy
constraints are temporally inseparable.

Compared to problem (7), the topology constraint in terms
of the adjacency matrix is eliminated without loss of perfor-
mance in (P1) since the sparsity structure of the adjacency
matrix has been taken into account while constructing the
collaboration vector. (P1) is a nonconvex optimization problem
since its objective function is given by a sum of rational
functions [27]. In the case of single-snapshot estimation
(namely, K = 1), the objective function of (P1) simpliï¬es to a
single quadratic ratio. It has been shown in [14] and [18] that
such a nonconvex problem can be readily solved via convex
programming. In contrast, the presence of the sum of quadratic
ratios makes solving (P1) more challenging. We present the
solution to (P1) in Sec. IV.

B. Collaboration problem for the estimation of correlated
parameters

When parameters are temporally correlated, the covariance
matrix Î£Î¸ is no longer diagonal. As a result, expressing
the estimation error in a succinct form as in (12) becomes
intractable. We recall from (6) that the dependence of the es-
timation error covariance on collaboration matrices is through
DW . According to the matrix inversion lemma, we are able
to simplify (6) via the relationship
Ï‚ I)âˆ’1Dw
 I + Ïƒ4

W + Ïƒ2
 I âˆ’ (Ïƒ2

 DW DT

= Ïƒâˆ’2

Ï‚ DT

W (Ïƒ2

(14)

DT

 Ïƒâˆ’2
Substituting (14) into (6), we obtain
Ï‚ DT

PW =(cid:0)C âˆ’ Ïƒâˆ’2

 Ïƒâˆ’2

W DW )âˆ’1.
(cid:1)âˆ’1

W DW )âˆ’1Dh

h (I + Ïƒ2
 DT

(15)
h Dh. According to the deï¬nition

 DT
where C := Î£âˆ’1
Î¸ + Ïƒâˆ’2
of DW in (5), we obtain

W DW = blkdiag{WT
DT
= blkdiag{GT

k gkgT
k wkwT

k Wk}K
k Gk}K

k=1
k=1,

(16)

(cid:1)âˆ’1

(cid:16)

Pw :=

Câˆ’Ïƒâˆ’2

 diag(cid:8)hT

k

Â· hk

(cid:0)I+Ïƒ2
(cid:17)âˆ’1
(cid:9)K
 Ïƒâˆ’2

Ï‚ GT

k wkwT

k Gk

(17)
From (17), the sensor collaboration problem for the estima-

k=1

.

tion of temporally correlated parameters becomes

minimize

subject to

tr(Pw)

K(cid:88)

k=1

k Qk,mwk â‰¤ Em, m âˆˆ [M ],
wT

(P2)

where w is the optimization variable, and the matrix â„¦m is
deï¬ned in (P1). Note that (P2) is not a convex optimization
k in
problem due to the presence of the rank-one matrix wkwT
(17). However, such a nonconvexity can be effectively handled
via proper convexiï¬cation techniques. We will present the
solution to (P2) in Sec. V.

We ï¬nally remark that the proposed sensor collaboration
methodologies in this paper apply equally well to the case of
sensor collaboration with respect to time-varying topologies,
namely, At for t âˆˆ [T ]. The only difference from sensor
collaboration with a ï¬xed topology is that the collaboration
vector wt would be constructed by concatenating the nonzero
entries of Wt according to At rather than A at each time
step.

From (19) and (20), problem (18) becomes

5

minimize
subject to

Ï‚ â‰¤ 0,
Ï‚ â‰¤ rk,

1T u
k + 2rk â‰¤ (sk + uk)2, k âˆˆ [K]
s2
k + u2
k âˆˆ [K]
sk âˆ’ wT
k Skwk âˆ’ Ïƒ2
k âˆˆ [K]
Ïƒ2
 wT
k Rkwk + Ïƒ2
m âˆˆ [M ]
wT Qmw â‰¤ Em,
s > 0,

(21a)
(21b)
(21c)
(21d)
(21e)
(21f)
where the optimization variables are w, u, r and s, r =
[r1, r2, . . . , rK]T , s = [s1, s2, . . . , sK]T , and > denotes el-
ementwise inequality. Note that the quadratic functions of DC
type in (21b) and (21c) bring in the nonconvexity of problem
(21). In what follows, we will show that CCP is a suitable
convex restriction approach for solving this problem.

IV. SPECIAL CASE: OPTIMAL SENSOR COLLABORATION
FOR THE ESTIMATION OF UNCORRELATED PARAMETERS

B. Convex restriction

In this section, we show that (P1) can be transformed into a
special nonconvex optimization problem, where the difference
of convex (DC) functions carries all the nonconvexity. Spurred
by the problem structure, we employ a convex-concave pro-
cedure (CCP) to solve (P1).

A. Equivalent optimization problem

We express (P1) in its epigraph form [28]

minimize

subject to

1T u
 wT
k Rkwk + Ïƒ2
Ïƒ2
Ï‚
wT
k Skwk + Ïƒ2
Ï‚
wT Qmw â‰¤ Em,

â‰¤ uk, k âˆˆ [K]
m âˆˆ [M ],

(18a)

(18b)

(18c)

where u = [u1, u2, . . . , uK]T is the vector of newly introduced
optimization variables, and Qm := blkdiag{Qk,m}K
k=1. We
note that the inequality (18b) implicitly adds the additional
constraint uk â‰¥ 0 since both Rk and Sk are positive semidef-
inite for k âˆˆ [K].
We further introduce new variables rk and sk for k âˆˆ [K]

to rewrite (18b) asï£±ï£´ï£´ï£²ï£´ï£´ï£³

â‰¤ uk, sk > 0
rk
sk
wT
k Skwk + Ïƒ2
Ïƒ2
 wT

k Rkwk + Ïƒ2

Ï‚ â‰¥ sk

Ï‚ â‰¤ rk,

(19)

where the equivalence between (18b) and (19) holds since
the minimization of 1T u with the above inequalities forces
the variable sk and rk to achieve its upper and lower bound,
respectively.
In (19), the ratio rk/sk â‰¤ uk together with sk > 0 can be

reformulated as a quadratic inequality of DC type

s2
k + u2

k + 2rk âˆ’ (sk + uk)2 â‰¤ 0,

(20)

where both s2
functions.

k + u2

k + 2rk and (sk + uk)2 are convex quadratic

Problem (21) is convex except for the nonconvex quadratic

constraints (21b) and (21c), which have the DC form

f (v) âˆ’ g(v) â‰¤ 0,

(22)

where both f and g are convex functions. In (21b), we have
k + 2rk, and g(sk, uk) = (sk + uk)2.
f (sk, uk, rk) = s2
In (21c), f (sk) = sk, and g(wk) = wT

Ï‚ .
k Skwk + Ïƒ2

We can convexify (22) by linearizing g around a feasible

k + u2

point Ë†v,

(23)

f (v) âˆ’ Ë†g(v) â‰¤ 0,
where Ë†g(v) := g(Ë†v) + (v âˆ’ Ë†v)T âˆ‚g(Ë†v)
is the ï¬rst-order
derivative of g at the point Ë†v. In (23), Ë†g is an afï¬ne lower
bound on the convex function g, and therefore, the set of v
that satisfy (23) is a strict subset of the set of v that satisfy
(22). This implies that a solution of the optimization problem
with the linearized constraint (23) is locally optimal for the
problem with the original nonconvex constraint (22).

âˆ‚v , âˆ‚g(Ë†v)

âˆ‚v

We can obtain a â€˜restrictedâ€™ convex version of problem (21)
by linearizing (21b) and (21c) as (23). We then solve a se-
quence of convex programs with iteratively updated lineariza-
tion points. The use of linearization to convexify nonconvex
problems with DC type functions is known as CCP [22], [23],
[29]. At each iteration of CCP, we solve

minimize 1T u
subject to s2

k + 2rk âˆ’ Ë†g1(sk, uk) â‰¤ 0,

k + u2
sk âˆ’ Ë†g2(wk) â‰¤ 0,
Ïƒ2
 wT
k Rkwk + Ïƒ2
wT Qmw â‰¤ Em,
s > 0,

Ï‚ â‰¤ rk,

k âˆˆ [K]
k âˆˆ [K]
k âˆˆ [K]
m âˆˆ [M ]

(24)
where the optimization variables are w, u, r, and s, Ë†g1 and Ë†g2
are afï¬ne approximations of (sk + uk)2 and wT
Ï‚ ,
k Skwk + Ïƒ2
namely, Ë†g1(sk, uk) := 2(sk + uk)(Ë†sk + Ë†uk) âˆ’ (Ë†sk + Ë†uk)2, and
Ï‚ . We summarize CCP
Ë†g2(wk) := 2 Ë†wT
for solving problem (21) or (P1) in Algorithm 1.

k Skwk âˆ’ Ë†wT

k Sk Ë†wk + Ïƒ2

To initialize Algorithm 1, we can choose the random points
(drawn from a standard uniform distribution) that are then

Algorithm 1 CCP for solving (P1)
Require: initial points Ë†w, Ë†s and Ë†u, and ccp > 0
1: for iteration t = 1, 2, . . . do
2:
3:

solve problem (24) for the solution (wt, st, ut)
update the linearization point, Ë†w = wt, Ë†s = st, and
Ë†u = ut
until |1T ut âˆ’ 1T utâˆ’1| â‰¤ ccp with t â‰¥ 2.

4:
5: end for

scaled to satisfy the constraints (21b) â€“ (21e). Our extensive
numerical examples show that Algorithm 1 is fairly robust with
respect to the choice of the initial point; see Fig. 4-(a) for an
example.

The convergence of Algorithm 1 is guaranteed, since at each
iteration, we solve a restricted convex problem with a smaller
feasible set which contains the linearization point (i.e., the
solution after the previous iteration) [23]. In other words, for
a given linearization point, we always obtain a new feasible
point with a lower or equal objective value at each iteration.
The computation cost of Algorithm 1 is dominated by the
solution of the convex program with quadratic constraints at
Step 2. This has the computational complexity O(a3 + a2b) in
the use of interior-point algorithm [30, Chapter. 10], where a
and b denote the number of optimization variables and con-
straints, respectively. In problem (24), we have a = 3K + KL
and b = 4K + M. Therefore, the complexity of our algorithm
is roughly given by O(L3) per iteration. Here we focus on
the scenario in which the number of collaboration links L is
much larger than K or M.

V. GENERAL CASE: OPTIMAL SENSOR COLLABORATION

FOR THE ESTIMATION OF CORRELATED PARAMETERS
Different from (P1), the presence of temporal correlation
makes ï¬nding the solution of (P2) more challenging. However,
we show that (P2) can be cast as a semideï¬nite program
(SDP) with a (nonconvex) rank-one constraint. Spurred by the
problem structure, we employ a penalty CCP to solve (P2), and
propose a fast optimization algorithm by using the alternating
direction method of multipliers (ADMM).

A. Equivalent optimization problem

From (17), (P2) can be equivalently transformed to

minimize
tr(V)
w (cid:23) Vâˆ’1
subject to Pâˆ’1
wT Qmw â‰¤ Em, m âˆˆ [M ]
k âˆˆ [K],
Uk = wkwT
k ,

(25)

where V âˆˆ SK and Uk âˆˆ SL are newly introduced optimiza-
tion variables for k âˆˆ [K], Sn represents the set of n Ã— n
symmetric matrices, and the notation X (cid:23) Y (or X (cid:22) Y)
indicates that Xâˆ’ Y (or Y âˆ’ X) is positive semideï¬nite. The
ï¬rst inequality constraint of problem (25) is obtained from
Pw (cid:22) V, where Pw is given by (17), and Pâˆ’1
w represents the
Bayesian Fisher information matrix.

6

(cid:1)âˆ’1

(cid:0)I + Ïƒ2

We further introduce a new vector of optimization variables
p = [p1, . . . , pK]T such that the ï¬rst matrix inequality of
problem (25) is expressed as

 hT
k

Ï‚ GT

 Ïƒâˆ’2

k UkGk

hk, k âˆˆ [K],

C âˆ’ diag(p) (cid:23) Vâˆ’1,
(26)
pk â‰¥ Ïƒâˆ’2
(27)
where we use the expression of Pw given by (17), and the
fact that Uk = wkwT
k . Note that the minimization of tr(V)
with inequalities (26) and (27) would force the variable pk
to achieve its lower bound. In other words, problem (25)
is equivalent
inequality
constraint of (25) is replaced by the above two inequalities.

to the problem in which the ï¬rst

By employing the Schur complement, we can express (26)

and (27) as the linear matrix inequalities (LMIs)

(cid:21)

(cid:20)C âˆ’ diag(p)
(cid:20) pk

I

Ïƒâˆ’1
 hk

I + Ïƒ2

(cid:23) 0,

I
V
Ïƒâˆ’1
 hT
k
 Ïƒâˆ’2
Ï‚ GT
k UkGk

(cid:21)

(cid:23) 0, k âˆˆ [K].

(28)

(29)

Replacing the ï¬rst inequality of problem (25) with LMIs
(28) â€“ (29), we obtain an SDP together with a (nonconvex)
rank-one constraint Uk = wkwT
k . This nonconvex constraint
can be recast as two inequalities

Uk âˆ’ wkwT

k (cid:23) 0, Uk âˆ’ wkwT

(30)
According to the Shur complement, the ï¬rst matrix inequality
is equivalent to the LMI

k (cid:22) 0, k âˆˆ [K].

(cid:23) 0, k âˆˆ [K].

(31)

(cid:20)Uk wk

(cid:21)

wT
k

1

And the second inequality in (30) involves a function of DC
type, where Uk and wkwT
k are matrix convex functions [28].

From (28) â€“ (31), problem (25) or (P2) is equivalent to

tr(V)

minimize
subject to wT Qmw â‰¤ Em, m âˆˆ [M ]

(32a)
(32b)
(32c)
(32d)
(32e)
where the optimization variables are w, p, V and Uk for
k âˆˆ [K], and (32e) is a nonconvex constraint of DC type.

LMIs in (28) â€“ (29)
LMIs in (31)
Uk âˆ’ wkwT

k âˆˆ [K],

k (cid:22) 0,

B. Convexiï¬cation

Proceeding with the same logic as in Sec. IV to convexify

the constraint (22), we linearize (32e) around a point Ë†wk,

k + Ë†wk Ë†wT

k âˆ’ wk Ë†wT

k (cid:22) 0, k âˆˆ [K].

Uk âˆ’ Ë†wkwT
(33)
It
is straightforward to apply CCP to solve problem (32)
by replacing (32e) with (33). However, such an approach
fails in practice. This is not surprising, since the feasible set
determined by (32d) and (33) only contains the linearization
point. Speciï¬cally, from (32d) and (33), we obtain

(wk âˆ’ Ë†wk)(wk âˆ’ Ë†wk)T
= wkwT
(cid:22) Uk âˆ’ Ë†wkwT

k âˆ’ Ë†wkwT

k âˆ’ wk Ë†wT

k âˆ’ wk Ë†wT

k + Ë†wk Ë†wT
k

k + Ë†wk Ë†wT

k (cid:22) 0,

(34)

which indicates that wk = Ë†wk. Therefore, CCP gets trapped
in a linearization point.

Remark 1: Dropping the nonconvex constraint (32e) is
another method to convexify problem (32), known as semidef-
inite relaxation [31]. However, such an approach makes the
optimization variable Uk unbounded, since the minimization
of tr(V) forces Uk to be as large as possible such that the
variable pk in (27) is as small as possible.

In order to circumvent the drawback of the standard CCP,
we consider its penalized version, known as penalty CCP [23],
where we add new variables to allow for constraints (33) to be
violated and penalize the sum of the violations in the objective
function. As a result, the convexiï¬cation (33) is modiï¬ed by
(35)
where Zk âˆˆ SL is a newly introduced variable. The constraint
(35) implicitly adds the additional constraint Zk (cid:23) 0 due to
Uk (cid:23) wkwk from (32d).

k (cid:22) Zk, k âˆˆ [K],

Uk âˆ’ Ë†wkwT

k âˆ’ wk Ë†wT

k + Ë†wk Ë†wT

After replacing (32e) with (35), we obtain the SDP,

minimize

tr(V) + Ï„

tr(Zk)

subject to

(32b) â€“ (32d) and (35)

k=1

(36)

where the optimization variables are w, p, V, Uk and Zk
for k âˆˆ [K], and Ï„ > 0 is a penalty parameter. Compared
to the standard CCP, problem (36) is optimized over a larger
feasible set since we allow for constraints to be violated by
adding variables Zk for k âˆˆ [K]. We summarize the use of
penalty CCP to solve (P2) in Algorithm 2.

Algorithm 2 Penalty CCP for solving (P2)
Require: an initial point Ë†w, ccp > 0, Ï„ 0 > 0, Ï„max > 0 and

K(cid:88)

Âµ > 1.

1: for iteration t = 1, 2, . . . do
2:

solve problem (36) for its solution wt via SDP solver
or ADMM-based algorithm in Sec. V-C
update the linearization point, Ë†w = wt
update the penalty parameter Ï„ t = min{ÂµÏ„ tâˆ’1, Ï„max}
let Ïˆt be the objective value of (36)
until |Ïˆt âˆ’ Ïˆtâˆ’1| â‰¤ ccp with t â‰¥ 2.

3:
4:
5:
6:
7: end for

In Algorithm 2, the initial point Ë†w is randomly picked from
a standard uniform distribution. Note that Ë†w is not necessarily
feasible for (P2) since violations of constraints are allowed.
We also remark that when Ï„ = Ï„max, penalty CCP reduces to
CCP, and therefore, its convergence is guaranteed [23].

The computation cost of Algorithm 2 is dominated by the
solution of the SDP (36) at Step 2. This leads to the complexity
O(a2b2 + ab3) by using the interior-point alogrithm in off-the-
shelf solvers [30, Chapter. 11], where a and b are the number of
optimization variables and the size of the semideï¬nite matrix,
respectively. In (36), the number of optimization variables is
proportional to L2. Therefore, the complexity of Algorithm 2 is
roughly given by O(L6). Clearly, computing solutions to SDPs
becomes inefï¬cient for problems of medium or large size. In
what follows, we will develop an ADMM-based algorithm that
is more amenable to large-scale optimization.

7

C. Fast algorithm via ADMM

It has been shown in [24], [32]â€“[34] that ADMM is a
powerful tool for solving large-scale optimization problems.
The major advantage of ADMM is that it allows us to split
the original problem into subproblems, each of which can be
solved more efï¬ciently or even analytically. In what follows,
we will employ ADMM to solve problem (36).

It is shown in Appendix A that problem (36) can be refor-
mulated in a way that lends itself to the application of ADMM.
This is achieved by introducing slack variables and indicator
functions to express the inequality constraints of problem
(36) as linear equality constraints together with proper cone
constraints with respect to slack variables, including second-
order cone and positive semideï¬nite cone constraints.

ADMM is performed based on the augmented Lagrangian
[24] of the reformualted problem (36), and leads to two
problems, the ï¬rst of which can be treated as an unconstrained
quadratic program and the latter renders an analytical solution.
These two problems are solved iteratively and communicate to
each other through special quadratic terms in their objectives;
the quadratic term in each problem contains information about
the solution of the other problem and also about dual variables
(also known as Lagrange multipliers). In what follows, we
refer to these problems as the â€˜X -minimizationâ€™ and â€˜Z -
minimizationâ€™ problems. Here X denotes the set of primal
variables w, p, V, Uk and Zk for k âˆˆ [K], and Z denotes the
set of slack variables Î»m, Î›1 and {Î›i,k}i=2,3,4 for m âˆˆ [M ]
and k âˆˆ [K]. We also use Y to denote the set of dual variables
Ï€m, Î 1 and {Î i,k}i=2,3,4 for m âˆˆ [M ] and k âˆˆ [K].
The ADMM algorithm is precisely described by (55) â€“ (57)
in Appendix A.

We emphasize that

the crucial property of the ADMM
approach is that, as we demonstrate in the rest of this section,
the solution of each of the X - and Z -minimization problems
can be found efï¬ciently and exactly.

1) X -minimization step: The X -minimization problem

can be cast as

minimize Ï•(w, p, V,{Uk},{Zk}).

(37)

m âˆ’ cm âˆ’ (1/Ï)Ï€t
1, and Î¥i,k := Î›t

The objective function of problem (37) is given by (38), where
1 âˆ’
Î±m := Î»t
i,k for i âˆˆ {2, 3, 4}
(1/Ï)Î t
and k âˆˆ [K], and t denotes the ADMM iteration. For ease of
notation, we will omit the ADMM iteration index t in what
follows.

m for m âˆˆ [M ], Î¥1 := Î›t
i,k âˆ’ (1/Ï)Î t

We note that problem (37) is an unconstrained quadratic
program (UQP) with large amounts of variables. In order to
reduce the computational complexity and memory requirement
in optimization, we will employ a gradient descent method
[28] together with a backtracking line search [28, Chapter 9.2]
to solve this UQP. In Proposition 2, we show the gradient of
the objective function of problem (37).

Proposition 2: The gradient of the objective function of

Ï•(w, p, V,{Uk},{Zk}) := tr(V) + Ï„

(cid:13)(cid:13)2

2 +

(cid:13)(cid:13) Â¯Qmw âˆ’ Î±m
(cid:21)

âˆ’ Î¥2,k

Ïƒâˆ’1
 hT
k
 Ïƒâˆ’2
Ï‚ GT
k UkGk

M(cid:88)

k=1

Ï
2

tr(Zk) +

K(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) pk
(cid:13)(cid:13)Zk âˆ’ Uk + Ë†wkwT

Ïƒâˆ’1
 hk

I + Ïƒ2

m=1

k + wk Ë†wT

k âˆ’ Ë†wk Ë†wT

k âˆ’ Î¥4,k

I

Ï
2

I
V

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)C âˆ’ diag(p)
(cid:21)
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)Uk wk
K(cid:88)
(cid:13)(cid:13)2

wT
k

Ï
2

k=1

+

1

F

F

(cid:13)(cid:13)(cid:13)(cid:13)2

F

âˆ’ Î¥1

(cid:21)

âˆ’ Î¥3,k

8

(38)

(cid:13)(cid:13)(cid:13)(cid:13)2

F

K(cid:88)
K(cid:88)

k=1

k=1

+

+

Ï
2

Ï
2

problem (37) is given by
m( Â¯Qmw âˆ’ Î±m) + 2Ï(w âˆ’ Î³3)
Â¯QT
k=1 Ë†w

âˆ‡wÏ• = Ï(cid:80)M
âˆ‡pÏ• = Ï diag(cid:0)C âˆ’ diag(p) âˆ’ Î¥11

(cid:1) + Ï(p âˆ’ Î³2)

+2Ï blkdiag{ Ë†wkwT

k âˆ’ Hk}K

k + wk Ë†wT

m=1

1

âˆ‡VÏ• = I + Ï(V âˆ’ Î¥22
1 )
k UkGk âˆ’ Î¥22
âˆ‡Uk Ï• = ÏÏƒ2
3,k âˆ’ Zk âˆ’ Tk), k âˆˆ [K]
+Ï(2Uk âˆ’ Î¥11

Ï‚ Gk(I + Ïƒ2

 Ïƒâˆ’2

 Ïƒâˆ’2

Ï‚ GT

âˆ‡Zk Ï• = Ï„ I + Ï(Zk âˆ’ Uk + Tk), k âˆˆ [K],

2,k)GT
k

1

1 , . . . , Ë†wT

K]T , Î¥11

3,1, . . . , Î³T

k + Î¥4,k, Ë†w = [ Ë†wT

where Î³3 = [Î³T
3,K]T , Î³3,k is the (L + 1) column
of Î¥3,k after the last entry is removed, Hk := Uk âˆ’ Zk +
is a submatrix
Ë†wk Ë†wT
of Î¥1 that contains its ï¬rst K rows and columns, Î³2 =
[Î³2,1, . . . , Î³2,K]T , Î³2,k is the ï¬rst element of Î¥2,k, diag(Â·)
returns the diagonal entries of its matrix argument in vector
is a submatrix of Î¥1 after the ï¬rst K rows and
form, Î¥22
1
2,k is a submatrix of Î¥2,k after
columns are removed, Î¥22
the ï¬rst row and column are removed, Î¥11
3,k is a submatrix
of Î¥3,k after the last row and column are removed, and
Tk := Ë†wkwT
k + wk Ë†wT
(cid:4)
Proof: See Appendix B.
In Proposition 2, we note that the optimal value of V is
achieved by letting âˆ‡VÏ• = 0. This leads to

k âˆ’ Ë†wk Ë†wT

k âˆ’ Î¥4,k.

1 âˆ’ (1/Ï)I.

V = Î¥22

(39)
To solve problem (37) for other variables, we employ the
gradient descent method summarized in Algorithm 3. This
algorithm calls on the backtracking line search (Algorithm 4)
to properly determine the step size such that the convergence
to a stationary point of problem (37) is accelerated.

Algorithm 3 Gradient descent method for solving UQP (37)
Require: values of w, p, {Uk} and {Zk} at the previous

ADMM iteration, grad > 0, and V given by (39)

1: repeat
2:
3:

4:
5:

:= (cid:80)K

2 +(cid:80)K

compute the gradient of Ï† following Proposition 2
compute cgrad
+(cid:107)âˆ‡pÏ•(cid:107)2
call Algorithm 4 to determine a step size Îº
update variables

k=1 (cid:107)âˆ‡Uk Ï•(cid:107)2

k=1 (cid:107)âˆ‡Zk Ï•(cid:107)2

F + (cid:107)âˆ‡wÏ•(cid:107)2

F

2

w := w + Îºâˆ‡wÏ•, p := p + Îºâˆ‡pÏ•
Uk := Uk + Îºâˆ‡Uk Ï•, Zk := Zk + Îºâˆ‡Zk Ï•

6: until cgrad â‰¤ grad.

Algorithm 4 Backtracking line search for choosing Îº
1: Given Îº := 1, a1 âˆˆ (0, 0.5), a2 âˆˆ (0, 1), and cgrad
2: repeat
3:
4:
5: until Ë†Ï• < Ï•(w, p, V,{Uk},{Zk}) âˆ’ a1Îº cgrad.

Îº := a2Îº,
let Ë†Ï• be the value of Ï• at the points w + Îºâˆ‡wÏ•,
p + Îºâˆ‡pÏ•, V, Uk + Îºâˆ‡Uk Ï•, and Zk + Îºâˆ‡Zk Ï•

decomposed with respect to each of slack variables.

2) Z -minimization step: The Z -minimization problem is
â€¢ Subproblem with respect to Î»m:
(cid:107)Î»m âˆ’ Î²m(cid:107)2

minimize
subject to (cid:107)[Î»m]1:KL(cid:107)2 â‰¤ [Î»m]KL+1,

(40)

2

where Î²m := Â¯Qmwt+1 + cm + (1/Ï)Ï€t
m, and t is the ADMM
iteration index. For notational simplicity, the ADMM iteration
will be omitted in what follows. The solution of problem (40)
is achieved by projecting Î²m onto a second-order cone [32,
Sec. 6.3],

ï£±ï£²ï£³ 0

Î»m =

(cid:18)

for m âˆˆ [M ], where

(cid:107)[Î²m]1:KL(cid:107)2 â‰¤ âˆ’[Î²m]KL+1
Î²m (cid:107)[Î²m]1:KL(cid:107)2 â‰¤ [Î²m]KL+1
ËœÎ²m (cid:107)[Î²m]1:KL(cid:107)2 â‰¥ |[Î²m]KL+1|,

(cid:19)(cid:2)[Î²m]T

1
2

1+

[Î²m]KL+1
(cid:107)[Î²m]1:KL(cid:107)2

ËœÎ²m =
â€¢ Subproblem with respect to Î›1:

1:KL,(cid:107)[Î²m]1:KL(cid:107)2

(41)

(cid:3)T

.

(42)

minimize
subject to Î›1 (cid:23) 0,

(cid:107)Î›1 âˆ’ Î¦1(cid:107)2

F

(cid:21)

I
V

(cid:20)C âˆ’ diag(p)
2K(cid:88)

Î›1 =

i=1

where Î¦1 :=
of problem (42) is given by [32, Sec. 6.3]

I

+ (1/Ï)Î 1. The solution

(Ïƒi)+Ï‰iÏ‰T
i ,

(43)

where(cid:80)2K

i=1 ÏƒiÏ‰iÏ‰T
i

is the eigenvalue decomposition of Î¦1,
â€¢ Subproblem with respect to Î›i,k for i âˆˆ {2, 3, 4} and

and (Â·)+ is the positive part operator.
k âˆˆ [K]:

(cid:107)Î›i,k âˆ’ Î¦i,k(cid:107)2

minimize
subject to Î›i,k (cid:23) 0,

F

(44)

where

Î¦2,k :=

(cid:20) pk
(cid:20)Uk wk

Ïƒâˆ’1
 hk

(cid:21)

(cid:21)

+ 1

Ï Î 2,k

9

I + Ïƒ2

Ïƒâˆ’1
 hT
k
 Ïƒâˆ’2
Ï‚ GT
k UkGk
Ï Î 3,k

+ 1

1

wT
k

Î¦3,k :=
Î¦4,k := Zk âˆ’ Uk + Ë†wkwT
The solution of problem (44) is the same as (43) except that
Î¦1 is replaced with Î¦i,k for i âˆˆ {2, 3, 4} and k âˆˆ [K].

k âˆ’ Ë†wk Ë†wT

k + wk Ë†wT

Ï Î 4,k.

k + 1

k = I for k âˆˆ [K], Î»0
1 = 0, and Î›0
i,k = Î 0

3) Summary of the proposed ADMM algorithm: We initial-
ize the ADMM algorithm by setting w0 = 1, p0 = 1, V0 = I,
m = 0 for m âˆˆ [M ],
U0
m = Ï€0
k = Z0
i,k = 0 for i âˆˆ {2, 3, 4,} and
Î›0
1 = Î 0
k âˆˆ [K]. The ADMM algorithm proceeds as (55) â€“ (57) shown
in Appendix A. The convergence of ADMM is guaranteed in
solving convex problems, and it typically takes a few tens of
iterations to converge with satisfactory accuracy [24].

At each iteration of ADMM, the computational complex-
ity of the X -minimization step is approximated by O(L4),
where O(L) roughly counts for the number of iterations of
the gradient descent method, and O(L3) is the complexity
of matrix multiplication while computing the gradient. Here
we assume that L is much larger than K and N. In Z -
minimization step, the computational complexity is dominated
by the eigenvalue decomposition used in (43). This leads to
the complexity O(L3.5). As a result, the total computation cost
of the ADMM algorithm is given by O(L4). For additional
perspective, we compare the computational complexity of
the ADMM algorithm with the interior-point algorithm that
takes complexity O(L6). Clearly, the complexity of ADMM
decreases signiï¬cantly in terms of the number of collaboration
links by a factor L2.

VI. NUMERICAL RESULTS

1

eâˆ’Ïcorr

ï£®ï£¯ï£¯ï£¯ï£°

Î£Î¸ = Ïƒ2
Î¸

This section empirically shows the effectiveness of our
approach for sensor collaboration in time-varying sensor net-
works. We assume that Î¸k follows a Ornstein-Uhlenbeck pro-
Î¸ eâˆ’|k1âˆ’k2|/Ïcorr
cess [16] with correlation cov(Î¸k1, Î¸k2 ) = Ïƒ2
for k1 âˆˆ [K] and k2 âˆˆ [K], where Ïcorr is a parameter that
governs the correlation strength, namely, a larger (or smaller)
Ïcorr corresponds to a weaker (or stronger) correlation. The
covariance matrix of Î¸ is given by
eâˆ’Ïcorr

Â·Â·Â·
Â·Â·Â·
...
Â·Â·Â·
Î¸ = 1 and Ïcorr =
where unless speciï¬ed otherwise, we set Ïƒ2
0.5. The spatial placement and neighborhood structure of the
sensor network is modeled by a random geometric graph [14],
RGG(N, d), where N = 10 sensors are randomly deployed
over a unit square and bidirectional communication links are
possible only for pairwise distances at most d. Clearly, the
adjacency matrix A is determined by RGG(N, d), and the
number of collaboration links increases as d increases. In our
numerical examples unless speciï¬ed otherwise, we set d = 0.3
which leads to RGG(10, 0.3) shown in Fig. 3.

eâˆ’(Kâˆ’1)Ïcorr
eâˆ’(Kâˆ’2)Ïcorr

eâˆ’(Kâˆ’1)Ïcorr

eâˆ’(Kâˆ’2)Ïcorr

ï£¹ï£ºï£ºï£ºï£» .

...

...

...

1

1

Fig. 3: RGG(10, 0.3), collaboration is depicted for sensors 3, 6 and 9.

 = Ïƒ2

In the collaborative estimation system shown in Fig. 1,
Ï‚ = 1, and
we assume that M = N, K = 3, Ïƒ2
Em = Etotal/M for m âˆˆ [M ], where Etotal = 1 gives
the total energy budget of M sensors. For simplicity, the
obverstion gain hk and channel gain gk are randomly chosen
from the uniform distribution U(0.1, 1). Moreover, we select
Ï„ 0 = 0.1, Âµ = 1.5, Ï„max = 100 in penalty CCP (namely,
Algorithm 2), a1 = 0.02 and a2 = 0.5 in backtracking line
search (namely, Algorithm 4) and ccp = admm = grad =
10âˆ’3 for the stopping tolerance of the proposed algorithms.
Unless speciï¬ed otherwise, the ADMM algorithm is adopted
at Step 2 of penalty CCP, and we use CVX [35] for all
other computations. The estimation performance is measured
through the empirical mean squared error (MSE), which is
computed over 1000 numerical trials.

In Fig. 4, we present convergence trajectories of CCP
(namely, Algorithm 1) and penalty CCP (namely, Algorithm 2)
as functions of interation index for 10 different initial points.
For comparison, we plot the worst objective function value of
collaboration problem (7) when w = 0, namely, LMMSE is
determined only by the prior information, which leads to the
worst estimation error tr(Î£Î¸) = K = 3. As we can see, much
of the beneï¬t of using CCP or penalty CCP is gained during
the ï¬rst few iterations. And each algorithm converges to almost
the same objective function value for different initial points.
Compared to CCP, the convergence trajectory of penalty CCP
is not monotonically decreasing. Namely, penalty CCP is not
a descent algorithm. The non-monotonicity of penalty CCP
is caused by the penalization on the violation of constraints
in the objective function. The objective function value of
penalty CCP converges until the penalization ceases to change
signiï¬cantly (after 15 iterations in this example).

In Fig. 5, we present the trace of error covariance matrix
PW given by (6) as a function of the correlation parameter
Ïcorr, where the sensor collaboration scheme is obtained
from Algorithm 1 and Algorithm 2 to solve (P1) and (P2),
respectively. We observe that the estimation error resulting
from the solution of (P1) remains unchanged for different
values of Ïcorr since the formulation of (P1) is independent

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91Xâˆ’axisYâˆ’axis SensorCollaboration LinkBoundarySensor 3Sensor 6Sensor 910

Fig. 5: Estimation error versus correlation parameter Ïcorr.

(a)

(b)

Fig. 4: Convergence of Algorithm 1 and 2 for different initial points.

Fig. 6: MSE versus total energy budget.

of the prior knowledge about parameter correlation. The
estimation error resulting from the solution of (P2) increases
as Ïcorr increases, and it eventually converges to the error
resulting from the solution of (P1) at an extremely large Ïcorr,
where parameters become uncorrelated. This is not surprising,
since the prior information about parameter correlation was
taken into account in (P2), thereby signiï¬cantly improving the
estimation performance.

In Fig. 6, we present the MSE of collaborative estimation as
a function of the total energy budget Etotal for Ïcorr = 0.5. For
comparison, we plot the estimation performance when using
a time-invariant collaboration scheme to solve (P1) and (P2),
respectively. The assumption of time-invariant collaboration
implicitly adds the additional constraint w1 = . . . = wK,
which reduces the problem size. By ï¬xing the type of algo-
rithm, we observe that the MSE when using time-invariant sen-
sor collaboration is larger than that of the originally proposed
algorithm. This is because the latter accounts for temporal
dynamics of the network, where observation and channel gains

vary in time. Moreover, the solution of (P2) yields lower MSE
than that of (P1). This result is consistent with Fig. 5 for a ï¬xed
correlation parameter. Lastly, the estimation error is smaller as
more energy is used in sensor collaboration.

In Fig. 7, we present the MSE and the number of collab-
oration links as functions of the collaboration radius d for
Ïcorr = 0.5 and Etotal = 1. We note that the estimation
accuracy improves as d increases, since a larger value of d
corresponds to more collaboration links in the network. For
a ï¬xed value of d, the MSE when solving (P2) is lower than
that when solving (P1), since the latter ignores the information
about parameter correlation. Moreover, we observe that the
MSE tends to saturate beyond a collaboration radius d â‰ˆ 0.7.
This indicates that a large part of the performance improve-
ment is achieved only through partial collaboration.

In Fig. 8, we present the computation time of our algorithms
as functions of problem size speciï¬ed in terms of the number
of collaboration links L. For comparison, we plot the computa-
tion time of penalty CCP when using an interior-point solver in

024681012142.12.22.32.42.52.62.72.82.933.1Iteration of CCPObjective function valueCCP with 10initial pointsWorst objective functionvalue provided by w = 0024681012141618202211.522.53Iteration of CCPObjective function valuePenatly CCP with differentinitial pointsWorst objective function value provided by w = 010âˆ’210âˆ’11001011021.31.41.51.61.71.81.922.12.22.3Correlation parameter, ÏcorrTrace of error covariance, tr(PW) Algorithm 1, CCP for (P1)Algorithm 2, penalty CCP for (P2)10âˆ’110010110211.522.53Total energy budget, EtotalMSEAlgorithm 1, CCP for (P1)Algorithm 1, CCP for (P1)under timeâˆ’invariant collaborationAlgorithm 2, penalty CCP for (P2)Algorithm 2, penalty CCP for (P2)under timeâˆ’invariant collaboration11

optimization problem, where a difference of convex functions
carries all the nonconvexity. By exploiting problem structure,
we solve the problem by using a convex-concave procedure,
which renders a good locally optimal solution evidenced by
numerical results. In the case of correlated parameters, we
show that the sensor collaboration problem can be converted
into a semideï¬nite program together with a nonconvex rank-
one constraint. Spurred by problem structure, we employ a
semideï¬nite programming based penalty convex-concave pro-
cedure to solve the sensor collaboration problem. Moreover,
we propose an ADMM-based algorithm that is more scalable
to large-scale optimization. Numerical results are provided to
demonstrate the effectiveness of our approach and the impact
of parameter correlation and temporal dynamics of sensor
networks on the performance of distributed estimation with
sensor collaboration.

There are multiple directions for future research. We would
like to consider noise-corrupted or quantization-based imper-
fect communication links in sensor collaboration. It is also
of interest to seek theoretical guarantees on the performance
of CCP and penalty CCP for sensor collaboration. Another
direction of future work is to seek an approach for the joint
design of optimal power allocation schemes and collaboration
topologies in dynamic networks.

APPENDIX A

APPLICATION OF ADMM

We introduce slack variables Î»m âˆˆ RKL+1 for m âˆˆ [M ] to
rewrite (32b) as an equality constraint together with a second-
order cone constraint,

Â¯Qmw âˆ’ Î»m + cm = 0, (cid:107)[Î»m]1:KL(cid:107)2 â‰¤ [Î»m]KL+1,

(45)

1
2

1
2

1
2

âˆš

m, 0]T , Q

where Â¯Qm := [Q
m is the square root of Qm
given by the matrix decomposition Qm = (Q
m, cm =
Em]T , and [a]1:n denotes a subvector of a that consists
[0T ,
of its ï¬rst n entries.
We further introduce slack variables Î›1 âˆˆ S2K, Î›2,k âˆˆ
SN +1, Î›3,k âˆˆ SL+1 and Î›4,k âˆˆ SL for k âˆˆ [K] to rewrite
LMIs of problem (36) as a sequence of equality constraints
together with positive semideï¬nite cone constraints

m)T Q

1
2

(cid:21)

(cid:20)C âˆ’ diag(p)
(cid:20) pk
(cid:21)
(cid:20)Uk wk

Ïƒâˆ’1
 hk

I

âˆ’ Î›1 = 0
I
V
Ïƒâˆ’1
 hT
k
 Ïƒâˆ’2
I + Ïƒ2
Ï‚ GT
k UkGk
âˆ’ Î›3,k = 0

(cid:21)

âˆ’ Î›2,k = 0

(46)

(47)

(48)

(49)

1

wT
k
Zk âˆ’ Uk + Ë†wkwT

k + wk Ë†wT

k âˆ’ Ë†wk Ë†wT

k âˆ’ Î›4,k = 0,

where Î›1 (cid:23) 0, Î›2,k (cid:23) 0, Î›3,k (cid:23) 0, and Î›4,k (cid:23) 0 for
k âˆˆ [K].

Fig. 7: MSE and collaboration links versus collaboration radius d.

CVX [35]. As we can see, penalty CCP requires much higher
computation time than CCP, since the former requires solutions
of SDPs. When L is small, we observe that the ADMM
based penalty CCP has a higher computation time than when
using the interior-point solver. This is because the gradient
descent method in ADMM takes relatively more iterations
(compared to small L) to converge with satisfactory accuracy.
However, the ADMM based algorithm performs much faster
for a relatively large problem with L > 80.

Fig. 8: Computation time versus number of collaboration links.

VII. CONCLUSIONS

We study the problem of sensor collaboration for estimation
of time-varying parameters in sensor networks. Based on prior
knowledge about parameter correlation, the resulting sensor
collaboration problem is solved for estimation of temporally
uncorrelated and correlated parameters. In the case of tem-
porally uncorrelated parameters, we show that
the sensor
collaboration problem can be cast as a special nonconvex

0.10.20.30.40.50.60.70.80.911.1123MSECollaboration radius, d 0.10.20.30.40.50.60.70.80.911.1050100No. of collaboration linksAlgorithm 1, CCP for (P1)Algorithm 2, penalty CCP for (P2)No. of collaboration links102030405060708090100100101102103No. of collaboration links, LComputation time (seconds)Algorithm 1, CCP for (P1)Algorithm 2, penalty CCP via CVX for (P2)Algorithm 2, penalty CCP via ADMM for (P2)12

until both of the conditions (cid:107)X t+1 âˆ’ Z t(cid:107)F â‰¤ admm and
(cid:107)Z t+1 âˆ’ Z t(cid:107)F â‰¤ admm are satisï¬ed, where with an abuse
of notation, (cid:107)X (cid:107)F denotes the sum of Frobenius norms of
variables in X , and admm is a stopping tolerance.

Substituting (54) into (55) and completing the squares with
respect to primal variables, the X -minimization problem (55)
becomes the unconstrained quadratic program given by (37).
Substituting (54) into (56), the Z -minimization problem
(56) is decomposed into a sequence of subproblems with
respect to each of slack variables, given by (40), (42) and
(cid:4)
(44).

We begin by collecting terms in Ï• associated with w,

APPENDIX B

PROOF OF PROPOSITION 2

Ï•w :=

Ï
2

(cid:13)(cid:13)2

M(cid:88)
(cid:13)(cid:13) Â¯Qmw âˆ’ Î±m
K(cid:88)

(cid:107) Ë†wkwT

m=1

+

Ï
2

k=1

K(cid:88)

k=1

2 + Ï

(cid:107)wk âˆ’ Î³3,k(cid:107)2

2

k + wk Ë†wT

k âˆ’ Hk(cid:107)2
F ,

(58)

where Î³3,k is the (L + 1) column of Î¥3,k after the last entry
is removed, and Hk := Uk âˆ’ Zk + Ë†wk Ë†wT
k + Î¥4,k, which is
a symmetric matrix.

In (58), we assume an incremental change Î´w in w.
Replacing w with w + Î´w and Ï•w with Ï•w + Î´Ï•w and
collecting ï¬rst order variation terms on both sides of (58),
we obtain

Î´Ï•w =Ï

( Â¯Qmw âˆ’ Î±m)T Â¯QmÎ´w + 2Ï(w âˆ’ Î³3)T Î´w

+ 2Ï Ë†wT blkdiag{ Ë†wkwT

(59)
K]T . It
where Î³3 = [Î³T
is clear from (59) that the gradient of Ï• with respect to w is
given by

3,K]T , and Ë†w = [ Ë†wT

k âˆ’ Hk}Î´w,
1 , . . . , Ë†wT

k + wk Ë†wT

3,1, . . . , Î³T

M(cid:88)

m=1

M(cid:88)

From (45) â€“ (49), problem (36) becomes

minimize

tr(V) + Ï„

tr(Zk) +

M(cid:88)

I0(Î»m)

K(cid:88)
4(cid:88)

k=1

K(cid:88)

+I1(Î›1) +

m=1

Ii(Î›i,k)

(50)

subject to

equality constraints in (45) â€“ (49),

i=2

k=1

where the optimization variables are w, p, V, Uk, Zk, Î»m,
Î›1, and {Î›i,k}i=2,3,4 for m âˆˆ [M ] and k âˆˆ [K], and Ii is
the indicator function speciï¬ed by

(cid:26) 0,
(cid:26) 0,
(cid:26) 0,

I0(Î»m) =
I1(Î›1) =
Ii(Î›i,k) =

if (cid:107)[Î»m]1:KL(cid:107)2 â‰¤ [Î»m]KL+1
âˆž otherwise,
if Î›1 (cid:23) 0
âˆž otherwise,
if Î›i,k (cid:23) 0
âˆž otherwise,

i = 2, 3, 4.

(51)

(52)

(53)

It is clear from problem (50) that the introduced indicator
functions helps to isolate the second-order cone and positive
semideï¬nite cone constraints with respect to slack variables.
Problem (50) is now in a form suitable for the application
of ADMM. The corresponding augmented Lagrangian [24] in
ADMM is given by

K(cid:88)

M(cid:88)

I0(Î»m)

Ii(Î›i,k) +

k=1

m=1

tr(Zk) +

mfm(X , Z )
Ï€T

M(cid:88)
1 F1(X , Z )(cid:1)
2 + tr(cid:0)Î T
K(cid:88)
4(cid:88)
tr(cid:0)Î T
i,kFi,k(X , Z )(cid:1)

m=1

LÏ(X , Z , Y ) = tr(V) + Ï„

4(cid:88)

K(cid:88)

i=2

k=1

(cid:107)fm(X , Z )(cid:107)2

(cid:107)F1(X , Z )(cid:107)2

F +

+ I1(Î›1) +

M(cid:88)

m=1

4(cid:88)

K(cid:88)

i=2

k=1

+

+

+

Ï
2

Ï
2

Ï
2

i=2

k=1

(cid:107)Fi,k(X , Z )(cid:107)2
F ,

where X denotes the set of primal variables w, p, V, Uk and
Zk for k âˆˆ [K], Z denotes the set of primal slack variables
Î»m, Î›1 and {Î›i,k}i=2,3,4 for m âˆˆ [M ] and k âˆˆ [K], Y
is the set of dual variables Ï€m, Î 1 and {Î i,k}i=2,3,4 for
m âˆˆ [M ] and k âˆˆ [K], fm(Â·), F1(Â·), and Fi,k(Â·) for i âˆˆ
{2, 3, 4} represent linear functions at the left hand side of
equality constraints in (45) â€“ (49), Ï > 0 is a regularization
parameter, and (cid:107)Â·(cid:107)F denotes the Frobenius norm of a matrix.
We iteratively execute the following three steps for ADMM

iteration t = 0, 1, . . .

X t+1 = arg min

X

Z t+1 = arg min

ï£±ï£²ï£³ Ï€t+1

Î t+1
Î t+1

Z
m = Ï€t
1 = Î t
i,k = Î t

L(X , Z t, Y t)
L(X t+1, Z , Y t)

m + Ï fm(X t+1, Z t+1), âˆ€m
1 + Ï F1(X t+1, Z t+1)
i,k + Ï Fi,k(X t+1, Z t+1), âˆ€i, k,

(55)

(56)

(57)

(54)

âˆ‡wÏ• =Ï

m( Â¯Qmw âˆ’ Î±m) + 2Ï(w âˆ’ Î³3)
Â¯QT
k âˆ’ Hk} Ë†w.

k + wk Ë†wT

+ 2Ï blkdiag{ Ë†wkwT

m=1

(60)

Second, we collect the terms associated with p in Ï• to

construct the function

Ï
2

Ï•p :=

(cid:107)C âˆ’ diag(p) âˆ’ Î¥11
1 (cid:107)2

Ï
2
is a matrix that consists of the ï¬rst K rows and
where Î¥11
1
columns of Î¥1, and Î³2 is a vector whose kth entry is given
by the ï¬rst entry of Î¥2,k for k âˆˆ [K].

(cid:107)p âˆ’ Î³2(cid:107)2
2,

(61)

F +

In (61), replacing p with p+Î´p and Ï•p with Ï•p +Î´Ï•p and
collecting ï¬rst order variation terms on both sides, we obtain
Î´Ï•p = Ï[diag(C âˆ’ diag(p) âˆ’ Î¥11
(62)
where diag(Â·) returns in vector form the diagonal entries of
its matrix argument. Therefore, the gradient of Ï• with respect
to p is given by

1 ) + (p âˆ’ Î³2)]T Î´p,

âˆ‡pÏ• = Ï diag(C âˆ’ diag(p) âˆ’ Î¥11

1 ) + Ï(p âˆ’ Î³2).

(63)

Third, given the terms associated with V in Ï•, the gradient

of Ï• with respect to V is readily cast as
âˆ‡VÏ• = I + Ï(V âˆ’ Î¥22
1 ),

(64)

(cid:13)(cid:13)2

F

(cid:13)(cid:13)I + Ïƒ2

where Î¥22
1
columns are removed.

is a submatrix of Î¥1 after the ï¬rst K rows and

Further, we collect the terms in Ï• with respect to the variable

Uk, and consider the function

Ï•Uk :=

Ï
2
+

Ï
2

 Ïƒâˆ’2
Ï‚ GT
3,k(cid:107)2
(cid:107)Uk âˆ’ Î¥11

k UkGk âˆ’ Î¥22
F +

2,k

Ï
2

(cid:107)Uk âˆ’ Zk âˆ’ Tk(cid:107)2
F ,

(65)

where Î¥22
column are removed, Î¥11
row and column are removed, and Tk := Ë†wkwT
Ë†wk Ë†wT

2,k is a submatrix of Î¥2,k after the ï¬rst row and
3,k is a submatrix of Î¥3,k after the last
k âˆ’

k + wk Ë†wT

k âˆ’ Î¥4,k.

In (65), replacing Uk with Uk + Î´Uk and Ï•Uk with Ï•Uk +
Î´Ï•Uk and collecting ï¬rst order variation terms on both sides,
we obtain

(cid:18)
(cid:16)(cid:0)Uk âˆ’ Î¥11

ÏÏƒ2

Ïƒ2
Ï‚

tr

Î´Ï•Uk =

+ Ï tr

Gk(I +

k UkGk âˆ’ Î¥22
GT

2,k)T GT

k Î´Uk

Ïƒ2

Ïƒ2
Ï‚

3,k)T Î´Uk + (Uk âˆ’ Zk âˆ’ Tk

Î´Uk

(cid:1)T

(cid:19)
(cid:17)

.

Therefore, the gradient of Ï• with respect to Uk is given by

âˆ‡Uk Ï• =ÏÏƒ2

Ï‚ Gk(I + Ïƒ2

 Ïƒâˆ’2
+ Ï(Uk âˆ’ Î¥11

Ï‚ GT

 Ïƒâˆ’2

k UkGk âˆ’ Î¥22
3,k) + Ï(Uk âˆ’ Zk âˆ’ Tk).

2,k)GT
k
(66)

Finally, the gradient of Ï• with respect to Zk is given by

âˆ‡Zk Ï• = Ï„ I + Ï(Zk âˆ’ Uk + Tk),

(67)

where Tk is deï¬ned in (65). We now complete the proof by
(cid:4)
combining (60), (63), (64), (66) and (67).

REFERENCES

[1] L. Oliveira and J. Rodrigues, â€œWireless sensor networks: a survey on
environmental monitoring,â€ Journal of Communications, vol. 6, no. 2,
2011.

[2] Y. Zou and K. Chakrabarty, â€œSensor deployment and target localization
ACM Transactions on Embedded

in distributed sensor networks,â€
Computing Systems, vol. 3, no. 1, pp. 61â€“91, Feb. 2004.

[3] T. He, P. Vicaire, T. Yan, L. Luo, L. Gu, G. Zhou, S. Stoleru, Q. Cao,
J. A. Stankovic, and T. Abdelzaher, â€œAchieving real-time target tracking
using wireless sensor networks,â€ in Proceedings of IEEE Real Time
Technology and Applications Symposium, 2006, pp. 37â€“48.

[4] S. Cui, J.-J. Xiao, A. J. Goldsmith, Z.-Q. Luo, and H. V. Poor,
â€œEstimation diversity and energy efï¬ciency in distributed sensing,â€ IEEE
Transactions on Signal Processing, vol. 55, no. 9, pp. 4683â€“4695, 2007.
[5] J.-J. Xiao, S. Cui, Z.-Q. Luo, and A. J. Goldsmith, â€œLinear coherent
decentralized estimation,â€ IEEE Transactions on Signal Processing, vol.
56, no. 2, pp. 757â€“770, 2008.

[6] A. Ribeiro and G. B. Giannakis, â€œBandwidth-constrained distributed
estimation for wireless sensor networks-part i: Gaussian case,â€ IEEE
Transactions on Signal Processing, vol. 54, no. 3, pp. 1131â€“1143, 2006.
identical binary
IEEE Transactions on

[7] S. Kar, H. Chen, and P. K. Varshney,

quantizer design for distributed estimation,â€
Signal Processing, vol. 60, no. 7, pp. 3896â€“3901, 2012.

â€œOptimal

[8] C. Huang, Y. Zhou, T. Jiang, P. Zhang, and S. Cui, â€œPower allocation
for joint estimation with energy harvesting constraints,â€ in Proceedings
of IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), May 2013, pp. 4804â€“4808.

[9] M. Nourian, S. Dey, and A. Ahlen, â€œDistortion minimization in multi-
sensor estimation with energy harvesting,â€ IEEE Journal on Selected
Areas in Communications, vol. 33, no. 3, pp. 524â€“539, Mar. 2015.

13

[10] J. Fang and H. Li,

cluster-based sensor collaboration,â€
Communications, vol. 8, no. 7, pp. 3822â€“3832, 2009.

â€œPower constrained distributed estimation with
IEEE Transactions on Wireless

[11] G. Thatte and U. Mitra,

â€œPower allocation in linear and tree wsn
topologies,â€ in Proceedings of Asilomar Conference on Signals, Systems
and Computers, Oct 2006, pp. 1342â€“1346.

[12] G. Thatte and U. Mitra, â€œSensor selection and power allocation for
distributed estimation in sensor networks: Beyond the star topology,â€
IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2649â€“2661,
July 2008.

[13] M. Fanaei, M. C. Valenti, A. Jamalipour, and N. A. Schmid, â€œOptimal
power allocation for distributed blue estimation with linear spatial
collaboration,â€ in Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp.
5452â€“5456.

[14] S. Kar and P. K. Varshney, â€œLinear coherent estimation with spatial
collaboration,â€ IEEE Transactions on Information Theory, vol. 59, no.
6, pp. 3532â€“3553, June 2013.
[15] S. Kar and P. K. Varshney,

â€œOn linear coherent estimation with
spatial collaboration,â€ in Proceedings of the 2012 IEEE International
Symposium on Information Theory Proceedings (ISIT), 2012, pp. 1448â€“
1452.

[16] S. Kar and P. K. Varshney,

â€œControlled collaboration for linear
coherent estimation in wireless sensor networks,â€ in Proceedings of
the 50th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), 2012, pp. 334â€“341.

[17] S. Liu, S. Kar, M. Fardad, and P. K. Varshney, â€œOn optimal sensor
collaboration topologies for linear coherent estimation,â€ in Proceedings
of IEEE International Symposium on Information Theory (ISIT), 2014,
pp. 2624â€“2628.

[18] S. Liu, S. Kar, M. Fardad, and P. K. Varshney, â€œSparsity-aware sensor
IEEE Transactions on

collaboration for linear coherent estimation,â€
Signal Processing, vol. 63, no. 10, pp. 2582â€“2596, May 2015.

[19] M. C. Vuran, O. B. Akan, and I. F. Akyildiz, â€œSpatio-temporal correla-
tion: theory and applications for wireless sensor networks,â€ Computer
Networks, vol. 45, no. 3, pp. 245â€“259, June 2004.

[20] M. C. Vuran and O. B. Akan,

â€œSpatio-temporal characteristics of
point and ï¬eld sources in wireless sensor networks,â€ in Proc. IEEE
International Conference on Communications (ICC), June 2006, vol. 1,
pp. 234â€“239.

[21] Phaeton C. Kyriakidis, â€œA spatial time series framework for modeling
daily precipitation at regional scales,â€ Journal of Hydrology, vol. 297,
no. 4, pp. 236 â€“ 255, Apr. 2004.

[22] A. L. Yuille and A. Rangarajan,

â€œThe concave-convex procedure,â€

[23] T. Lipp and S. Boyd,

Neural Computation, vol. 15, no. 4, pp. 915â€“936, 2003.
â€œVariations and extensions of the convex-
concave procedure,â€ http://web.stanford.edu/âˆ¼boyd/papers/pdf/cvx ccv.
pdf, 2014.

[24] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, â€œDistributed
optimization and statistical learning via the alternating direction method
of multipliers,â€ Foundations and Trends in Machine Learning, vol. 3,
no. 1, pp. 1â€“122, 2011.

[25] S. M. Kay, Fundamentals of Statistical Signal Processing, Volume I:

Estimation Theory, Prentice Hall, Englewood Cliffs, NJ, 1993.

[26] A. Beck, â€œQuadratic matrix programming,â€ SIAM Journal on Optimiza-

tion, vol. 17, no. 4, pp. 1224â€“1238, 2007.

[27] F. Bugarin, D. Henrion, and J.-B. Lasserre, â€œMinimizing the sum of
many rational functions,â€ Mathematical Programming Computation, pp.
1â€“29, 2015.

[28] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge

University Press, Cambridge, 2004.

[29] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann, â€œKernel methods
for missing variables,â€ Proceedings of the 10th International Workshop
on Artiï¬cial Intelligence and Statistics (AISTATS), 2005.

[30] A. Nemirovski,

â€œInterior point polynomial time methods in convex
programming,â€ 2012 [Online], Available: http://www2.isye.gatech.edu/
âˆ¼nemirovs/Lect IPM.pdf .

[31] Z.-Q. Luo, W.-K. Ma, A. M.-C. So, Y. Ye, and S. Zhang, â€œSemideï¬nite
relaxation of quadratic optimization problems,â€ IEEE Signal Processing
Magazine, vol. 27, no. 3, pp. 20â€“34, May 2010.

[32] N. Parikh and S. Boyd, â€œProximal algorithms,â€ Foundations and Trends

in Optimization, vol. 1, no. 3, pp. 123â€“231, 2013.

[33] B. Oâ€™Donoghue, E. Chu, N. Parikh, and S. Boyd, â€œOperator splitting
for conic optimization via homogeneous self-dual embedding,â€ Arxiv
preprint http://arxiv.org/abs/1312.3039, 2013.

[34] Y. Shi, J. Zhang, B. Oâ€™Donoghue, and K. B. Letaief,

â€œLarge-scale
convex optimization for dense wireless cooperative networks,â€ IEEE
Transactions on Signal Processing, vol. 63, no. 18, pp. 4729â€“4743, Sept.
2015.

[35] Inc. CVX Research, â€œCVX: Matlab software for disciplined convex

programming, version 2.0,â€ http://cvxr.com/cvx, Aug 2012.

14

