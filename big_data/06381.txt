6
1
0
2

 
r
a

 

M
1
2

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
1
8
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Forward and Inverse Uncertainty Quantiï¬cation using

Multilevel Monte Carlo Algorithms for an Elliptic Nonlocal

Equation

A. Jasra, âˆ—

K.J.H. Law, â€ 

Y. Zhou â€¡

March 22, 2016

Abstract

This paper considers uncertainty quantiï¬cation for an elliptic nonlocal equation. In par-

ticular, it is assumed that the parameters which deï¬ne the kernel in the nonlocal operator

are uncertain and a priori distributed according to a probability measure. It is shown that

the induced probability measure on some quantities of interest arising from functionals of the

solution to the equation with random inputs is well-deï¬ned; as is the posterior distribution

on parameters given observations. As the elliptic nonlocal equation cannot be solved ap-

proximate posteriors are constructed. The multilevel Monte Carlo (MLMC) and multilevel

sequential Monte Carlo (MLSMC) sampling algorithms are used for a priori and a posteriori

estimation, respectively, of quantities of interest. These algorithms reduce the amount of

work to estimate posterior expectations, for a given level of error, relative to Monte Carlo

and i.i.d. sampling from the posterior at a given level of approximation of the solution of

the elliptic nonlocal equation.

Key words: Uncertainty quantiï¬cation, multilevel Monte Carlo, sequential Monte Carlo,

nonlocal equations, Bayesian inverse problem

AMS subject classiï¬cation: 82C80, 60K35.

âˆ—Department of Statistics & Applied Probability National University of Singapore Singapore, Singapore
â€ Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA, 37831
â€¡Department of Statistics & Applied Probability National University of Singapore Singapore, Singapore

1

1

Introduction

Anomalous diï¬€usion, where the associated underlying stochastic process is not Brownian motion,

has recently attracted considerable attention [2]. This case is interesting because there may be

long-range correlations, among other reasons. Anomalous superdiï¬€usion can be be related to

fractional Laplacian operators and/or so-called nonlocal operators [10], deï¬ned point-wise by
their operation on a function u : Rd â†’ R as

L(u)(x) :=

B(x, x(cid:48))[u(x(cid:48)) âˆ’ u(x)]dx(cid:48).

(1.1)

(cid:90)

Rd

The fractional Laplacian is actually a special case of this equation. The present work will focus on

these operators and in particular the associated stationary equation, analogous to the local elliptic

equation. It should be noted that these nonlocal operators and the associated equations can be

applied not only to problems of anomalous diï¬€usion, but to a wide range of phenomena, including

peridynamic models of continuum mechanics which allow crack nucleation and propagation [17,

10].

In this work it will be assumed that the kernel B appearing above is parametrized by a (pos-
sibly inï¬nite-dimensional) parameter Î» âˆˆ E; Î» is assumed random. Furthermore, some partial

noisy observations of the solution u will be available. In a probabilistic framework, a prior distri-
bution is placed on Î» âˆ¼ Âµ and the posterior distribution Î»|y âˆ¼ Î· results from conditioning on the

observations. The joint distribution can often trivially be derived and evaluated in closed form
for a given pair (Î», y), as P(Î», y) = P(y|Î»)Âµ(Î»). Hence, the posterior for a given observed value of

y can be evaluated, up to a normalizing constant. One aims to approximate quantities of interest

E Q(Î»)Î½(dÎ») for some Q : E â†’ R, where Î½ = Âµ for the forward problem and Î½ = Î· for
the inverse problem. The likelihood P(y|Î») is often concentrated in a small, possibly nonlinear,

q = (cid:82)
dard forward approximation algorithms independently to the numerator, (cid:82)
and denominator,(cid:82)

E Q(Î»)P(y|Î»)Âµ(dÎ»),
P(y|Î»)Âµ(dÎ»). It is noted that, typically, (1.1) will have to be approximated

subspace of E. This posterior concentration generically precludes the naive application of stan-

E

numerically, and this will lead to an approximate posterior density.

Monte Carlo (MC) and Sequential Monte Carlo (SMC) methods are amongst the most widely

this N times and approximates (1/N )(cid:80)N

used computational techniques in statistics, engineering, physics, ï¬nance and many other disci-
plines. In particular, if i.i.d. samples Î»i âˆ¼ Âµ may be obtained, the MC sampler simply iterates
E Q(Î»)Âµ(dÎ»). SMC samplers [8] are de-
signed to approximate a sequence {Î·l}lâ‰¥0 of probability distributions on a common space, whose
densities are only known up-to a normalising constant. The method uses N â‰¥ 1 samples (or

i=1 Q(Î»i) â‰ˆ (cid:82)

2

particles) that are generated in parallel, and are propagated with importance sampling (often)

via Markov chain Monte Carlo (MCMC) and resampling methods. Several convergence results,

as N grows, have been proved (see e.g. [7]).

For problems which must ï¬rst be approximated at ï¬nite resolution, as is the case in this article,

and must subsequently be sampled from using MC-based methods, a multilevel (ML) framework

may be used. This can potentially reduce the cost to obtain a given level of mean square error

(MSE) [14, 11, 12], relative to performing i.i.d. sampling from the approximate posterior at a

given (high) resolution. A telescopic sum of successively reï¬ned approximation increments are

estimated instead of a single highly resolved approximation. The convergence of the reï¬ned

approximation increments allows one to balance the cost between levels, and optimally obtain a
cost O(Îµâˆ’2) for MSE O(Îµ2). This is shown in the context of the models in this article. Speciï¬cally,
it is shown that the cost of MLMC, to provide a MSE of O(Îµ2), is less than i.i.d. sampling from

the most accurate prior approximation.

SMC within the ML framework has been primarily developed in [3, 9, 16]. These methodolo-

gies, some of which consider a similar context to this article, have been introduced where the ML

approach is thought to be beneï¬cial, but i.i.d. sampling is not possible. Indeed, one often has to

resort to advanced MCMC or SMC methods to implement the ML identity; see for instance [15]

for MCMC. SMC for nonlocal problems however, is a very sensible framework to implement the

ML identity, as it will approximate a sequence of related probabilities of increasing complexity; in

some cases, exact simulation from couples of the probabilities is not possible. As a result, SMC is

the computational framework which we will primarily pursue in this article. It is shown that the
cost of MLSMC, to provide a MSE of O(Îµ2), is less than i.i.d. sampling from the most accurate

posterior approximation.

It is noted, however, that some developments both with regards to

theoretical analysis and implentation of the algorithm, must be carefully considered in order to

successfully use MLSMC for nonlocal models.

This article is structured as follows. In section 2 the nonlocal elliptic equations are given, along

with the Bayesian inverse problem, and well-posedness of both are established. In particular, it

is shown that the posterior has a well-deï¬ned Radon-Nikodym derivative w.r.t. prior measure. In

section 3 we consider how one can approximate expectations w.r.t. the prior and the posterior,

speciï¬cally using MLMC and MLSMC methods. Our complexity results are given also. Section

4 provides some numerical implementations of MLMC and MLSMC for the prior, and posterior,

respectively.

3

2 Nonlocal elliptic equations

2.1 Setup

Consider the following equation

L(u)(x) = b(x),

for x âˆˆ â„¦ âŠ‚ Rd

u(x) = 0,

for x âˆˆ Î“,

(2.1)

(2.2)

where L is given by (1.1), the domain â„¦ is simply connected, and its â€œboundaryâ€ Î“ is suï¬ƒciently
regular and nonlocal, in the sense that it has non-zero volume in Rd,
Under appropriate conditions on B, Lâˆ’1 : Hâˆ’s(â„¦ âˆª Î“) â†’ H s

c (â„¦ âˆª Î“), for s âˆˆ [0, 1), with
s = 1 only for the limiting local version in which B(x, x(cid:48)) = (I âˆ’ âˆ‚2/âˆ‚x2)Î´(x(cid:48) âˆ’ x) (or a similar
Rd u(x)dx = 0} denotes the
uniformly elliptic form) [10]. Following [10] H s
volume constrained space of functions, and the fractional Sobolev space H s is deï¬ned as follows,
for s âˆˆ (0, 1),

c = {u âˆˆ H s; u|Î“ = 0,(cid:82)

H s := {u âˆˆ L2(â„¦ âˆª Î“) : (cid:107)u(cid:107)L2(â„¦âˆªÎ“) + |u|H s(â„¦âˆªÎ“) < âˆ},

(2.3)

where

|u|H s(â„¦âˆªÎ“) :=

(cid:90)

(cid:90)

(u(x) âˆ’ u(y))2
|x âˆ’ y|d+2s dydx.

â„¦âˆªÎ“

â„¦âˆªÎ“

We deï¬ne H 0

c := L2

c for ease of notation.

For b âˆˆ V âˆ—(â„¦âˆª Î“), the dual with respect to L2 of some space V (â„¦âˆª Î“), the weak formulation

of (2.2) can be deï¬ned as follows.
v âˆˆ V (â„¦ âˆª Î“). Now, ï¬nd u âˆˆ V (â„¦ âˆª Î“) (satisfying the boundary conditions) such that

Integrate the equation against an arbitrary test function

B(u, v) = F (v),

for all v âˆˆ V (â„¦ âˆª Î“),

(2.4)

where

(cid:90)
(cid:90)

Rd

Rd

B(u, v) :=

F (v) :=

(cid:90)

Rd

[u(x(cid:48)) âˆ’ u(x)]B(x, x(cid:48))dx(cid:48)v(x)dx,

b(x)v(x)dx.

2.2 Numerical methods for forward solution
Let h(cid:96) denote the maximum diameter of an element of V(cid:96) âŠ‚ V(cid:96)+1 âŠ‚ Â·Â·Â· âŠ‚ V . The ï¬nite
approximation of (2.4) is stated as follows. Identify some u(cid:96) âˆˆ V(cid:96) such that

B(u(cid:96), v(cid:96)) = F (v(cid:96)),

for all v(cid:96) âˆˆ V(cid:96).

(2.5)

4

u(cid:96) =(cid:80)M(cid:96)

M(cid:96)(cid:88)

Assuming the spaces V(cid:96) are spanned by elements {Ï†k

(cid:96)}M(cid:96)

k=1, then one can substitute the ansatz

k=1 uk

(cid:96) Ï†k

(cid:96) into the above equation, resulting in the ï¬nite linear system

B(Ï†j

(cid:96), Ï†k

(cid:96) )uk = F (Ï†j

(cid:96)),

for j = 1, . . . , M(cid:96).

(2.6)

k=1

In particular, the spaces {V(cid:96)} will be comprised of discontinuous elements, so that the method
described is a discontinuous Galerkin ï¬nite element method (FEM) [4]. Piecewise polynomial
c for s âˆˆ [0, 1/2), and are therefore conforming when
c = V for s âˆˆ [0, 1/2), so there is no need to impose penalty terms at the boundaries as

discontinuous element spaces are dense in H s
u âˆˆ H s

one must do for smoother problems in which the discontinuous elements are non-conforming.

2.3 Forward UQ

The following assumption will be made for simplicity
Assumption 2.1. V = L2(â„¦âˆªÎ“), with norm (cid:107)Â·(cid:107) and inner product (cid:104)Â·,Â·(cid:105). B(x, x(cid:48)) = B(|x(cid:48)âˆ’x|) â‰¥
0 is continuous with B(0) â‰¥ c(cid:48) > 0, and there exist K1 > 0 such that for all x âˆˆ â„¦

B(x, x(cid:48))dx(cid:48) â‰¤ K1.

(2.7)

(cid:90)

â„¦âˆªÎ“

As shown in Section 6 of [13], this implies that for all u, v, b âˆˆ V
â€¢ F is continuous: |F (v)| â‰¤ (cid:107)b(cid:107)(cid:107)v(cid:107);

â€¢ B is continuous: |B(u, v)| â‰¤ 4K1(cid:107)u(cid:107)(cid:107)v(cid:107);
â€¢ B is coercive: |B(u, u)| â‰¥ K2(cid:107)u(cid:107)2 for some K2(B, â„¦) > 0. This actually follows from the

PoincarÂ´e-type inequality derived in Proposition 2.5 of [1].

Hence, Lax-Milgram lemma ([5], Thm. 1.1.3) can be invoked, guaranteeing existence of a

unique solution u âˆˆ L2(â„¦) such that

In other words, the map b (cid:55)â†’ u is continuous. The system (2.6) inherits solvability since the

(cid:107)u(cid:107) â‰¤ Kâˆ’1

2 (cid:107)b(cid:107).

bilinear is a fortiori coercive on V(cid:96), so that

(cid:107)u(cid:96)(cid:107) â‰¤ Kâˆ’1

2 (cid:107)b(cid:107).

Let BÎ» be a parametrization of B, where Î» âˆ¼ Âµ0 and Î» âˆˆ E, and either E = Rp or E âŠ‚ Rp

compact. Then the following theorem holds.

5

Theorem 2.2 (Well-posedness of forward UQ). If Assumption 2.1 holds almost surely for Î» âˆ¼ Âµ
and the map u (cid:55)â†’ Q is continuous from L2 to R, then the map Î» (cid:55)â†’ Q is almost surely continuous.
Hence Q(Î») âˆˆ Lâˆ âŠƒ Lp for all p â‰¥ 1, i.e. all moments exist. Lp here denotes the space of random

variables X such that(cid:82)

E |X|pÂµ(dÎ») < âˆ.

Proof. Since Assumption 2.1 holds almost surely for Î» âˆ¼ Âµ, then (cid:107)u(cid:107) â‰¤ uâˆ— := Kâˆ’1
2 (cid:107)b(cid:107) uniformly.
So, the quantity of interest Q(Î») = (cid:107)u(Î»)(cid:107) âˆˆ Lâˆ âŠƒ Lp for all p â‰¥ 1. Therefore, for any Q :
L2(â„¦) â†’ R such that Q(Î») := Q(u(Î»)) â‰¤ K(cid:107)u(cid:107), the result follows, since then Q(Î») â‰¤ Qâˆ— := Kuâˆ—

for all Î».

Corollary 2.3 (Well-posedness of ï¬nite approximation). If Assumption 2.1 holds almost surely
for Î» âˆ¼ Âµ and the map u(cid:96) (cid:55)â†’ Q(cid:96) is continuous from L2 to R, then the map through the discrete
system Î» (cid:55)â†’ Q(cid:96) is almost surely continuous and Q(cid:96) âˆˆ Lâˆ.

2.4 Inverse UQ

For the inverse problem, let us assume that some data is given in the form

y = G(Î») + Î¾,

Î» âŠ¥ Î¾ âˆ¼ N (0, Î£),

where

G(Î») := [(cid:104)g1, u(Î»)(cid:105), . . . ,(cid:104)gM , u(Î»)(cid:105)](cid:62),

gi âˆˆ V.

Then the following theorem holds.

(2.8)

(2.9)

Theorem 2.4 (Well-posedness of inverse UQ). The posterior distribution of Î»|y is well-deï¬ned

and takes the form

with Z =(cid:82)

E exp{âˆ’ 1

(Î») =

dÎ·y
dÂµ

exp{âˆ’ 1
2
2|Î£âˆ’1/2(G(Î») âˆ’ y)|2}Âµ(dÎ»).

1
Z

|Î£âˆ’1/2(G(Î») âˆ’ y)|2},

(2.10)

Proof. The form of the posterior is obtained by a change of variables to {Î», Î¾ = y âˆ’G(Î»)}, which

are independent by assumption. Note the change of variables has Jacobian 1. So, changing

variables back, this also gives the joint density. The posterior is obtained by normalizing for the
observed value of y. The form of the observation operator guarantees Z > exp{âˆ’|Î£âˆ’1|(|Gâˆ—|2 +
i is deï¬ned as in the proof of Theorem 2.2, and |Î£âˆ’1|
|y|2)}, where Gâˆ— = (Gâˆ—

M ) and Gâˆ—

2 , . . . ,Gâˆ—

1 ,Gâˆ—

is the matrix norm.

Now deï¬ne G(cid:96)(Î») := [(cid:104)g1, u(cid:96)(Î»)(cid:105), . . . ,(cid:104)gM , u(cid:96)(Î»)(cid:105)],

gi âˆˆ L2(â„¦).

6

Corollary 2.5 (Well-posedness of inverse UQ for ï¬nite problem). The ï¬nite approximation of
the posterior distribution of Î»|y is well-deï¬ned and takes the form

with Z(cid:96) =(cid:82)

dÎ·y
(cid:96)
dÂµ

(Î») =

1
Z

exp{âˆ’ 1
2

|Î£âˆ’1/2(G(cid:96)(Î») âˆ’ y)|2},

(2.11)

E exp{âˆ’ 1

2|Î£âˆ’1/2(G(cid:96)(Î») âˆ’ y)|2}Âµ(dÎ»).

Remark 2.6. The forcing may also be taken as uncertain, although the uniformity will require it

to be deï¬ned on a compact space. The probability space E will be taken as compact for simplicity,

as it is then easy to verify Assumption 2.1.

3 Approximation of expectations

The objective here is to approximate expectations of some functional Ï• : E â†’ R with respect
to a probability measure Î· (Âµ or Î·y), denoted EÎ·(Ï•). The solution u of (2.2) above must be

approximated by some u(cid:96), with a degree of accuracy which depends on (cid:96). Indeed there exists

a hierarchy of levels (cid:96) = 0, . . . , L (where L may be arbitrarily large) of increasing accuracy

(cid:96)=0 via the approximation of (2.10). For the forward problem Î·(cid:96) = Î· for all (cid:96), but

and increasing cost. For the inverse problem this manifests in a hierarchy of target probability
measures {Î·(cid:96)}L
it will be assumed that the function Ï• requires evaluation of the solution of (2.2), as in Theorem
2.2, and the corresponding approximations will be denoted by {Ï•(cid:96)}L
the estimator Ë†Y N
Î·L â‰¡ Î· for all L. The mean square error (MSE) is given by

(cid:96)=0. One may then compute
L âˆ¼ Î·L, where for the forward problem it may be that

n=1 Ï•L(Î»n

L), Î»n

L = 1
N

(cid:80)N
E(cid:12)(cid:12)(cid:12)EÎ·[Ï•(Î»)] âˆ’ Ë†Y N
(cid:12)(cid:12)(cid:12)2

L

= E(cid:110)EÎ·L [Ï•L(Î»)] âˆ’ Ë†Y N
(cid:124)

(cid:123)(cid:122)

L

variance

(cid:111)2
(cid:125)

(cid:124)
(cid:125)
+{EÎ·L[Ï•L(Î»)] âˆ’ EÎ·[Ï•(Î»)]}2

(cid:123)(cid:122)

bias

.

(3.1)

Now assume there is some discretization level, say of diameter hL, which gives rise to an error
estimate on the output of size O(hÎ±

L), for example arising from the deterministic numerical

L , where d is the spatio-temporal dimension. Now the complexity of

approximation of a spatio-temporal problem. This also translates to a number of degrees of
freedom proportional to hâˆ’d
typical forward solves will range from a dot product O(hâˆ’d
L ) (linear) to a full Gaussian elimination
L = O(Îµ), so one would ï¬nd the cost controlled
O(hâˆ’3d
by O(Îµâˆ’Î¶/Î±), for Î¶ âˆˆ (d, 3d). If one can obtain independent, identically distributed samples un
L,
then the necessary number of samples to obtain a variance of size O(Îµ2) is given by N = O(Îµâˆ’2).
The total cost to obtain a mean-square error tolerance of O(Îµ2) is therefore O(Îµâˆ’2âˆ’Î¶/Î±).

L ) (cubic). In this problem one aims to ï¬nd hÎ±

7

3.1 Multilevel Monte Carlo

For the forward UQ problem, in which one can sample directly from Î·(cid:96), one very popular method-

ology for improving the eï¬ƒciency of solution to such problems is the multilevel Monte Carlo

(MLMC) method [14, 11]. Indeed there has been an explosion of recent activity [12] since its

introduction in [11].

In this methodology the simple estimator Ë†Y N

L above for a given desired
(cid:96) ) âˆ’
L is replaced by a telescopic sum of unbiased increment estimators Y N(cid:96)
(cid:96) } are i.i.d. samples, with marginal laws Î·(cid:96)âˆ’1, Î·(cid:96), respectively,
Ï•(cid:96)âˆ’1(Î»(i)
carefully constructed on a joint probability space. This is repeated independently for 0 â‰¤ (cid:96) â‰¤ L.

(cid:96) where {Î»(i)

i=1{Ï•(cid:96)(Î»(i)

(cid:96)âˆ’1)}Nâˆ’1

(cid:96)âˆ’1, Î»(i)

(cid:96) = (cid:80)N(cid:96)

The overall multilevel estimator will be

Ë†YL,Multi =

L(cid:88)

(cid:96)=0

Y N(cid:96)
(cid:96)

,

(3.2)

under the convention that g(Î»(i)âˆ’1) = 0. A simple error analysis shows that the mean squared
error (MSE) in this case is given by

E{ Ë†YL,Multi âˆ’ EÎ·[Ï•(Î»)]}2 =

L(cid:88)
(cid:124)

(cid:96)=0

E(cid:110)

(cid:111)2
(cid:96) âˆ’ [EÎ·(cid:96)Ï•(cid:96)(Î») âˆ’ EÎ·(cid:96)âˆ’1Ï•(cid:96)âˆ’1(Î»)]
(cid:125)
Y N(cid:96)
(cid:125)
(cid:124)
+{EÎ·L[Ï•L(Î»)] âˆ’ EÎ·[Ï•(Î»)]}2

(cid:123)(cid:122)
(cid:123)(cid:122)

variance

.

bias

(3.3)

Notice that the bias is given by the ï¬nest level, whilst the variance is decomposed into a sum of
variances of the increments. The variance of the (cid:96)th increment estimator has the form V(cid:96)Nâˆ’1
,
where the terms V(cid:96) = E|Ï•(cid:96)(Î»(cid:96))âˆ’Ï•(cid:96)âˆ’1(Î»(cid:96)âˆ’1)|2 decay, following from reï¬nement of the approxima-
tion of Î· and/or Ï•. One can therefore balance the variance at a given level V(cid:96) with the number of
samples N(cid:96). As the level increases, the corresponding cost increases, but the variance decreases,

(cid:96)

allowing fewer samples to achieve a given variance. This can be optimized, and results in a total
cost of O(Îµâˆ’ max{2,Î¶/Î±}), in the optimal case.

To be explicit, denote by Q(cid:96) := Q(u(cid:96)(Î»(cid:96)), Î»(cid:96)) the level (cid:96) approximation of the quantity of

interest Q. Introduce the following assumptions

(A1) There exist Î±, Î², Î¶ > 0, and a C > 0 such that

ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³

|E(QL âˆ’ Qâˆ)| â‰¤ ChÎ±
L;
E|Q(cid:96) âˆ’ Q(cid:96)âˆ’1|2 â‰¤ ChÎ²
(cid:96) ;
âˆ’Î¶
â‰¤ Ch
(cid:96)

C(Q(cid:96))

,

(3.4)

where C(Q(cid:96)) denotes the cost to evaluate Q(cid:96).

8

We have the following classical MLMC Theorem [12]

Theorem 3.1. Assume (A1) and max{Î², Î¶} â‰¤ 2Î±. Then for any Îµ > 0, there exist L,{N(cid:96)}L
and C > 0 such that

(cid:96)=0

E(cid:104)(cid:16) Ë†YL,Multi âˆ’ EÎ·[Q]

(cid:17)2(cid:105) â‰¤ CÎµ2,

(3.5)

(3.6)

for the following cost

COST â‰¤ C

ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³

Îµâˆ’2,
Îµâˆ’2| log(Îµ)|2,
Îµâˆ’(2+ Î¶âˆ’Î²

Î± ),

if Î² > Î¶,

if Î² = Î¶,

if Î² < Î¶.

3.2 Multilevel sequential Monte Carlo sampler
Now the inverse problem will be considered. There is a sequence of probability measures {Î·(cid:96)}(cid:96)â‰¥0
on a common measurable space (E,E), and for each l there is a measure Î³(cid:96) : E â†’ R+, such that

where the normalizing constant Z(cid:96) =(cid:82)

Î·(cid:96)(dÎ») =

Î³(cid:96)(dÎ»)

Z(cid:96)

(3.7)

E Î³(cid:96)(dÎ») is unknown. The objective is to compute:

(cid:90)

E

EÎ·âˆ[Ï•(Î»)] :=

Ï•(Î»)Î·âˆ(dÎ»)

for potentially many measurable Î·âˆâˆ’integrable functions Ï• : E â†’ R.

3.2.1 Notations
Let (E,E) be a measurable space. The notation Bb(E) denotes the class of bounded and measur-
able real-valued functions, and the supremum norm is written as (cid:107)f(cid:107)âˆ = supÎ»âˆˆE |f (Î»)|. Consider
non-negative operators K : E Ã— E â†’ R+ such that for each Î» âˆˆ E the mapping A (cid:55)â†’ K(Î», A) is
a ï¬nite non-negative measure on E and for each A âˆˆ E the function Î» (cid:55)â†’ K(Î», A) is measurable;
the kernel K is Markovian if K(Î», dv) is a probability measure for every Î» âˆˆ E. For a ï¬nite
measure Âµ on (E,E), and a real-valued, measurable f : E â†’ R, we deï¬ne the operations:

ÂµK : A (cid:55)â†’

We also write Âµ(f ) =(cid:82) f (Î»)Âµ(dÎ»). In addition (cid:107) Â· (cid:107)r, r â‰¥ 1, denotes the Lrâˆ’norm, where the

f (v) K(Î», dv).

K(Î», A) Âµ(dÎ») ; Kf : Î» (cid:55)â†’

(cid:90)

(cid:90)

expectation is w.r.t. the law of the appropriate simulated algorithm.

9

3.2.2 Algorithm
As described in Section 1, the context of interest is when a sequence of densities {Î·(cid:96)}(cid:96)â‰¥0, as
in (3.7), are associated to an â€˜accuracyâ€™ parameter hl, with h(cid:96) â†’ 0 as (cid:96) â†’ âˆ, such that
âˆ > h0 > h1 Â·Â·Â· > hâˆ = 0. In practice one cannot treat hâˆ = 0 and so must consider these
distributions with h(cid:96) > 0. The laws with large h(cid:96) are easy to sample from with low computational

cost, but are very diï¬€erent from Î·âˆ, whereas, those distributions with small h(cid:96) are hard to sample

with relatively high computational cost, but are closer to Î·âˆ. Thus, we choose a maximum level
L â‰¥ 1 and we will estimate

By the standard telescoping identity used in MLMC, one has

EÎ·L [Ï•(Î»)] :=

EÎ·L[Ï•(Î»)] = EÎ·0 [Ï•(Î»)] +

= EÎ·0 [Ï•(Î»)] +

E

Ï•(Î»)Î·L(Î»)dÎ» .

(cid:90)
(cid:110)EÎ·(cid:96)[Ï•(Î»)] âˆ’ EÎ·(cid:96)âˆ’1[Ï•(Î»)]
(cid:111)
L(cid:88)
(cid:17)
L(cid:88)

(cid:104)(cid:16) Î³(cid:96)(Î»)Z(cid:96)âˆ’1

(cid:96)=1

EÎ·(cid:96)âˆ’1

Î³(cid:96)âˆ’1(Î»)Z(cid:96)

(cid:96)=1

âˆ’ 1

Ï•(Î»)

(cid:105)

.

(3.8)

Suppose now that one applies an SMC sampler [8] to obtain a collection of samples (par-

ticles) that sequentially approximate Î·0, Î·1, . . . , Î·L. We consider the case when one initial-

izes the population of particles by sampling i.i.d. from Î·0, then at every step resamples and

, . . . , Î»1:NLâˆ’1
applies a MCMC kernel to mutate the particles. We denote by (Î»1:N0
Lâˆ’1
+âˆ > N0 â‰¥ N1 â‰¥ Â·Â·Â· NLâˆ’1 â‰¥ 1, the samples after mutation; one resamples Î»1:Nl
accord-
(cid:96)), for indices l âˆˆ {0, . . . , L âˆ’ 1}. We will denote by
ing to the weights G(cid:96)(Î»i
{M(cid:96)}1â‰¤(cid:96)â‰¤Lâˆ’1 the sequence of MCMC kernels used at stages 1, . . . , L âˆ’ 1, such that Î·(cid:96)M(cid:96) = Î·(cid:96).
For Ï• : E â†’ R, (cid:96) âˆˆ {1, . . . , L}, we have the following estimator of EÎ·(cid:96)âˆ’1 [Ï•(Î»)]:

(cid:96)) = (Î³(cid:96)+1/Î³(cid:96))(Î»i

), with

0

l

Î·N(cid:96)âˆ’1
(cid:96)âˆ’1 (Ï•) =

1

N(cid:96)âˆ’1

Ï•(Î»i

(cid:96)âˆ’1) .

N(cid:96)âˆ’1(cid:88)
N(cid:96)âˆ’1(cid:88)

i=1

i=1

We deï¬ne

Î·N(cid:96)âˆ’1
(cid:96)âˆ’1 (G(cid:96)âˆ’1M(cid:96)(dÎ»(cid:96))) =

1

N(cid:96)âˆ’1

G(cid:96)âˆ’1(Î»i

(cid:96)âˆ’1)M(cid:96)(Î»i

(cid:96)âˆ’1, dÎ»(cid:96)) .

The joint probability distribution for the SMC algorithm is

N0(cid:89)

Lâˆ’1(cid:89)

N(cid:96)(cid:89)

Î·0(dÎ»i
0)

i=1

(cid:96)=1

i=1

Î·N(cid:96)âˆ’1
(cid:96)âˆ’1 (G(cid:96)âˆ’1M(cid:96)(dÎ»i

(cid:96)))

Î·N(cid:96)âˆ’1
(cid:96)âˆ’1 (G(cid:96)âˆ’1)

.

If one considers one more step in the above procedure, that would deliver samples {Î»i
standard SMC sampler estimate of the quantity of interest in (3.8) is Î·N

L}NL

i=1, a

L (g); the earlier samples

are discarded. Within a multilevel context, a consistent SMC estimate of (3.8) is

(cid:111)

(cid:96)âˆ’1 (Ï•G(cid:96)âˆ’1)
Î·N(cid:96)âˆ’1
(cid:96)âˆ’1 (G(cid:96)âˆ’1)

âˆ’ Î·N(cid:96)âˆ’1

(cid:96)âˆ’1 (Ï•)

,

(3.9)

(cid:98)Y = Î·N0

0 (Ï•) +

L(cid:88)

(cid:110) Î·N(cid:96)âˆ’1

(cid:96)=1

10

and this will be proven to be superior than the standard one, under assumptions.

The relevant MSE error decomposition here is:

E(cid:2){(cid:98)Y âˆ’ EÎ·âˆ [Ï•(Î»)]}2(cid:3) â‰¤ 2 E(cid:2){(cid:98)Y âˆ’ EÎ·L[Ï•(Î»)]}2(cid:3) + 2{EÎ·L[Ï•(Î»)] âˆ’ EÎ·âˆ [Ï•(Î»)]}2 .

(3.10)

3.2.3 Multilevel SMC

We will now restate an analytical result from [3] that controls the error term E[{(cid:98)Y âˆ’EÎ·L[Ï•(Î»)]}2]
in expression (3.10). For any (cid:96) âˆˆ {0, . . . , L} and Ï• âˆˆ Bb(E) we write: Î·(cid:96)(Ï•) :=(cid:82)

E Ï•(Î»)Î·(cid:96)(Î»)dÎ».

The following standard assumptions will be made ; see [3, 7].

(A2) There exist 0 < C < C < +âˆ such that

G(cid:96)(Î») â‰¤ C ;
G(cid:96)(Î») â‰¥ C .

sup
(cid:96)â‰¥1
inf
(cid:96)â‰¥1

sup
Î»âˆˆE
inf
Î»âˆˆE

(A3) There exists a Ï âˆˆ (0, 1) such that for any (cid:96) â‰¥ 1, (Î», v) âˆˆ E2, A âˆˆ E:

(cid:90)

(cid:90)

M(cid:96)(Î», dÎ»(cid:48)) â‰¥ Ï

M(cid:96)(v, dÎ»(cid:48)) .

Under these assumptions the following Theorem is proven in [3]

A

A

Theorem 3.2. Assume (A2-3). There exist C < +âˆ such that for any Ï• âˆˆ Bb(E), with
(cid:107)Ï•(cid:107)âˆ = 1,

E(cid:2){(cid:98)Y âˆ’ EÎ·L [Ï•(Î»)]}2(cid:3) â‰¤ C

(cid:18) 1

+

N0

(cid:18) Vl

Nlâˆ’1

L(cid:88)

(cid:96)=1

+

V 1/2
(cid:96)
N 1/2
(cid:96)âˆ’1

(cid:19)(cid:19)

,

V 1/2
Nqâˆ’1

q

L(cid:88)

q=(cid:96)+1

where V(cid:96) := (cid:107) Z(cid:96)âˆ’1

Z(cid:96)

G(cid:96)âˆ’1 âˆ’ 1(cid:107)2âˆ.

The following additional assumption will now be made

(A4) There exist Î±, Î², Î¶ > 0, and a C > 0 such that

ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³

V(cid:96)
â‰¤ ChÎ²
(cid:96) ;
|(Î·L âˆ’ Î·âˆ)(1)| â‰¤ ChÎ±
L;
âˆ’Î¶
â‰¤ Ch
(cid:96)

C(G(cid:96))

,

(3.11)

where C(G(cid:96)) denotes the cost to evaluate G(cid:96).

The following Theorem may now be proven

11

Theorem 3.3. Assume (A2-4) and max{Î², Î¶} â‰¤ 2Î±. Then for any Îµ > 0, there exist L,{N(cid:96)}L
and C > 0 such that

(cid:96)=0

E(cid:104)(cid:16)(cid:98)Y âˆ’ EÎ·[Ï•(Î»)]

(cid:17)2(cid:105) â‰¤ CÎµ2,

for the following cost

COST â‰¤ C

ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³

Îµâˆ’2,
Îµâˆ’2| log(Îµ)|2,
Îµâˆ’(2+ Î¶âˆ’Î²

Î± ),

if Î² > Î¶,

if Î² = Î¶,

if Î² < Î¶.

(3.12)

(3.13)

that hÎ±
L

Proof. The MSE can be bounded using (3.10). Following from (A4)(ii), the second term requires
(cid:104) Îµ, and assuming hL = Mâˆ’L for some M â‰¥ 2, this translates to L (cid:104) log Îµ. As in
[3], the additional error term is dealt with by ï¬rst ignoring it and optimizing COST(N), for a

given V(N) = Îµ2, where N := (N1, . . . , NL). This requires that N(cid:96) âˆ(cid:112)Vl/Cl (cid:104) h(Î²+Î¶)/2

. The

(cid:96)

, where KL =(cid:80)Lâˆ’1

l=1 h(Î²âˆ’Î¶)/2

l

, so one has

constraint then requires that N(cid:96) = Îµâˆ’2KLh(Î²+Î¶)/2

(cid:96)

L(cid:88)

COST(N) =

N(cid:96)C(cid:96) = Îµâˆ’2K 2
L.

It was shown in [3] that the result follows, provided Î¶ < 2Î±. In fact, re-examining the additional

(cid:96)=0

L(cid:88)

L(cid:88)

term as in Section 3.3 of [3], one has

Lâˆ’1(cid:88)
where the second line follows from the inequality ((cid:80)L
(cid:96)=0 a(cid:96))2 â‰¤ L(cid:80)L

= O(Îµ2Îµ1âˆ’Î¶/2Î±L1/2Kâˆ’1
L ),

= O(Îµ2Îµ1âˆ’Î¶/2Î±

h(Î²âˆ’Î¶)/4

V 1/2
(cid:96)
N 1/2
(cid:96)âˆ’1

V 1/2
Nqâˆ’1

q

q=(cid:96)+1

(cid:96)=1

(cid:96)=1

(cid:96)

âˆ’3/2
L

)

K

(cid:96) and the deï¬nition
of KL. Now substituting KL = O(1),O(L),O(Îµâˆ’(Î¶âˆ’Î²)/2Î±) for the 3 cases and recalling the
assumption max{Î², Î¶} â‰¤ 2Î± gives V(N) = Îµ2 for the costs in (3.13).

(cid:96)=0 a2

3.2.4 Veriï¬cation of assumptions

Assume a uniform prior Âµ. Following from (2.8), the unnormalized measures will be given by

Î³(cid:96) = exp{âˆ’Î¦(G(cid:96)(Î»))},

(3.14)

where Î¦(Gl(Î»)) = 1

2|Î£âˆ’1/2(Gl(Î») âˆ’ y)|2, and

G(cid:96)(Î») := [(cid:104)g1, u(cid:96)(Î»)(cid:105), . . . ,(cid:104)gM , u(cid:96)(Î»)(cid:105)](cid:62),

gi âˆˆ L2(â„¦),

and ul is the solution of the numerical approximation of (2.2). Notice that these are uniformly
bounded, over both Î» âˆˆ E and over (cid:96), following from Corollary 2.5 for ï¬nite (cid:96), and Theorem 2.4

in the limit.

12

It is shown in [3] Section 4 that

|Î¦(G(cid:96)(Î»)) âˆ’ Î¦(G(cid:96)âˆ’1(Î»))| â‰¤ C(|G(cid:96)|,|G(cid:96)âˆ’1|)|G(cid:96)(Î») âˆ’ G(cid:96)âˆ’1(Î»)|.

One proceeds similarly to that paper, and ï¬nds that

|G(cid:96)(Î») âˆ’ G(cid:96)âˆ’1(Î»)| â‰¤ C(cid:107)u(cid:96)(Î») âˆ’ u(cid:96)âˆ’1(Î»)(cid:107)V .

(3.15)

(3.16)

Note that G(cid:96) = Î³(cid:96)+1/Î³(cid:96), so inserting the bound (3.15) into (3.14), and observing the bound-
edness of the {G(cid:96)} noted above, one can see that Assumption (A2) holds. Now inserting (3.15)
and (3.16) into (3.14), one can see that in order to establish Assumption (A4), it suï¬ƒces to
establish rates of convergence for (cid:107)u(cid:96)(Î») âˆ’ u(cid:96)âˆ’1(Î»)(cid:107)V . In particular, Assumption (A2) and the
C2 inequality (Minkowski plus Youngâ€™s) provide that

V(cid:96) = (cid:107) Z(cid:96)âˆ’1

Z(cid:96)

G(cid:96)âˆ’1 âˆ’ 1(cid:107)2âˆ =

(cid:13)(cid:13)(cid:13)(cid:13) G(cid:96)âˆ’1

Î·(cid:96)âˆ’1(G(cid:96)âˆ’1)

(cid:13)(cid:13)(cid:13)(cid:13)2

âˆ

âˆ’ 1

â‰¤ C((cid:107)G(cid:96)âˆ’1 âˆ’ 1(cid:107)2âˆ + (cid:107)Î·(cid:96)âˆ’1(G(cid:96)âˆ’1) âˆ’ 1(cid:107)2âˆ) â‰¤ 2C(cid:107)G(cid:96)âˆ’1 âˆ’ 1(cid:107)2âˆ
â‰¤ C(cid:48)(cid:107)u(cid:96)(Î») âˆ’ u(cid:96)âˆ’1(Î»)(cid:107)2
V .

Therefore, the rate of convergence of (cid:107)u(cid:96)(Î») âˆ’ u(cid:96)âˆ’1(Î»)(cid:107)V is the required quantity for both
the forward (for Lipschitz Q) and the inverse multilevel estimation problems. By the triangle
inequality it suï¬ƒces to consider the approximation of the truth (cid:107)u(cid:96)(Î») âˆ’ u(Î»)(cid:107)V , and by CÂ´eaâ€™s
lemma ([5], Thm. 2.4.1), Assumption 2.1 guarantees the existence of a C > 0 such that

(cid:107)u(cid:96)(Î») âˆ’ u(Î»)(cid:107)V â‰¤ C inf
v(cid:96)âˆˆV(cid:96)

(cid:107)v(cid:96)(Î») âˆ’ u(Î»)(cid:107)V

(3.17)

Theorem 6.2 of [10] provides rates of convergence of the best approximation above, in the case

that V = H s and the FEM triangulation is shape regular and quasi-uniform. In particular, their
Case 1 corresponds to the case of a singular kernel, i.e. not even integrable, and L : H s
c â†’ Hâˆ’s,
for s âˆˆ (0, 1), so the solution operator is smoothing. Their Case 2 corresponds to a slightly more
regular kernel than ours, where in fact B(x,Â·) âˆˆ L2, rather than L1 as given by Assumption
2.1 and consequently L : L2
c â†’ L2. It is shown that in Case 2, if the solution u âˆˆ H m+t, for
polynomial elements of order m and some t âˆˆ [0, 1], then convergence with respect to the L2
norm is in fact O(hm+t). So, for linear elements, second order convergence can be obtained,

leading to Î² = 4 in (3.11).

Recall that 2.1 ensures well-posedness of the solution from L2 (cid:51) b (cid:55)â†’ u âˆˆ L2, and so discon-

tinuities are allowed. This is actually a strength of nonlocal models and one impetus for their

use. It is reasonable to expect that if the the nodes of the discontinuous elements match up with

the discontinuities, i.e. there is a node at each point of discontinuity for d = 1 or there are nodes

13

all along a curve/surface of discontinuity for d > 1, and if the solution is suï¬ƒciently smooth in

the subdomains, then the rates of convergence should match those of the smooth subproblems.

For example, if there is a point discontinuity such that the domain can be separated at that
point into â„¦1 âˆª â„¦2 = â„¦ and one has u|â„¦1 âˆˆ H m+t(â„¦1) and u|â„¦2 âˆˆ H m+t(â„¦2), where u|â„¦i is the
restriction of u to the set â„¦i, then one expects the convergence rate of m+t to be preserved. This

is postulated and veriï¬ed numerically in [4]. It is also illustrated numerically in that work that

even with discontinuous elements, if there is no node at the discontinuity then the convergence

rate reverts to 1/2, so Î² = 1 in (3.11).

4 Numerical examples

The particular nonlocal model which will be of interest here is that in which the kernel is given

by

BÎ»(x, x(cid:48)) = f (x, x(cid:48), Î¸)

1

(4.1)
where Î» := (Î¸, Î±, Î´), and 1A is the characteristic function which takes the value 1 if (x, x(cid:48)) âˆˆ A
and zero otherwise, and f (x, x(cid:48)) = f (x(cid:48), x). Notice that Î± âˆˆ (0, d) is scalar, but (Î¸, Î´) may

Î´2|x âˆ’ x(cid:48)|Î± 1{|xâˆ’x(cid:48)|<Î´},

be deï¬ned on either a ï¬nite-dimensional subspace of function-space, or in principle in the full

inï¬nite dimensional space. This may be of interest to incorporate spatial dependence of material

properties.

Following [6] we consider the following model. Let â„¦ = [0, 1] and Î“ = [âˆ’Î´, 0) âˆª (1, 1 + Î´]. For

(x, x(cid:48)) âˆˆ â„¦2, let z = (x + x(cid:48))/2. The kernel is deï¬ned by,

0.2 + (z âˆ’ 0.625)2

z + Î¸
14.4 + (z âˆ’ 0.75) + 2

if z âˆˆ [0, 0.625)
if z âˆˆ [0.625, 0.75)
if z âˆˆ [0.75, 1)

ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³

f (x, x(cid:48), Î¸) =

b(x) = 5

The following prior is used for the parameters,

Î¸ âˆ¼ Uniform(1, 2)
Î± âˆ¼ Beta(2, 2)
Î´ âˆ¼ Gamma(1, 1) truncated on (0.125, 1)

This is an extension of an example used in [6], which is identical to this model with Î¸ = 1.25 and

Î± = 1 (various values of Î´ are used there). To solve the equation with FEM, a uniform partition

14

Figure 1: Cost vs. MSE for MLMC

on â„¦ is used with h(cid:96) = 2âˆ’(k+(cid:96)), k = 3. Thus for each level (cid:96), h < Î´. The same discontinuous
Galerkin FEM as in [6] is applied to solve the system.

Similar methods to those in [3] are used to estimate the convergence rates for both the forward

and inverse problems. The estimated rates are Ë†Î± = 2.005 and Ë†Î² = 4.271. The rates Î± = 2 and

Î² = 4 are used for the simulations.

For the inverse problem, data is generated with Î¸ = 2, Î± = 0.5 and Î´ = 0.2. The observations
are y|u âˆ¼ N (m(u), Ïƒ2), where Ïƒ2 = 0.01 and m(u) = [u(a), u(b)](cid:62). The quantity of interest for

both the forward and inverse problem is u(0.5).

The forward problem is solved in the standard multilevel fashion by generating coupled par-

ticles from the prior at each level. The main result, the cost vs. MSE of the estimates is shown

in Figure 1. The Bayesian inverse problem is also solved for this model, and the main result is

shown in Figure 2.

5 Summary

This is the ï¬rst systematic treatment of UQ for nonlocal models, to the knowledge of the au-

thors. Natural extensions include obtaining rigorous convergence results for piecewise smooth

15

20252102152202252302âˆ’202âˆ’152âˆ’102âˆ’520MSE(ğœ€2)Runtimecostâˆâˆ‘ğ¿ğ‘™=0ğ‘ğ‘™â„âˆ’1ğ‘™AlgorithmMLMCMCFigure 2: Cost vs. MSE for MLSMC

solutions, exploring higher-dimensional examples, spatial parameters, time-dependent models,

and parameters deï¬ned on non-compact spaces.

Acknowledgements. KJHL was supported by the DARPA FORMULATE project. AJ &

YZ were supported by Ministry of Education AcRF tier 2 grant, R-155-000-161-112. We express

our gratitude to Marta Dâ€™Elia, Pablo Seleson, and Max Gunzburger for useful discussions.

References

[1] Andreu, F., Mazon, J. M., Jose, M. Rossi, J. & Toledo, J. (2009). A nonlocal p-Laplacian

evolution equation with nonhomogeneous Dirichlet boundary conditions. SIAM Journal on

Mathematical Analysis, 40, 1815â€“1851.

[2] Bakunin, O. G. (2008). Turbulence and diï¬€usion: scaling versus equations. Springer Science

& Business Media.

[3] Beskos, A., Jasra, A., Law, K. J. H, Tempone, R. & Zhou, Y. (2015). Multilevel sequential

Monte Carlo samplers. arXiv preprint arXiv:1503.07259.

16

20252102152202252302âˆ’202âˆ’152âˆ’102âˆ’520MSE(ğœ€2)Runtimecostâˆâˆ‘ğ¿ğ‘™=0ğ‘ğ‘™â„âˆ’1ğ‘™AlgorithmMLSMCSMC[4] Chen, X. & Gunzburger, M. (2011). Continuous and discontinuous ï¬nite element meth-

ods for a peridynamics model of mechanics. Computer Methods in Applied Mechanics and

Engineering, 200, 1237â€“1250.

[5] Ciarlet, P. G. (2002). The ï¬nite element method for elliptic problems. SIAM.

[6] Dâ€™Elia, M. & Gunzburger, M. (2014). Optimal distributed control of nonlocal steady

diï¬€usion problems. SIAM Journal on Control and Optimization. 52, 243â€“273.

[7] Del Moral, P. (2004). Feynman-Kac Formulae: Genealogical and Interacting Particle Sys-

tems with Applications. Springer: New York.

[8] Del Moral, P., Doucet, A. & Jasra, A. (2006). Sequential Monte Carlo samplers.

J. R. Statist. Soc. B, 68, 411â€“436.

[9] Del Moral, P., Jasra, A., Law, K. J. H, & Zhou, Y. (2016). Multilevel sequential Monte

Carlo samplers for normalizing constants. arXiv preprint arXiv:1603.01136.

[10] Du, A., Gunzburger, M., Lehoucq, R. & Zhou, K. (2012). Analysis and approximation

of nonlocal diï¬€usion problems with volume constraints. SIAM review, 54, 667â€“696.

[11] Giles, M. B. (2008). Multilevel Monte Carlo path simulation. Op. Res. 56, 607-617.

[12] Giles, M. B (2015). Multilevel Monte Carlo methods. Acta Numerica, 24, 259-328.

[13] Gunzburger, M. & Lehoucq, R. B. (2010). A nonlocal vector calculus with application

to nonlocal boundary value problems. Multiscale Modeling & Simulation, 8, 1581â€“1598.

[14] Heinrich, S. (2001). Multilevel Monte Carlo methods. Large-scale scientiï¬c computing.

Springer Berlin Heidelberg, 2001. 58-67.

[15] Hoang, V., Schwab, C. & Stuart, A. (2013). Complexity analysis of accelerated MCMC

methods for Bayesian inversion. Inverse Prob., 29, 085010.

[16] Jasra, A., Kamatani, K., Law, K. J., & Zhou, Y. (2015). Multilevel particle ï¬lter. arXiv

preprint arXiv:1510.04977.

[17] Silling, S. A., Zimmermann, M & Abeyaratne, R. (2003). Deformation of a peridynamic

bar. Journal of Elasticity. 73, 173â€“190.

17

