Audio Word2Vec: Unsupervised Learning of Audio Segment Representations

using Sequence-to-sequence Autoencoder

Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-Yi Lee, Lin-Shan Lee

College of Electrical Engineering and Computer Science, National Taiwan University

{b01902040, b01902038, r04921047, hungyilee}@ntu.edu.tw, lslee@gate.sinica.edu.tw

6
1
0
2

 
r
a

 

M
7
1

 
 
]

D
S
.
s
c
[
 
 

3
v
2
8
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

The vector representations of Ô¨Åxed dimensionality for
words (in text) offered by Word2Vec have been shown to be
very useful in many application scenarios, in particular due
to the semantic information they carry. This paper proposes a
parallel version, the Audio Word2Vec. It offers the vector rep-
resentations of Ô¨Åxed dimensionality for variable-length audio
segments. These vector representations are shown to describe
the sequential phonetic structures of the audio segments to a
good degree, with very attractive real world applications such as
query-by-example Spoken Term Detection (STD). In this STD
application, the proposed approach signiÔ¨Åcantly outperformed
the conventional Dynamic Time Warping (DTW) based ap-
proaches at signiÔ¨Åcantly lower computation requirements. We
propose unsupervised learning of Audio Word2Vec from au-
dio data without human annotation using Sequence-to-sequence
Autoencoder (SA). SA consists of two RNNs equipped with
Long Short-Term Memory (LSTM) units: the Ô¨Årst RNN (en-
coder) maps the input audio sequence into a vector represen-
tation of Ô¨Åxed dimensionality, and the second RNN (decoder)
maps the representation back to the input audio sequence. The
two RNNs are jointly trained by minimizing the reconstruction
error. Denoising Sequence-to-sequence Autoencoder (DSA) is
furthered proposed offering more robust learning.

1. Introduction

Word2Vec [1, 2, 3] transforming each word (in text) into a vec-
tor of Ô¨Åxed dimensionality has been shown to be very useful
in various applications in natural language processing. In par-
ticular, the vector representations obtained in this way usually
carry plenty of semantic information about the word. It is there-
fore interesting to ask a question: can we transform the audio
segment of each word into a vector of Ô¨Åxed dimensionality? If
yes, what kind of information these vector representations can
carry and in what kind of applications can these ‚Äúaudio version
of Word2Vec‚Äù be useful? This paper is to try to answer these
questions at least partially.

Representing variable-length audio segments by vectors
with Ô¨Åxed dimensionality has been very useful for many speech
applications. For example, in speaker identiÔ¨Åcation [4], audio
emotion classiÔ¨Åcation [5], and spoken term detection (STD) [6,
7, 8], the audio segments are usually represented as feature vec-
tors to be applied to standard classiÔ¨Åers to determine the speaker
or emotion labels, or whether containing the input queries. In
query-by-example spoken term detection (STD), by represent-
ing each word segment as a vector, easier indexing makes the
retrieval much more efÔ¨Åcient [9].

But audio segment representation is still an open problem.
It is common to use i-vectors to represent utterances in speaker

identiÔ¨Åcation [4], and several approaches have been success-
fully used in STD [9, 6, 7, 8]. But these vector representations
may not be able to precisely describe the sequential phonetic
structures of the audio segments as we wish to have in this pa-
per, and these approaches were developed primarily in more
heuristic ways, rather than learned from data. Deep learning has
also been used for this purpose [10, 11]. By learning Recurrent
Neural Network (RNN) with an audio segment as the input and
the corresponding word as the target, the outputs of the hidden
layer at the last few time steps can be taken as the representation
of the input segment [11]. However, this approach is supervised
and therefore needs a large amount of labeled training data.

On the other hand, autoencoder has been a very success-
ful machine learning technique for extracting representations in
an unsupervised way [12, 13], but its input should be vectors
of Ô¨Åxed dimensionality. This is a dreadful limitation because
audio segments are intrinsically expressed as sequences of ar-
bitrary length. A general framework was proposed to encode
a sequence using Sequence-to-sequence Autoencoder (SA), in
which a RNN is used to encode the input sequence into a Ô¨Åxed-
length representation, and then another RNN to decode this
input sequence out of that representation. This general frame-
work has been applied in natural language processing [14, 15]
and video processing [16], but not yet on speech signals to our
knowledge.

In this paper, we propose to use Sequence-to-sequence Au-
toencoder (SA) to represent variable-length audio segments by
vectors with Ô¨Åxed dimensionality. We hope the vector repre-
sentations obtained in this way can describe more precisely the
sequential phonetic structures of the audio signals, so the au-
dio segments that sound alike would have vector representations
nearby in the space. This is referred to as Audio Word2Vec in
this paper. Different from the previous work [10, 11], here learn-
ing SA does not need any supervision. That is, only the audio
segments without human annotation are needed, good for ap-
plications in the low resource scenario. Inspired from denois-
ing autoencoder [17, 18], an improved version of Denoising
Sequence-to-sequence Autoencoder (DSA) is also proposed.
Among the many possible applications, we chose query-by-
example STD in this preliminary study, and show that the pro-
posed Audio Word2Vec can be very useful. Query-by-example
STD using Audio Word2Vec here is much more efÔ¨Åcient than
the conventional Dynamic Time Warping (DTW) based ap-
proaches, because only the similarities between two single vec-
tors are needed, in additional to the signiÔ¨Åcantly better retrieval
performance obtained.

2. Proposed Approach

The goal here is to transform an audio segment represented by
a variable-length sequence of acoustic features such as MFCC,

x = (x1, x2, ..., xT ), where xt is the acoustic feature at time t
and T is the length, into a vector representation of Ô¨Åxed dimen-
sionality z ‚àà Rd, where d is the dimensionality of the encoded
space. It is desired that this vector representation can describe to
some degree the sequential phonetic structure of the original au-
dio segment. Below we Ô¨Årst give a recap on the RNN Encoder-
Decoder framework [19, 20] in Section 2.1, followed by the
formal presentation of the proposed Sequence-to-sequence Au-
toencoder (SA) in Section 2.2, and its extension in Section 2.3.

2.1. RNN Encoder-Decoder framework
RNNs are neural networks whose hidden neurons form a di-
rected cycle. Given a sequence x = (x1, x2, ..., xT ), RNN
updates its hidden state ht according to the current input xt
and the previous ht‚àí1. The hidden state ht acts as an internal
memory at time t that enables the network to capture dynamic
temporal information, and also allows the network to process
sequences of variable length. In practice, RNN does not seem
to learn long-term dependencies [21], so LSTM [22] has been
widely used to conquer such difÔ¨Åculties. Because many amaz-
ing results were achieved by LSTM, RNN has widely been
equipped with LSTM units [23, 24, 25, 26, 20, 27, 28].

RNN Encoder-Decoder [20, 19] consists of an Encoder
RNN and a Decoder RNN. The Encoder RNN reads the in-
put sequence x = (x1, x2, ..., xT ) sequentially and the hid-
den state ht of the RNN is updated accordingly. After the last
symbol xT is processed, the hidden state hT is interpreted as
the learned representation of the whole input sequence. Then,
by taking hT as input, the Decoder RNN generates the output
sequence y = (y1, y2, ..., xT (cid:48) ) sequentially, where T and T (cid:48)
can be different, or the length of x and y can be different. Such
RNN Encoder-Decoder framework is able to handle input of
variable length. Although there may exists a considerable time
lag between the input symbols and their corresponding output
symbols, LSTM units in RNN are able to handle such situations
well due to the powerfulness of LSTM in modeling long-term
dependencies.

2.2. Sequence-to-sequence Autoencoder (SA)

of an Encoder RNN (the left part of Figure 1) and a Decoder
RNN (the right part). Given an audio segment represented as an
acoustic feature sequence x = (x1, x2, ..., xT ) of any length T ,
the Encoder RNN reads each acoustic feature xt sequentially
and the hidden state ht is updated accordingly. After the last
acoustic feature xT has been read and processed, the hidden
state hT of the Encoder RNN is viewed as the learned repre-
sentation z of the input sequence (the small red block in Fig-
ure 1). The Decoder RNN takes hT as the Ô¨Årst input and gen-
erates its Ô¨Årst output y1. Then it takes y1 as input to generate
y2, and so on. Based on the principles of Autoencoder [12, 13],
the target of the output sequence y = (y1, y2, ..., yT ) is the in-
put sequence x = (x1, x2, ..., xT ). In other words, the RNN
Encoder and Decoder are jointly trained by minimizing the re-
construction error, measured by the general mean squared er-
t=1 (cid:107)xt ‚àí yt(cid:107)2. Because the input sequence is taken as
the learning target, the training process does not need any la-
beled data. The Ô¨Åxed-length vector representation z will be a
meaningful representation for the input audio segment x, be-
cause the whole input sequence x can be reconstructed from z
by the RNN Decoder.

ror(cid:80)T

2.3. Denoising Sequence-to-sequence Autoencoder (DSA)
To learn more robust representation, we further apply the de-
noising criterion [18] to the SA learning proposed above. The
input acoustic feature sequence x is randomly added with some
noise, yielding a corrupted version Àúx. Here the input to SA is Àúx,
and SA is expected to generate the output y closest to the orig-
inal x based on Àúx. The SA learned with this denoising criterion
is referred to as Denoising SA (DSA) below.

3. An Example Application:

Query-by-example STD

Figure 1: Sequence-to-sequence Autoencoder (SA) consists of two
RNNs: RNN Encoder (the left large block in yellow) and RNN Decoder
(the right large block in green). The RNN Encoder reads an audio seg-
ment represented as an acoustic feature sequence x = (x1, x2, ..., xT )
and maps it into a vector representation of Ô¨Åxed dimensionality z;
and the RNN Decoder maps the vector z to another sequence y =
(y1, y2, ..., yT ). The RNN Encoder and Decoder are jointly trained to
make the output sequence y as close the input sequence x as possible,
or to minimize the reconstruction error.

Figure 1 depicts the structure of the proposed Sequence-
to-sequence Autoencoder (SA), which integrates the RNN
Encoder-Decoder framework with Autoencoder for unsuper-
vised learning of audio segment representations. SA consists

Figure 2: The example application of query-by-example STD. All audio
segments in the audio archive are segmented based on word boundaries
and represented by Ô¨Åxed-length vectors off-line. When a spoken query
is entered, it is also represented as a vector, and the similarity between
this vector and all vectors for segments in the archive are calculated,
and the audio segments are ranked accordingly.

The audio segment representation z learned in the last sec-
tion can be applied in many possible scenarios. Here in the pre-
liminary tests we consider the unsupervised query-by-example
STD, whose target is to locate the occurrence regions of the in-
put spoken query term in a large spoken archive without speech
recognition. Figure 2 shows how the representation z proposed
here can be easily used in this task. This approach is inspired
from the previous work [9], but completely different in the
ways to represent the audio segments. In the upper half of Fig-
ure 2, the audio archive are segmented based on word bound-
aries into variable-length sequences, and then the system ex-
ploits the trained RNN encoder in Figure 1 to encode these au-

‚ãØ‚ãØùë•#ùë•$ùë•%ùë•&ùë¶&ùë¶$ùë¶#ùë¶&ùë¶%ùë¶$ùë¶#(&‚Ñé#: VectorRepresentation zRNNEncoderRNNDecoderAudio SegmentAcoustic Features Learning Targetsùë•#ùë•$ùë•%ùë•&‚ãØ‚ãØVariable-lengthaudio segmentsSpoken QueryRNNEncoderFixed-length vectors ùõøFixed-length vector ùõøSimilaritySearchresultsAudio ArchiveRNNEncoderOff-lineOn-linedio segments into Ô¨Åxed-length vectors. All these are done off-
line. In the lower left corner of Figure 2, when a spoken query
is entered, the input spoken query is similarly encoded by the
same RNN encoder into a vector. The system then returns a list
of audio segments in the archive ranked according to the co-
sine similarities evaluated between the vector representation of
the query and those of all segments in the archive. Note that
the computation requirements for the on-line process here are
extremely low.

4. Experiments

Here, we report the experiments and results including the exper-
imental setup (Section 4.1), analysis for the vector representa-
tions learned (Section 4.2), and query-by-example STD results
(Section 4.3).
4.1. Experimental Setup
We used LibriSpeech corpus [29] as the data for the exper-
iments. Due to the computation resource limitation, only 5.4
hours dev-clean (produced by 40 speakers) was used for train-
ing SA and DSA. Another 5.4 hours test-clean (produced by
a completely different group of 40 speakers) was the testing
set. MFCCs of 13-dim were used as the acoustic features. Both
the training and testing sets were segmented according to the
word boundaries obtained by forced alignment with respect to
the reference transcriptions. Although the oracle word bound-
aries were used here for the query-by-example STD in the pre-
liminary tests, the comparison in Section 4.3 was fair since the
baselines used the same segmentation.

SA and DSA were implemented with Theano [30, 31]. The
network structure and hyper parameters were set as below with-
out further tuning if not speciÔ¨Åed:

‚Ä¢ Both RNN Encoder and Decoder consisted of one hidden
layer with 100 LSTM units. That is, each input segment
was mapped into a 100-dim vector representation by SA
or DSA. We adopted the LSTM described in [32], and
the peephole connections [33] were added.

‚Ä¢ The networks were trained by SGD without momentum,

with a Ô¨Åxed learning rate of 0.3 and 500 epochs.

‚Ä¢ For DSA, zero-masking [18] was used to generate the
noisy input Àúx, which randomly wiped out some elements
in each input acoustic feature and set them to zero. The
probability to be wiped out was set to 0.3.

For query-by-example STD, the testing set served as the au-
dio archive to be searched through. Each word segment in the
testing set was taken as a spoken query once. When a word seg-
ment was taken as a query, it was excluded from the archive
during retrieval. There were 5557 queries in total. Mean Aver-
age Precision (MAP) [34] was used as the evaluation measure
for query-by-example STD as the previous work [35].

4.2. Analysis of the Learned Representations
In this section, we wish to verify that the vector representa-
tions obtained here describe the sequential phonetic structures
of the audio segments, or words with similar pronunciations
have close vector representations.

We Ô¨Årst used the SA and DSA learned from the training
set to encode the segments in the testing set, which were never
seen during training, and then computed the cosine similarity
between each segment pair. The average results for groups of
pairs clustered by the phoneme edit distances between the two
words in the pair are in Table 1. It is clear from Table 1 that

two segments for words with larger phoneme sequence edit dis-
tances have obviously smaller cosine similarity in average. We
also found that SA and DSA can even very clearly distinguish
those word segments with only one different phoneme, because
the average similarity for the cluster of zero edit distance is 0
(exactly the same pronunciation) is remarkably larger than that
for edit distance being 1. The gap is even larger for clusters
with edit distances being 2 v.s. 1. For the cluster of zero edit
distance, the average similarity is only 0.48-0.50. This is rea-
sonable because it is well known that even with exactly iden-
tical phoneme segments, the acoustic realizations can be very
different. It seems DSA is slightly better than SA, because the
similarity by DSA for pairs with 0 or 1 edit distances is slightly
larger, while much smaller for those with 4, 5 or more edit dis-
tances. These results show that SA and DSA can in fact encode
the acoustic signals into vector representations describing the
sequential phonetic structures of the signals. The performance
between SA and DSA can be further compared with other base-
lines in Section 4.3.

Table 1: The average cosine similarity between the vector presentations
for all the segment pairs in the testing set, clustered by the phoneme
sequence edit distances.

Phoneme Sequence Edit Distance # of pairs

Average cosine

similarity
SA

DSA
0.4847 0.5012
95222
95355
0.4016 0.4237
987934 0.2674 0.2798
4651318 0.0835 0.0986
4791482 0.0255 0.0196
3078684 0.0051 0.0013

0 (same)

1
2
3
4

5 or more

Because RNN Encoder in Figure 1 reads the input acoustic
sequence sequentially, it may be possible that the last few acous-
tic features dominate the vector representation, and as a result
those words with the same sufÔ¨Åxes are hard to be distinguished
by the learned representations. To justify this hypothesis, we
analyze the following two sets of words with the same sufÔ¨Åxes:

‚Ä¢ Set 1: father, mother, another
‚Ä¢ Set 2: ever, never, however

Same as Table 1, we used cosine similarity between the two
encoded vectors averaged over each group of pairs to measure
the differences, with results in Table 2. For Set 1 in Table 2,

Table 2: The average cosine similarity between the vector representa-
tions for words with the same sufÔ¨Åxes.

Set 1

father v.s. father
father v.s. mother
father v.s. another

Set 2

# of pairs Average cosine similarity

406
832
1276
# of pairs Average cosine similarity

0.5047
0.2639
0.1964

SA

SA

DSA
0.5209
0.2748
0.1861

DSA
0.4265
0.2238
0.2419

ever v.s. ever
ever v.s. never
ever v.s. however

46
1581
713

0.4027
0.2475
0.2638

we see the vector representations of the audio segments of ‚Äúfa-
ther‚Äù are much closer to each other than to those for ‚Äúmother‚Äù,
while those for ‚Äúanother‚Äù are even much farther away. Simi-
lar for Set 2. There exists much more similar examples in the

dataset, but we just show two here due to the space limitation.
These results show that although SA or DSA read the acous-
tic signals sequentially, words with the same sufÔ¨Åx are actually
clearly distinguishable from the learned vectors.

For another test, we selected two sets of word pairs differing

by only the Ô¨Årst phoneme or the last phoneme as below.

‚Ä¢ Set 3: (new, few), (near, fear), (name, fame), (night, Ô¨Åght)
‚Ä¢ Set 4: (hand, hands), (word, words), (day, days), (say,

says), (thing, things)

We evaluated the difference vectors between the averages of the
vector representations by SA for the two words in a pair, e.g.,
Œ¥(word1) ‚àí Œ¥(word2), and reduced the dimensionality of the
difference vectors to 2 to see more easily how they actually dif-
ferentiated in the vector space [1]. The results for Sets 3 and
4 are respectively in Figure 3(a) and Figure 3(b). We see in
each case the difference vectors are in fact close in their direc-
tions and magnitudes, for example, Œ¥(new) ‚àí Œ¥(few) is close
to Œ¥(night) ‚àí Œ¥(Ô¨Åght) in Figure 3(a), and Œ¥(days) ‚àí Œ¥(day)
is close to Œ¥(things) ‚àí Œ¥(thing) in Figure 3(b). This implies
‚Äúphoneme replacement‚Äù is somehow realizable here, for exam-
ple, Œ¥(new)‚àí Œ¥(few) + Œ¥(Ô¨Åght) is not very far from Œ¥(Ô¨Åght) in
Figure 3(a), and Œ¥(things) ‚àí Œ¥(thing) + Œ¥(day) is not very far
from Œ¥(days) in Figure 3(b). They are not ‚Äúvery close‚Äù because
it is well known that the same words can have very different
acoustic realizations.

(a) Set 3: the Ô¨Årst phoneme changes from f to n.

(b) Set 4: the last phoneme differs by existing s or not.

Figure 3: Difference vectors between the average vector representations
for word pairs differing by (a) the Ô¨Årst phoneme as in Set 3 and (b) the
last phoneme as in Set 4.

All results above show the vector representations in test of-
fer very good descriptions for the sequential phonemic struc-
tures of the acoustic segments.
4.3. Query-by-example STD
Here we compared the performance of using the vector repre-
sentations from SA and DSA in query-by-example STD with
two baselines. The Ô¨Årst baseline used frame-based DTW [36]
which is commonly used in query-by-example STD tasks. Here

we used the vanilla version of DTW (with Euclidean distance
as the frame-level distance) to compute the similarities between
the input query and the audio segments, and ranked the seg-
ments accordingly. The second baseline used the same frame-
work in Figure 2, except that the RNN Encoder is replaced by a
manually designed encoder. In this encoder, we Ô¨Årst divide the
input acoustic feature sequence x = (x1, x2, ..., xT ), where xt
is the 13-dimensional acoustic feature vector at time t, into m
segments with roughly equal length T
m , then average each seg-
ment of vectors into a single 13-dimensional vector, and Ô¨Ånally
concatenate these m average vectors sequentially into a vector
representation of dimensionality 13 √ó m. We refer to this ap-
proach as Na¬®ƒ±ve Encoder (NE) below. Although NE is simple,
similar approaches have been used in STD and achieved suc-
cessful results [6, 7, 8]. We found empirically that NE with 52,
78 and 104 dimensions (or m = 4, 6, 8), represented by NE52,
NE78, and NE104 below, yielded the best results. So NE52,
NE78, and NE104 were used in the following experiments.

Figure 4: The retrieval performance in MAP of DTW, Na¬®ƒ±ve Encoder
(NE52 , NE78, and NE104), SA and DSA. The horizontal axis is the
number of epochs in training SA and DSA, not relevant to all baselines.
Figure 4 shows the retrieval performance in MAP of DTW,
NE52, NE78, NE104, SA and DSA. The horizontal axis is the
number of epochs in training SA and DSA, not relevant to
all baselines. Besides DTW which compute the distance be-
tween audio segments directly, all other approaches mapped
the variable-length audio segments to Ô¨Åxed-length vectors for
similarity evaluation, but in different approaches. DTW is not
comparable with NE52, NE78, and NE104 probably because
only the vanilla version was used, and the averaging process in
NE may smooth some of the local signal variations which may
cause disturbances in DTW. We see as the training epochs in-
creased, both SA and DSA resulted in higher MAP than NE. SA
outperformed NE52, NE78, and NE104 after about 450 epochs,
while DSA did that after about 390 epochs, and was able to
achieve much higher MAP. Here DSA clearly outperformed SA
in most cases. These results veriÔ¨Åed that the learned vector rep-
resentations can be useful in real world applications.

5. Conclusions and Future Work

We propose to use Sequence-to-sequence Autoencoder (SA)
and its extension for unsupervised learning of Audio Word2Vec
to obtain vector representations for audio segments. We show
SA can learn vector representations describing the sequential
phonetic structures of the audio segments, and these represen-
tations can be useful in real world applications, such as query-
by-example STD tested in this preliminary study. For the future
work, we are training the SA on larger corpora with different di-
mensionality in different extensions, and considering other ap-
plication scenarios.

‚àí0.25‚àí0.2‚àí0.15‚àí0.1‚àí0.0500.050.10.150.20.25‚àí0.2‚àí0.15‚àí0.1‚àí0.0500.050.10.150.20.250.3fightnightfewnewfearnearfamename‚àí0.2‚àí0.15‚àí0.1‚àí0.0500.050.10.150.20.25‚àí0.15‚àí0.1‚àí0.0500.050.10.150.2daydayshandhandssaysaysthingthingswordwords1502002503003504004505000.120.140.160.180.20.220.240.260.280.30.32EpochsMAP  SADSANE52NE78NE104DTW[23] J. Schmidhuber, D. Wierstra, M. Gagliolo, and F. Gomez, ‚ÄúTrain-
ing recurrent networks by evolino,‚Äù Neural Computation, vol. 19,
no. 3, pp. 757‚Äì779, 2007.

[24] J. Bayer, D. Wierstra, J. Togelius, and J. Schmidhuber, ‚ÄúEvolving
memory cell structures for sequence learning,‚Äù in ICANN, 2009.
[25] H. Sak, A. Senior, and F. Beaufays, ‚ÄúLong short-term memory re-
current neural network architectures for large scale acoustic mod-
eling,‚Äù in INTERSPEECH, 2014.

[26] P. Doetsch, M. Kozielski, and H. Ney, ‚ÄúFast and robust training of
recurrent neural networks for ofÔ¨Çine handwriting recognition,‚Äù in
ICFHR, 2014.

[27] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, ‚ÄúEmpirical evalu-
ation of gated recurrent neural networks on sequence modeling,‚Äù
arXiv preprint arXiv:1412.3555, 2014.

[28] K. Greff, R. K. Srivastava, J. Koutn¬¥ƒ±k, B. R. Steunebrink, and
J. Schmidhuber, ‚ÄúLstm: A search space odyssey,‚Äù arXiv preprint
arXiv:1503.04069, 2015.

[29] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib-
rispeech: an ASR corpus based on public domain audio books,‚Äù
in ICASSP, 2015.

[30] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio,
‚ÄúTheano: a CPU and GPU math expression compiler,‚Äù in SciPy,
2010.

[31] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfel-
low, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Ben-
gio, ‚ÄúTheano: new features and speed improvements,‚Äù CoRR, vol.
abs/1211.5590, 2012.

[32] A. Graves, ‚ÄúGenerating sequences with recurrent neural net-

works,‚Äù CoRR, vol. abs/1308.0850, 2013.

[33] F. Gers, J. Schmidhuber et al., ‚ÄúRecurrent nets that time and

count,‚Äù in IJCNN, 2000.

[34] C. D. Manning, P. Raghavan, and H. Sch?tze, Introduction to In-

formation Retrieval. Cambridge University Press, 2008, ch. 8.

[35] C.-T. Chung, C.-A. Chan, and L.-S. Lee, ‚ÄúUnsupervised spoken
term detection with spoken queries by multi-level acoustic pat-
terns with varying model granularity,‚Äù in ICASSP, 2014.

[36] H. Sakoe and S. Chiba, ‚ÄúDynamic programming algorithm op-
timization for spoken word recognition,‚Äù Acoustics, Speech and
Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43‚Äì
49, 1978.

6. References

[1] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
‚ÄúDistributed representations of words and phrases and their com-
positionality,‚Äù in NIPS, 2013, pp. 3111‚Äì3119.

[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfÔ¨Åcient esti-
mation of word representations in vector space,‚Äù arXiv preprint
arXiv:1301.3781, 2013.

[3] Q. V. Le and T. Mikolov, ‚ÄúDistributed representations of sentences

and documents,‚Äù arXiv preprint arXiv:1405.4053, 2014.

[4] N. Dehak, R. Dehak, P. Kenny, N. Brummer, P. Ouellet, and P. Du-
mouchel, ‚ÄúSupport vector machines versus fast scoring in the low-
dimensional total variability space for speaker veriÔ¨Åcation,‚Äù in IN-
TERSPEECH, 2009.

[5] B. Schuller, S. Steidl, and A. Batliner, ‚ÄúThe INTERSPEECH 2009

emotion challenge,‚Äù in INTERSPEECH, 2009.

[6] H.-Y. Lee and L.-S. Lee, ‚ÄúEnhanced spoken term detection using
support vector machines and weighted pseudo examples,‚Äù Audio,
Speech, and Language Processing, IEEE Transactions on, vol. 21,
no. 6, pp. 1272‚Äì1284, 2013.

[7] I.-F. Chen and C.-H. Lee, ‚ÄúA hybrid HMM/DNN approach to key-

word spotting of short words,‚Äù in INTERSPEECH, 2013.

[8] A. Norouzian, A. Jansen, R. Rose, and S. Thomas, ‚ÄúExploiting
discriminative point process models for spoken term detection,‚Äù
in INTERSPEECH, 2012.

[9] K. Levin, A. Jansen, and B. V. Durme, ‚ÄúSegmental acoustic in-

dexing for zero resource keyword search,‚Äù in ICASSP, 2015.

[10] S. Bengio and G. Heigold, ‚ÄúWord embeddings for speech recog-

nition,‚Äù in INTERSPEECH, 2014.

[11] G. Chen, C. Parada, and T. N. Sainath, ‚ÄúQuery-by-example
keyword spotting using long short-term memory networks,‚Äù in
ICASSP, 2015.

[12] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimension-
ality of data with neural networks,‚Äù Science, vol. 313, no. 5786,
pp. 504‚Äì507, 2006.

[13] P. Baldi, ‚ÄúAutoencoders, unsupervised learning, and deep archi-
tectures,‚Äù Unsupervised and Transfer Learning Challenges in Ma-
chine Learning, Volume 7, p. 43, 2012.

[14] J. Li, M.-T. Luong, and D. Jurafsky, ‚ÄúA hierarchical neural autoen-
coder for paragraphs and documents,‚Äù in arXiv preprint arXiv:
1506.01057, 2015.

[15] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba,
R. Urtasun, and S. Fidler, ‚ÄúSkip-thought vectors,‚Äù in arXiv
preprint arXiv: 1506.06726, 2015.

[16] N. Srivastava, E. Mansimov, and R. Salakhutdinov, ‚ÄúUnsuper-
vised learning of video representations using LSTMs,‚Äù arXiv
preprint arXiv:1502.04681, 2015.

[17] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, ‚ÄúEx-
tracting and composing robust features with denoising autoen-
coders,‚Äù in ICML, 2008.

[18] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Man-
zagol, ‚ÄúStacked denoising autoencoders: Learning useful repre-
sentations in a deep network with a local denoising criterion,‚Äù
JMLR, vol. 11, pp. 3371‚Äì3408, 2010.

[19] I. Sutskever, O. Vinyals, and Q. V. Le, ‚ÄúSequence to sequence

learning with neural networks,‚Äù in NIPS, 2014.

[20] K. Cho, B. Van Merri¬®enboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, ‚ÄúLearning phrase repre-
sentations using rnn encoder-decoder for statistical machine trans-
lation,‚Äù arXiv preprint arXiv:1406.1078, 2014.

[21] Y. Bengio, P. Simard, and P. Frasconi, ‚ÄúLearning long-term de-
pendencies with gradient descent is difÔ¨Åcult,‚Äù Neural Networks,
IEEE Transactions on, vol. 5, no. 2, pp. 157‚Äì166, 1994.

[22] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù

Neural Computation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.

