A Character-Level Decoder without Explicit Segmentation

for Neural Machine Translation

Junyoung Chung

University of Montreal

Kyunghyun Cho

New York University

junyoung.chung@umontreal.ca

Yoshua Bengio

University of Montreal
CIFAR Senior Fellow

Abstract

The existing machine translation systems,
whether phrase-based or neural, have
relied almost exclusively on word-level
modelling with explicit segmentation. In
this paper, we ask a fundamental question:
can neural machine translation generate
a character sequence without any explicit
segmentation? To answer this question,
we evaluate an attention-based encoder–
decoder with a subword-level encoder and
a character-level decoder on four language
pairs–En-Cs, En-De, En-Ru and En-Fi–
using the parallel corpora from WMT’15.
Our experiments show that the models
with a character-level decoder outperform
the ones with a subword-level decoder on
all of the four language pairs. Further-
more, the ensembles of neural models with
a character-level decoder outperform the
state-of-the-art non-neural machine trans-
lation systems on En-Cs, En-De and En-Fi
and perform comparably on En-Ru.

Introduction

1
The existing machine translation systems have re-
lied almost exclusively on word-level modelling
with explicit segmentation. This is mainly due
to the issue of data sparsity which becomes much
more severe, especially for n-grams, when a sen-
tence is represented as a sequence of characters
rather than words, as the length of the sequence
grows signiﬁcantly.
In addition to data sparsity,
we often have a priori belief that a word, or its
segmented-out lexeme, is a basic unit of meaning,
making it natural to approach translation as map-
ping from a sequence of source-language words to
a sequence of target-language words.

This has continued with the more recently
proposed paradigm of neural machine transla-

tion, although neural networks do not suffer from
character-level modelling and rather suffer from
the issues speciﬁc to word-level modelling, such
as the increased computational complexity from a
very large target vocabulary (Jean et al., 2015; Lu-
ong et al., 2015b). Therefore, in this paper, we ad-
dress a question of whether neural machine trans-
lation can be done directly on a sequence of char-
acters without any explicit word segmentation.

To answer this question, we focus on represent-
ing the target side as a character sequence. We
evaluate neural machine translation models with
a character-level decoder on four language pairs
from WMT’15 to make our evaluation as convinc-
ing as possible. We represent the source side as
a sequence of subwords extracted using byte-pair
encoding from Sennrich et al. (2015), and vary the
target side to be either a sequence of subwords or
characters. On the target side, we further design a
novel recurrent neural network (RNN), called bi-
scale recurrent network, that better handles multi-
ple timescales in a sequence, and test it in addition
to a naive, stacked recurrent neural network.

On all of the four language pairs–En-Cs, En-De,
En-Ru and En-Fi–, the models with a character-
level decoder outperformed the ones with a
subword-level decoder. We observed a similar
trend with the ensemble of each of these con-
ﬁgurations, outperforming both the previous best
neural and non-neural translation systems on En-
Cs, En-De and En-Fi, while achieving a compara-
ble result on En-Ru. We ﬁnd these results to be
a strong evidence that neural machine translation
can indeed learn to translate at the character-level
and that in fact, it beneﬁts from doing so.

2 Neural Machine Translation

Neural machine translation refers to a recently
proposed approach to machine translation (Cho et

6
1
0
2

 
r
a

 

M
3
2

 
 
]
L
C
.
s
c
[
 
 

2
v
7
4
1
6
0

.

3
0
6
1
:
v
i
X
r
a

al., 2014; Sutskever et al., 2014; Bahdanau et al.,
2015). This approach aims at building an end-to-
end neural network that takes as input a source
sentence X = (x1, . . . , xTx) and outputs its trans-
lation Y = (y1, . . . , yTy ), where xt and yt(cid:48) are re-
spectively source and target symbols. This neural
network is constructed as a composite of an en-
coder network and a decoder network.

The encoder network encodes the input sen-
tence X into its continuous representation.
In
this paper, we closely follow the neural transla-
tion model proposed in Bahdanau et al. (2015)
and use a bidirectional recurrent neural network,
which consists of two recurrent neural networks.
The forward network reads the input sentence
−→
in a forward direction: −→z t =
φ (ex(xt),−→z t−1),
where ex(xt) is a continuous embedding of the
t-th input symbol, and φ is a recurrent activa-
Similarly,
tion function.
the reverse network
reads the sentence in a reverse direction (right
←−
φ (ex(xt),←−z t+1). At each loca-
to left): ←−z t =
tion in the input sentence, we concatenate the hid-
(cid:3). Then the decoder computes the
zt = (cid:2)−→z t;←−z t
den states from the forward and reverse RNNs
to form a context set C = {z1, . . . , zTx} , where
tion: log p(Y |X) =(cid:80)Ty

conditional distribution over all possible transla-
tions based on this context set. This is done by ﬁrst
rewriting the conditional probability of a transla-
t(cid:48)=1 log p(yt(cid:48)|y<t(cid:48), X). For
each conditional term in the summation, the de-
coder RNN updates its hidden state by

ht(cid:48) = φ(ey(yt(cid:48)−1), ht(cid:48)−1, ct(cid:48)),

(1)

where ey is the continuous embedding of a target
symbol. ct(cid:48) is a context vector computed by a soft-
alignment mechanism:

ct(cid:48) = falign(ey(yt(cid:48)−1), ht(cid:48)−1, C)).

(2)

The soft-alignment mechanism falign weights
each vector in the context set C according to its
relevance given what has been translated. The
weight of each vector zt is computed by

αt,t(cid:48) =

1
Z

efscore(ey(yt(cid:48)−1),ht(cid:48)−1,zt),

(3)

where fscore is a parametric function returning an
unnormalized score for zt given ht(cid:48)−1 and yt(cid:48)−1.
We use a feedforward network with a single hid-
den layer in this paper.1 Z is a normalization con-
k=1 efscore(ey(yt(cid:48)−1),ht(cid:48)−1,zk). This

stant: Z =(cid:80)Tx

1For other possible implementations, see (Luong et al., 2015a).

procedure can be understood as computing the
alignment probability between the t(cid:48)-th target
symbol and t-th source symbol.

The hidden state ht(cid:48), together with the previous
target symbol yt(cid:48)−1 and the context vector ct(cid:48), is
fed into a feedforward neural network to result in
the conditional distribution:
yt(cid:48)
out(ey(yt(cid:48)−1),ht(cid:48) ,ct(cid:48) ).

p(yt(cid:48) | y<t(cid:48), X) ∝ ef

(4)

The whole model, consisting of the encoder,
decoder and soft-alignment mechanism, is then
tuned end-to-end to minimize the negative log-
likelihood using stochastic gradient descent.

3 Towards Character-Level Translation
3.1 Motivation
Let us revisit how the source and target sen-
tences (X and Y ) are represented in neural ma-
chine translation. For the source side of any given
training corpus, we scan through the whole cor-
pus to build a vocabulary Vx of unique tokens to
which we assign integer indices. A source sen-
tence X is then built as a sequence of the indices
of such tokens belonging to the sentence,
i.e.,
X = (x1, . . . , xTx), where xt ∈ {1, 2, . . . ,|Vx|}.
The target sentence is similarly transformed into a
target sequence of integer indices.

Each token, or its index, is then transformed
into a so-called one-hot vector of dimensionality
|Vx|. All but one elements of this vector are set
to 0. The only element whose index corresponds
to the token’s index is set to 1. This one-hot vec-
tor is the one which any neural machine transla-
tion model sees. The embedding function, ex or
ey, is simply the result of applying a linear trans-
formation (the embeddings matrix) to this one-hot
vector.

The important property of this approach based
on one-hot vectors is that the neural network is
oblivious to the underlying semantics of the to-
kens. To the neural network, each and every token
in the vocabulary is equal distance away from ev-
ery other token. The semantics of those tokens are
simply learned (into the embeddings) to maximize
the translation quality, or the log-likelihood of the
model.

This property allows us great freedom in the
choice of tokens’ unit. Neural networks have been
shown to work well with word tokens (Bengio et
al., 2001; Schwenk, 2007; Mikolov et al., 2010)

but also with ﬁner units, such as subwords (Sen-
nrich et al., 2015; Botha and Blunsom, 2014; Lu-
ong et al., 2013) as well as symbols resulting
from compression/encoding (Chitnis and DeNero,
2015). Although there have been a number of
previous research reporting the use of neural net-
works with characters (see, e.g., Mikolov et al.
(2012)), the dominant approach has been to pre-
process the text into a sequence of symbols, each
associated with a sequence of characters, after
which the neural network is presented with those
symbols rather than with characters.

More recently in the context of neural machine
translation, two research groups have proposed to
directly use characters. Kim et al. (2015) proposed
to represent each word not as a single integer index
as before, but as a sequence of characters, and use
a convolutional network followed by a highway
network (Srivastava et al., 2015) to extract a con-
tinuous representation of the word. This approach,
which effectively replaces the embedding func-
tion ex, was adopted by Costa-Juss`a and Fonollosa
(2016) for neural machine translation. Similarly,
Ling et al. (2015b) use a bidirectional recurrent
neural network to replace the embedding functions
ex and ey to respectively encode a character se-
quence to and from the corresponding continuous
word representation. A similar, but slightly differ-
ent approach was proposed by Lee et al. (2015),
where they explicitly mark each character with its
relative location in a word (e.g., “B”eginning and
“I”ntermediate).

Despite the fact that these recent approaches
work at the level of characters, it is less satisfying
that they all rely on knowing how to segment char-
acters into words. Although it is generally easy
for languages like English, this is not always the
case. This word segmentation procedure can be
as simple as tokenization followed by some punc-
tuation normalization, but also can be as compli-
cated as morpheme segmentation requiring a sep-
arate model to be trained in advance (Creutz and
Lagus, 2005; Huang and Zhao, 2007). Further-
more, these segmentation2 steps are often tuned
or designed separately from the ultimate objective
of translation quality, potentially contributing to a
suboptimal quality.

Based on this observation and analysis, in this
paper, we ask ourselves and the readers a question

2From here on, the term segmentation broadly refers to
any method that splits a given character sequence into a se-
quence of subword symbols.

which should have been asked much earlier: Is it
possible to do character-level translation without
any explicit segmentation?

3.2 Why Word-Level Translation?
(1) Word as a Basic Unit of Meaning A word
can be understood in two different senses. In the
abstract sense, a word is a basic unit of mean-
ing (lexeme), and in the other sense, can be un-
derstood as a “concrete word as used in a sen-
tence.” (Booij, 2012). A word in the former sense
turns into that in the latter sense via a process
of morphology, including inﬂection, compound-
ing and derivation. These three processes do al-
ter the meaning of the lexeme, but often it stays
close to the original meaning. Because of this
view of words as basic units of meaning (either
in the form of lexemes or derived form) from lin-
guistics, much of previous work in natural lan-
guage processing has focused on using words as
basic units of which a sentence is encoded as a
sequence. Also, the potential difﬁculty in ﬁnding
a mapping between a word’s character sequence
and meaning3 has likely contributed to this trend
toward word-level modelling.

(2) Data Sparsity There is a further technical
reason why much of previous research on ma-
chine translation has considered words as a ba-
sic unit. This is mainly due to the fact that ma-
jor components in the existing translation systems,
such as language models and phrase tables, are a
count-based estimator of probabilities.
In other
words, a probability of a subsequence of sym-
bols, or pairs of symbols, is estimated by counting
the number of its occurrences in a training cor-
pus. This approach severely suffers from the is-
sue of data sparsity, which is due to a large state
space which grows exponentially w.r.t. the length
of subsequences while growing only linearly w.r.t.
the vocabulary size. This poses a great challenge
to character-level modelling, as any subsequence
will be on average 4–5 times longer when charac-
ters, instead of words, are used. Indeed, Vilar et
al. (2007) reported worse performance when the
character sequence was directly used by a phrase-
based machine translation system.

(3) Vanishing Gradient Speciﬁcally to neural
machine translation, a major reason behind the

3For instance, “quit”, “quite” and “quiet” are one edit-

distance away from each other but have distinct meanings.

wide adoption of word-level modelling is due to
the difﬁculty in modelling long-term dependen-
cies with recurrent neural networks (Bengio et al.,
1994; Hochreiter, 1998). As the lengths of the
sentences on both sides grow when they are repre-
sented in characters, it is easy to believe that there
will be more long-term dependencies that must be
captured by the recurrent neural network for suc-
cessful translation.

3.3 Why Character-Level Translation?

Why not Word-Level Translation? The most
pressing issue with word-level processing is that
we do not have a perfect word segmentation al-
gorithm for any one language. A perfect segmen-
tation algorithm needs to be able to segment any
given sentence into a sequence of lexemes and
morphemes. This problem is however a difﬁcult
problem on its own and often requires decades of
research (see, e.g., Creutz and Lagus (2005) for
Finnish and other morphologically rich languages
and Huang and Zhao (2007) for Chinese). There-
fore, many opt to using either a rule-based tok-
enization approach or a suboptimal, but still avail-
able, learning based segmentation algorithm.

The outcome of this naive, sub-optimal segmen-
tation is that the vocabulary is often ﬁlled with
many similar words that share a lexeme but have
different morphology. For instance, if we apply
a simple tokenization script to an English corpus,
“run”, “runs”, “ran” and “running” are all sepa-
rate entries in the vocabulary, while they clearly
share the same lexeme “run”. This prevents any
machine translation system, in particular neural
machine translation, from modelling these mor-
phological variants efﬁciently. More speciﬁcally
in the case of neural machine translation, each of
these morphological variants–“run”, “runs”, “ran”
and “running”– will be assigned a d-dimensional
word vector, leading to four independent vectors,
while it is clear that if we can segment those vari-
ants into a lexeme and other morphemes, we can
model them more efﬁciently. For instance, we can
have a d-dimensional vector for the lexeme “run”
and much smaller vectors for “s” and“ing”. Each
of those variants will be then a composite of the
lexeme vector (shared across these variants) and
morpheme vectors (shared across words sharing
the same sufﬁx, for example) (Botha and Blun-
som, 2014). This makes use of distributed rep-
resentation, which generally yields better general-

ization, but seems to require an optimal segmen-
tation, which is unfortunately almost never avail-
able.

In addition to inefﬁciency in modelling, there
are two additional negative consequences from us-
ing (unsegmented) words. First, the translation
system cannot generalize well to novel words,
which are often mapped to a token reserved for
an unknown word. This effectively ignores any
meaning or structure of the word to be incorpo-
rated when translating. Second, even when a lex-
eme is common and frequently observed in the
training corpus, its morphological variant may not
be. This implies that the model sees this speciﬁc,
rare morphological variant much less and will not
be able to translate it well. However, if this rare
morphological variant shares a large part of its
spelling with other more common words, it is de-
sirable for a machine translation system to exploit
those common words when translating those rare
variants.

Why Character-Level Translation? All of
these issues can be addressed to certain extent by
directly modelling characters. Although the is-
sue of data sparsity arises in character-level trans-
lation, it is elegantly addressed by using a para-
metric approach based on recurrent neural net-
works instead of a non-parametric count-based ap-
proach. Furthermore, in recent years, we have
learned how to build and train a recurrent neu-
ral network that can well capture long-term de-
pendencies by using more sophisticated activa-
tion functions, such as long short-term memory
units (Hochreiter and Schmidhuber, 1997) and
gated recurrent units (Cho et al., 2014).

Kim et al. (2015) and Ling et al. (2015a) re-
cently showed that by having a neural network that
converts a character sequence into a word vector,
we avoid the issues from having many morpho-
logical variants appearing as separate entities in
a vocabulary. This is made possible by sharing
the character-to-word neural network across all the
unique tokens. A similar approach was applied to
machine translation by Ling et al. (2015b).

These recent approaches, however, still rely on
the availability of a good, if not optimal, segmen-
tation algorithm. Ling et al. (2015b) indeed states
that “[m]uch of the prior information regarding
morphology, cognates and rare word translation
among others, should be incorporated.”

It however becomes unnecessary to consider

these prior information, if we use a neural net-
work, be it recurrent, convolution or their combi-
nation, directly on the unsegmented character se-
quence. The possibility of using a sequence of un-
segmented characters has been studied over many
years in the ﬁeld of deep learning. For instance,
Mikolov et al. (2012) and Sutskever et al. (2011)
trained a recurrent neural network language model
(RNN-LM) on character sequences. The latter
showed that it is possible to generate sensible text
sequences by simply sampling a character at a
time from this model. More recently, Zhang et
al. (2015) and Xiao and Cho (2016) successfully
applied a convolutional net and a convolutional-
recurrent net respectively to character-level docu-
ment classiﬁcation without any explicit segmen-
tation. These previous works suggest the possi-
bility of applying neural networks for the task of
machine translation, which is often considered a
substantially more difﬁcult problem compared to
document classiﬁcation and language modelling.

3.4 Challenges and Questions
There are two overlapping sets of challenges for
the source and target sides. On the source side, it
is unclear how to build a neural network that learns
a highly nonlinear mapping from a spelling to the
meaning of a sentence.

On the target side, there are two challenges. The
ﬁrst challenge is the same one from the source
side, as the decoder neural network needs to sum-
marize what has been translated.
In addition to
this, the character-level modelling on the target
side is more challenging, as the decoder network
must be able to generate a long, coherent sequence
of characters. This is a great challenge, as the size
of the state space grows exponentially w.r.t.
the
number of symbols, and in the case of characters,
it is often 300-1000 symbols long.

All these challenges should ﬁrst be framed as
questions; whether the current recurrent neural
networks, which are already widely used in neu-
ral machine translation, are able to address these
challenges as they are. In this paper, we aim at an-
swering these questions empirically and focus on
the challenges on the target side (as the target side
shows both of the challenges).

4 Character-Level Translation

In this paper, we try to answer the questions posed
earlier by testing two different types of recurrent

(a) Gating units

(b) One-step processing

Figure 1: Bi-scale recurrent neural network

neural networks on the target side (decoder).

First, we test an existing recurrent neural net-
work with gated recurrent units (GRU). We call
this decoder a base decoder.

Second, we build a novel two-layer recurrent
neural network, inspired by the gated-feedback
network from Chung et al. (2015), called a bi-
scale recurrent neural network. We design this
network to facilitate capturing two timescales, mo-
tivated by the fact that characters and words may
work at two separate timescales.

We choose to test these two alternatives for the
following purposes. Experiments with the base
decoder will clearly answer whether the existing
neural network is enough to handle character-level
decoding, which has not been properly answered
in the context of machine translation. The alterna-
tive, the bi-scale decoder, is tested in order to see
whether it is possible to design a better decoder, if
the answer to the ﬁrst question is positive.

4.1 Bi-Scale Recurrent Neural Network
In this proposed bi-scale recurrent neural network,
there are two sets of hidden units, h1 and h2. They
contain the same number of units, i.e., dim(h1) =
dim(h2). The ﬁrst set h1 models a fast-changing
timescale (thereby, a faster layer), and h2 a slower
timescale (thereby, a slower layer). For each hid-
den unit, there is an associated gating unit, to
which we refer by g1 and g2. For the descrip-
tion below, we use yt(cid:48)−1 and ct(cid:48) for the previous
target symbol and the context vector (see Eq. (2)),
respectively.

Let us start with the faster layer. The faster layer
outputs two sets of activations, a normal output h1
t(cid:48)
and its gated version ˇh1
t(cid:48). The activation of the
faster layer is computed by

(cid:105)(cid:17)

(cid:16)

Wh1(cid:104)

h1

t(cid:48) = tanh

ey(yt(cid:48)−1); ˇh1

t(cid:48)−1; ˆh2

t(cid:48)−1; ct(cid:48)

,

where ˇh1

t(cid:48)−1 and ˆh2

t(cid:48)−1 are the gated activations of

the faster and slower layers respectively. These
gated activations are computed by

t(cid:48) = (1 − g1
ˇh1

t(cid:48)) (cid:12) h1

t(cid:48), ˆh2

t(cid:48) = g1

t(cid:48) (cid:12) h2
t(cid:48).

In other words, the faster layer’s activation is
based on the adaptive combination of the faster
and slower layers’ activations from the previous
time step. Whenever the faster layer determines
t(cid:48)−1 ≈ 1, the next
that it needs to reset, i.e., g1
activation will be determined based more on the
slower layer’s activation.
(cid:105)(cid:17)

The faster layer’s gating unit is computed by

(cid:16)

Wg1(cid:104)

ey(yt(cid:48)−1); ˇh1

t(cid:48)−1; ˆh2

t(cid:48)−1; ct(cid:48)

,

g1
t(cid:48) = σ

where σ is a sigmoid function.

The slower layer also outputs two sets of acti-
t(cid:48) and its gated version

vations, a normal output h2
ˇh2
t(cid:48). These activations are computed as follows:

t(cid:48) (cid:12) ˜h2
t(cid:48),

t(cid:48)) (cid:12) h2
t(cid:48)−1 + g1
t(cid:48)) (cid:12) h2
t(cid:48),

t(cid:48) = (1 − g1
h2
t(cid:48) = (1 − g2
ˇh2
t(cid:48) is a candidate activation. The slower
(cid:16)

t(cid:48) is computed by

Wg2(cid:2)(g1

t(cid:48)−1; ct(cid:48)(cid:3)(cid:17)

t(cid:48) (cid:12) h1

t(cid:48)); ˇh2

.

g2
t(cid:48) =σ

where ˜h2
layer’s gating unit g2

This adaptive leaky integration based on the gat-
ing unit from the faster layer has a consequence
that the slower layer updates its activation only
when the faster layer resets. This puts a soft con-
straint that the faster layer runs at a faster rate by
preventing the slower layer from updating while
the faster layer is processing a current chunk.

The candidate activation is then computed by

t(cid:48)−1; ct(cid:48)(cid:3)(cid:17)

. (5)

(cid:16)

Wh2(cid:2)(g1

˜h2

t(cid:48) = tanh

t(cid:48) (cid:12) h1

t(cid:48)); ˇh2

t(cid:48) (cid:12)h1

According to g1

ˇh2
t(cid:48)−1 indicates the reset activation from the pre-
vious time step, similarly to what happened in the
faster layer, and ct(cid:48) is the input from the context.
t(cid:48) in Eq. (5), the faster layer
inﬂuences the slower layer, only when the faster
layer has ﬁnished processing the current chunk
and is about to reset itself (g1
In other
words, the slower layer does not receive any in-
put from the faster layer, until the faster layer has
quickly processed the current chunk, thereby run-
ning at a slower rate than the faster layer does.

t(cid:48) ≈ 1).

At each time step, the ﬁnal output of the pro-
posed bi-scale recurrent neural network is the con-
catenation of the output vectors of the faster and

Figure 2:
(left) The BLEU scores on En-Cs
w.r.t. the length of source sentences. (right) The
difference of word negative log-probabilities be-
tween the subword-level decoder and either of the
character-level base or bi-scale decoder.

slower layers, i.e., (cid:2)h1; h2(cid:3). This concatenated

vector is used to compute the probability distribu-
tion over all the symbols in the vocabulary, as in
Eq. (4). See Fig. 1 for graphical illustration.

5 Experiment Settings

For evaluation, we represent a source sentence as
a sequence of subword symbols extracted by byte-
pair encoding (BPE, Sennrich et al. (2015)) and a
target sentence either as a sequence of BPE-based
symbols or as a sequence of characters.
Corpora and Preprocessing We use all avail-
able parallel corpora for four language pairs from
WMT’15: En-Cs, En-De, En-Ru and En-Fi. They
consist of 12.1M, 4.5M, 2.3M and 2M sentence
pairs, respectively. We tokenize each corpus using
a tokenization script included in Moses.4 We only
use the sentence pairs, when the source side is up
to 50 subword symbols long and the target side is
either up to 100 subword symbols or 500 charac-
ters. We do not use any monolingual corpus.

For all

the pairs other than En-Fi, we use
newstest-2013 as a development set, and newstest-
2014 (Test1) and newstest-2015 (Test2) as test sets.
For En-Fi, we use newsdev-2015 and newstest-
2015 as development and test sets, respectively.
Models and Training We test three models set-
tings: (1) BPE→BPE, (2) BPE→Char (base) and
(3) BPE→Char (bi-scale). The latter two differ by
the type of recurrent neural network we use. We
use gated recurrent units (GRU) for the encoder in
all the settings. We used GRU for the decoders in
the ﬁrst two settings, (1) and (2), while the pro-
posed bi-scale recurrent network was used in the

4Although tokenization is not necessary for character-
level modelling, we tokenize the all target side corpora to
make comparison against word-level modelling easier.

c
r
S

Trgt

BPE

E
P
B

Char

E
P
B

BPE

Char

E
P
B

BPE

Char

E
P
B

BPE

Char

(a)
(b)
(c)
(d)
(e)
(f)
(g)

(h)
(i)
(j)

(k)
(l)
(m)

(n)
(o)
(p)

e
D
-
n
E

s
C
-
n
E

u
R
-
n
E

i
F
-
n
E

Depth

Attention
h1
h2

D

D D

D

D D

D

Model

Base

Base

Development
Single
20.78

Ens
–

21.1721.45
20.62
21.4521.88
20.88

20.31

23.25
22.80

–

Test1

Single
19.98

20.2820.76
19.30
21.3121.46
19.82

19.70

Ens
–

22.59
22.60

–

Test2

Single
21.72

21.8122.21
21.35
23.4523.45
21.72

21.30

Ens
–

24.18
24.76

–

D D

Bi-S

D

D D Base
D Base
D Bi-S

D D Base
D Base
D Bi-S

D D Base
D Base
D Bi-S

21.2421.41
21.13

22.79

20.9621.33
20.62

22.76

23.0323.44
22.85

25.01

20.78
20.08

–
16.1216.96
15.96
17.6817.78
17.39
17.6217.93
17.43
–
18.5618.70
18.26
18.5618.87
18.39
18.3018.54
17.88
–
9.6110.02
9.24
11.1911.55
11.09
10.7311.04
10.40
–

–
–

19.21
19.52
19.83

21.17
20.53
20.53

11.92
13.72
13.39

20.19
19.39

20.60(1)

17.1617.68
16.38
19.2519.55
18.89
19.2719.53
19.15
21.00(3)

25.3025.40
24.95
26.0026.07
25.04
25.5925.76
24.57
28.70(5)
–
–
–

–

–
–

20.79
21.95
22.15

29.26
29.37
29.26

–
–
–

22.26
20.94

24.00(2)

14.6315.09
14.26
16.9817.17
16.81
16.8617.10
16.68
18.20(4)

19.7220.29
19.02
21.1021.24
20.14
20.7321.02
19.97
24.30(6)

8.979.17
8.88

10.9311.56
10.11
10.2410.63
9.71
12.70(7)

–
–

17.61
18.92
18.93

22.96
23.51
23.75

11.73
13.48
13.32

1
2
2
2
2
2
2

2
2
2

2
2
2

2
2
2

State-of-the-art Non-Neural Approach∗

State-of-the-art Non-Neural Approach∗

State-of-the-art Non-Neural Approach∗

State-of-the-art Non-Neural Approach∗

Table 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders
for both single models and ensembles. The best scores among the single models per language pair
are bold-faced, and those among the ensembles are underlined. When available, we report the median
value, and the minimum and maximum values as a subscript and a superscript, respectively. (∗) http:
//matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015).
(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015).

last setting, (3). The encoder has 512 hidden units
for each direction (forward and reverse), and the
decoder has 1024 hidden units per layer.

We train each model using stochastic gradient
descent with Adam (Kingma and Ba, 2014). Each
update is computed using a minibatch of 128 sen-
tence pairs. The norm of the gradient is clipped
with a threshold 1 (Pascanu et al., 2013).

Decoding and Evaluation We use beamsearch
to approximately ﬁnd the most likely translation
given a source sentence. The beam widths are
5 and 15 respectively for the subword-level and
character-level decoders. They were chosen based
on the translation quality on the development set.
The translations are evaluated using BLEU.5

Multilayer Decoder and Soft-Alignment Mech-
anism When the decoder is a multilayer re-
current neural network (including a stacked net-
work as well as the proposed bi-scale network),
the decoder outputs multiple hidden vectors–

(cid:8)h1, . . . , hL(cid:9) for L layers, at a time. This allows

5We used the multi-bleu.perl script from Moses.

an extra degree of freedom in the soft-alignment
mechanism (fscore in Eq. (3)). We evaluate using
alternatives, including (1) using only hL (slower
layer) and (2) using all of them (concatenated).

Ensembles We also evaluate an ensemble of
neural machine translation models and compare
its performance against the state-of-the-art phrase-
based translation systems on all four language
pairs. We decode from an ensemble by taking the
average of the output probabilities at each step.

6 Quantitative Analysis
Slower Layer for Alignment On En-De, we
test which layer of the decoder should be used
for computing soft-alignments.
In the case of
subword-level decoder, we observed no difference
between choosing any of the two layers of the de-
coder against using the concatenation of all the
layers (Table 1 (a–b)) On the other hand, with the
character-level decoder, we noticed an improve-
ment when only the slower layer (h2) was used
for the soft-alignment mechanism (Table 1 (c–g)).
This suggests that the soft-alignment mechanism

Figure 3: Alignment matrix of a test example from En-De using the BPE→Char (bi-scale) model.

beneﬁts by aligning a larger chunk in the target
with a subword unit in the source, and we use only
the slower layer for all the other language pairs.

Single Models
In Table 1, we present a com-
prehensive report of the translation qualities of
(1) subword-level decoder, (2) character-level base
decoder and (3) character-level bi-scale decoder,
for all the language pairs. We see that the both
types of character-level decoder outperform the
subword-level decoder for En-Cs and En-Fi quite
signiﬁcantly. On En-De, the character-level base
decoder outperforms both the subword-level de-
coder and the character-level bi-scale decoder,
validating the effectiveness of the character-level
modelling. On En-Ru, among the single mod-
els, the character-level decoders outperform the
subword-level decoder, but in general, we observe
that all the three alternatives work comparable to
each other.

These results clearly suggest that it is indeed
possible to do character-level translation without
explicit segmentation. In fact, what we observed is
that character-level translation often surpasses the
translation quality of word-level translation. Of
course, we note once again that our experiment is
restricted to using an unsegmented character se-
quence at the decoder only, and a further explo-
ration toward replacing the source sentence with
an unsegmented character sequence is needed.

Ensembles Each ensemble was built using four
(En-De) and eight (En-{Cs,Ru,Fi}) independent
models. The ﬁrst observation we make is that
in all the language pairs, neural machine transla-
tion performs comparably to, or often better than,
the state-of-the-art non-neural translation system.
Furthermore, the character-level decoders outper-
form the subword-level decoder in all the cases.

7 Qualitative Analysis
(1) Can the character-level decoder generate
a long, coherent sentence? The translation in
characters is dramatically longer than that
in
words, likely making it more difﬁcult for a recur-
rent neural network to generate a coherent sen-
tence in characters. This belief turned out to be
false. As shown in Fig. 2 (left), there is no sig-
niﬁcant difference between the subword-level and
character-level decoders, even though the lengths
of the generated translations are generally 5–10
times longer in characters.

(2) Does the character-level decoder help with
rare words? One advantage of character-level
modelling is that it can model the composition of
any character sequence, thereby better modelling
rare morphological variants. We empirically con-
ﬁrm this by observing the growing gap in the aver-
age negative log-probability of words between the
subword-level and character-level decoders as the
frequency of the words decreases. This is shown
in Fig. 2 (right) and explains one potential cause
behind the success of character-level decoding in
our experiments (we deﬁne diﬀ(x, y) = x − y).
(3) Can the character-level decoder soft-align
between a source word and a target charac-
ter? In Fig. 3 (left), we show an example soft-
alignment of a source sentence, “Two sets of light
so close to one another.”
It is clear that the
character-level translation model well captured the
alignment between the source subwords and tar-
get characters. We observe that the character-
level decoder correctly aligns to “lights” and “sets
of” when generating a German compound word
“Lichtersets” (see Fig. 3 (right) for the zoomed-
in version). This type of behaviour happens simi-
larly between “one another” and “einander”. Of
course, this does not mean that there exists an
alignment between a source word and a target
character. Rather, this suggests that the internal

state of the character-level decoder, the base or bi-
scale, well captures the meaningful chunk of char-
acters, allowing the model to map it to a larger
chunk (subword) in the source.

8 Conclusion

In this paper, we addressed a fundamental ques-
tion on whether a recently proposed neural ma-
chine translation system can directly handle trans-
lation at the level of characters without any word
segmentation. We focused on the target side, in
which a decoder was asked to generate one char-
acter at a time, while soft-aligning between a tar-
get character and a source subword. Our extensive
experiments, on four language pairs–En-Cs, En-
De, En-Ru and En-Fi– strongly suggest that it is
indeed possible for neural machine translation to
translate at the level of characters, and that it actu-
ally beneﬁts from doing so.

Our result has one limitation that we used sub-
word symbols in the source side. However, this
has allowed us a more ﬁne-grained analysis, but in
the future, a setting where the source side is also
represented as a character sequence must be inves-
tigated.

Acknowledgments

The authors would like to thank the developers
of Theano (Bastien et al., 2012). We acknowl-
edge the support of the following agencies for re-
search funding and computing support: NSERC,
Calcul Qu´ebec, Compute Canada, the Canada Re-
search Chairs, CIFAR and Samsung. KC thanks
the support by Facebook and Google (Google Fac-
ulty Award 2016).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the International Conference on Learning Represen-
tations (ICLR).

Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. arXiv preprint arXiv:1211.5590.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difﬁcult. IEEE Transactions on Neu-
ral Networks, 5(2):157–166.

Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent.
2001. A neural probabilistic language model. In Ad-
vances in Neural Information Processing Systems,
pages 932–938.

Geert Booij. 2012. The grammar of words: An intro-
duction to linguistic morphology. Oxford University
Press.

Jan A Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In ICML 2014.

Rohan Chitnis and John DeNero. 2015. Variable-
length word encodings for neural translation models.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2088–2093.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio.
2014. Learning phrase representations
using RNN encoder-decoder for statistical machine
the Empiricial
translation.
Methods in Natural Language Processing (EMNLP
2014), October.

In Proceedings of

Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,
and Yoshua Bengio. 2015. Gated feedback recur-
In Proceedings of the 32nd
rent neural networks.
International Conference on Machine Learning.

Marta R Costa-Juss`a and Jos´e AR Fonollosa. 2016.
Character-based neural machine translation. arXiv
preprint arXiv:1603.00810.

Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Helsinki
University of Technology.

Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heaﬁeld. 2014. Edinburgh’s phrase-based
In Pro-
machine translation systems for wmt-14.
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
pages 97–104.

Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, et al. 2014. Eu-bridge mt: Combined ma-
chine translation.

Barry Haddow, Matthias Huck, Alexandra Birch, Niko-
lay Bogoychev, and Philipp Koehn. 2015. The edin-
burgh/jhu phrase-based machine translation systems
In Proceedings of the Tenth Work-
for wmt 2015.
shop on Statistical Machine Translation, pages 126–
133.

Sepp Hochreiter and J¨urgen Schmidhuber.

1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Sepp Hochreiter.

1998.

The vanishing gradient
problem during learning recurrent neural nets and
International Journal of Un-
problem solutions.
certainty, Fuzziness and Knowledge-Based Systems,
6(02):107–116.

Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3):8–20.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2.

Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,
2013. How to construct
arXiv preprint

and Yoshua Bengio.
deep recurrent neural networks.
arXiv:1312.6026.

Raphael Rubino, Tommi Pirinen, Miquel Espla-Gomis,
N Ljubeˇsic, Sergio Ortiz Rojas, Vassilis Papavassil-
iou, Prokopis Prokopidis, and Antonio Toral. 2015.
Abu-matran at wmt 2015 translation task: Morpho-
logical segmentation and web crawling. In Proceed-
ings of the Tenth Workshop on Statistical Machine
Translation, pages 184–191.

Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492–
518.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2015. Character-aware neural lan-
guage models. arXiv preprint arXiv:1508.06615.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Rupesh K Srivastava, Klaus Greff, and J¨urgen Schmid-
huber. 2015. Training very deep networks. In Ad-
vances in Neural Information Processing Systems,
pages 2368–2376.

Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML’11), pages
1017–1024.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.

David Vilar, Jan-T Peter, and Hermann Ney. 2007.
In Proceedings of the
Can we translate letters?
Second Workshop on Statistical Machine Transla-
tion, pages 33–39. Association for Computational
Linguistics.

Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, and Philipp Koehn. 2015. Edin-
burgh’s syntax-based systems at wmt 2015. In Pro-
ceedings of the Tenth Workshop on Statistical Ma-
chine Translation, pages 199–209.

Yijun Xiao and Kyunghyun Cho.

2016. Efﬁcient
character-level document classiﬁcation by combin-
ing convolution and recurrent layers. arXiv preprint
arXiv:1602.00367.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in Neural Information Pro-
cessing Systems, pages 649–657.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Hyoung-Gyu Lee, JaeSong Lee, Jun-Seok Kim, and
Chang-Ki Lee. 2015. Naver machine translation
In Proceedings of the 2nd
system for wat 2015.
Workshop on Asian Translation (WAT2015), pages
69–73.

Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso.
Finding
function in form: Compositional character models
arXiv
for open vocabulary word representation.
preprint arXiv:1508.02096.

2015a.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
2015b. Character-based neural machine

Black.
translation. arXiv preprint arXiv:1511.04586.

Thang Luong, Richard Socher, and Christopher D
Better word representations
Manning.
with recursive neural networks for morphology. In
CoNLL, pages 104–113.

2013.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015a. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2015b. Address-
ing the rare word problem in neural machine trans-
lation. arXiv preprint arXiv:1410.8206.

Tomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, volume 2, page 3.

Tomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and J Cernocky. 2012.
Subword language modeling with neural networks.
Preprint.

