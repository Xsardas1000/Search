Identity Mappings in Deep Residual Networks

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun

Microsoft Research

Abstract Deep residual networks [1] have emerged as a family of ex-
tremely deep architectures showing compelling accuracy and nice con-
vergence behaviors. In this paper, we analyze the propagation formula-
tions behind the residual building blocks, which suggest that the forward
and backward signals can be directly propagated from one block to any
other block, when using identity mappings as the skip connections and
after-addition activation. A series of ablation experiments support the
importance of these identity mappings. This motivates us to propose a
new residual unit, which further makes training easy and improves gen-
eralization. We report improved results using a 1001-layer ResNet on
CIFAR-10/100, and a 200-layer ResNet on ImageNet.

1 Introduction

Deep residual networks (ResNets) [1] consist of many stacked “Residual Units”.
Each unit (Fig. 1 (a)) can be expressed in a general form:

6
1
0
2

 
r
a

 

M
6
1

 
 
]

V
C
.
s
c
[
 
 

1
v
7
2
0
5
0

.

3
0
6
1
:
v
i
X
r
a

yl = h(xl) + F(xl,Wl),

xl+1 = f (yl),

where xl and xl+1 are input and output of the l-th unit, and F is a residual
function. In [1], h(xl) = xl is an identity mapping and f is a ReLU [2] function.
ResNets that are over 100-layer deep have shown state-of-the-art accuracy for
several challenging recognition tasks on ImageNet [3] and MS COCO [4] compe-
titions. The central idea of ResNets is to learn the additive residual function F
with respect to h(xl), with a key choice of using an identity mapping h(xl) = xl.
This is realized by attaching an identity skip connection (“shortcut”).

In this paper, we analyze deep residual networks by focusing on creating a
“direct” path for propagating information — not only within a residual unit,
but through the entire network. Our derivations reveal that if both h(xl) and
f (yl) are identity mappings, the signal could be directly propagated from one
unit to any other units, in both forward and backward passes. Our experiments
empirically show that training in general becomes easier when the architecture
is closer to the above two conditions.

To understand the role of skip connections, we analyze and compare various
types of h(xl). We ﬁnd that the identity mapping h(xl) = xl chosen in [1]
achieves the fastest error reduction and lowest training loss among all variants

2

Figure 1. Left: (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey
arrows indicate the easiest paths for the information to propagate, corresponding to
the additive term “xl” in Eqn.(4) (forward propagation) and the additive term “1” in
Eqn.(5) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer
ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote
training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.

we investigated, whereas skip connections of scaling, gating [5,6,7], and 1×1
convolutions all lead to higher training loss and error. These experiments suggest
that keeping a “clean” information path (indicated by the grey arrows in Fig. 1, 2,
and 4) is helpful for easing optimization.

To construct an identity mapping f (yl) = yl, we view the activation func-
tions (ReLU and BN [8]) as “pre-activation” of the weight layers, in contrast
to conventional wisdom of “post-activation”. This point of view leads to a new
residual unit design, shown in (Fig. 1(b)). Based on this unit, we present com-
petitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier
to train and generalizes better than the original ResNet in [1]. We further report
improved results on ImageNet using a 200-layer ResNet, for which the counter-
part of [1] starts to overﬁt. These results suggest that there is much room to
exploit the dimension of network depth, a key to the success of modern deep
learning.

2 Analysis of Deep Residual Networks

The ResNets developed in [1] are modularized architectures that stack building
blocks of the same connecting shape. In this paper we call these blocks “Residual
Units”. The original Residual Unit in [1] performs the following computations:

yl = h(xl) + F(xl,Wl),

xl+1 = f (yl).

(1)

(2)

0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training LossResNet−1001, original (error: 7.61%)ResNet−1001, proposed (error: 4.92%)BNReLUweightBNweightadditionReLUxlxl+1(a) originalReLUweightBNReLUweightBNadditionxlxl+1(b) proposed3

Here xl is the input feature to the l-th Residual Unit. Wl = {Wl,k|1≤k≤K} is a
set of weights (and biases) associated with the l-th Residual Unit, and K is the
number of layers in a Residual Unit (K is 2 or 3 in [1]). F denotes the residual
function, e.g., a stack of two 3×3 convolutional layers in [1]. The function f is
the operation after element-wise addition, and in [1] f is ReLU. The function h
is set as an identity mapping: h(xl) = xl.
If f is also an identity mapping: xl+1 ≡ yl, we can put Eqn.(2) into Eqn.(1)

and obtain:

(3)
Recursively (xl+2 = xl+1 + F (xl+1, Wl+1) = xl + F (xl, Wl) + F (xl+1, Wl+1), etc.) we
will have:

xl+1 = xl + F(xl,Wl).

xL = xl +

F(xi,Wi),

(4)

for any deeper unit L and any shallower unit l. Eqn.(4) exhibits some nice
properties. (i) The feature xL of any deeper unit L can be represented as the
i=l F,
indicating that the model is in a residual fashion between any units L and l. (ii)
i=0 F(xi,Wi), of any deep unit L, is the summation
of the outputs of all preceding residual functions (plus x0). This is in contrast to
a “plain network” where a feature xL is a series of matrix-vector products, say,

feature xl of any shallower unit l plus a residual function in a form of(cid:80)L−1
The feature xL = x0 +(cid:80)L−1
(cid:81)L−1

i=0 Wix0.
Eqn.(4) also leads to nice backward propagation properties. Denoting the

loss function as E, from the chain rule of backpropagation [9] we have:

L−1(cid:88)

i=l

(cid:32)

∂E
∂xl

∂E
∂xL

∂xL
∂xl

=

∂E
∂xL

=

1 +

∂
∂xl

F(xi,Wi)

.

(5)

(cid:33)

L−1(cid:88)

i=l

∂xl

∂E
∂xL

Eqn.(5) indicates that the gradient ∂E
can be decomposed into two additive
(cid:16) ∂
terms: a term of
that propagates information directly without concern-
that propagates
ing any weight layers, and another term of
through the weight layers. The additive term of ∂E
ensures that information is
directly propagated back to any shallower unit l. Eqn.(5) also suggests that it
(cid:80)L−1
is unlikely for the gradient ∂E
to be canceled out, because in general the term
i=l F cannot be always -1 for all xl. This implies that the gradient of a

∂
∂xl
layer does not vanish even when the weights are arbitrarily small.

i=l F(cid:17)
(cid:80)L−1

∂E
∂xL

∂xL

∂xl

∂xl

Discussions

Eqn.(4) and Eqn.(5) suggest that the signal can be directly propagated from any
unit to one another, both forwardly and backwardly. The foundation of Eqn.(4)
is two identity mappings: (i) the identity skip connection h(xl) = xl, and (ii)
the condition that f is an identity mapping.

4

These directly propagated information ﬂows are represented by the grey ar-
rows in Fig. 1, 2, and 4. And the above two conditions are true when these grey
arrows cover no operations (expect addition) and thus are “clean”. In the fol-
lowing two sections we separately investigate the impacts of the two conditions.
It is noteworthy that there are Residual Units for increasing dimensions and
reducing feature map sizes [1], which do not strictly follow the above formula-
tions. But as there are only a very few such units (two on CIFAR and three on
ImageNet, depending on image sizes [1]), we expect that they aﬀect little on de-
grading propagation. On the other hand, one may think of the above derivations
as applied to all Residual Units within the same feature map size.

3 On the Importance of Identity Skip Connections

Let’s consider a simple modiﬁcation, h(xl) = λlxl, to break the identity shortcut:

xl+1 = λlxl + F(xl,Wl),

(6)

where λl is a modulating scalar (for simplicity we still assume f is identity).
Recursively applying this formulation we obtain an equation similar to Eqn. (4):

xL =(cid:81)L−1

i=l λixl +(cid:80)L−1

i=l

(cid:81)L−1
j=i+1 λjF(xi,Wi), or simply:
L−1(cid:89)

L−1(cid:88)

ˆF(xi,Wi),

λixl +

xL =

i=l

i=l

(7)

where the notation ˆF absorbs the scalars into the residual functions. Similar to
Eqn.(5), we have backpropagation of the following form:

(cid:32)L−1(cid:89)

i=l

∂E
∂xl

∂E
∂xL

=

L−1(cid:88)

i=l

(cid:33)

λi +

∂
∂xl

ˆF(xi,Wi)

.

(8)

(cid:81)L−1

Unlike Eqn.(5), in Eqn.(8) the ﬁrst additive term is modulated by a factor
i=l λi. For an extremely deep network (L is large), if λi > 1 for all i, this
factor can be exponentially large; if λi < 1 for all i, this factor can be expo-
nentially small and vanish, which blocks the backpropagated signal from the
shortcut and forces it to ﬂow through the weight layers. This results in opti-
mization diﬃculties as we show by experiments.

the ﬁrst term becomes (cid:81)L−1

In the above analysis, the original identity skip connection in Eqn.(3) is re-
placed with a simple scaling h(xl) = λlxl. If the skip connection h(xl) represents
more complicated transforms (such as gating and 1×1 convolutions), in Eqn.(8)
i where h(cid:48) is the derivative of h. This product
may also impede information propagation and hamper the training procedure
as witnessed in the following experiments.

i=l h(cid:48)

5

Figure 2. Various types of shortcut connections used in Table 1. The grey arrows
indicate the easiest paths for the information to propagate. The shortcut connections
in (b-f) are impeded by diﬀerent components. For simplifying illustrations we do not
display the BN layers, which are adopted right after the weight layers for all units here.

3.1 Experiments on Skip Connections

We experiment with the 110-layer ResNet as presented in [1] on CIFAR-10 [10].
This extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of
3×3 convolutional layers) and is challenging for optimization. Our implementa-
tion details (see appendix) are the same as [1]. Throughout this paper we report
the median accuracy of 5 runs for each architecture on CIFAR, reducing the
impacts of random variations.

Though our above analysis is driven by identity f , the experiments in this
section are all based on f = ReLU as in [1]; we address identity f in the next sec-
tion. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons
of other variants (Fig. 2 and Table 1) are summarized as follows:

Constant scaling. We set λ = 0.5 for all shortcuts (Fig. 2(b)). We further
study two cases of scaling F: (i) F is not scaled; or (ii) F is scaled by a constant
scalar of 1− λ = 0.5, which is similar to the highway gating [6,7] but with frozen
gates. The former case does not converge well; the latter is able to converge,
but the test error (Table 1, 12.35%) is substantially higher than the original
ResNet-110. Fig 3(a) shows that the training error is higher than that of the
original ResNet-110, suggesting that the optimization has diﬃculties when the
shortcut signal is scaled down.

(f) dropout shortcut(e) conv shortcut3x3 conv3x3 convadditionReLU1x1 convReLU3x3 conv3x3 convadditiondropoutReLUReLU(d) shortcut-only gating(c) exclusive gating3x3 conv3x3 convaddition1x1 convsigmoid1-ReLUReLU3x3 conv3x3 convaddition1x1 convsigmoid1-ReLUReLU(a) original(b) constant scaling3x3 conv3x3 convadditionReLUReLU3x3 conv3x3 convaddition0.50.5ReLUReLU6

Table 1. Classiﬁcation error on the CIFAR-10 test set using ResNet-110 [1], with
diﬀerent types of shortcut connections applied to all Residual Units. We report “fail”
when the test error is higher than 20%.

case

Fig.

on shortcut

original [1]

Fig. 2(a)

constant
scaling

exclusive

gating

Fig. 2(b)

Fig. 2(c)

shortcut-only

gating

1×1 conv shortcut
dropout shortcut

Fig. 2(d)

Fig. 2(e)

Fig. 2(f)

1

0

0.5

0.5

1 − g(x)
1 − g(x)
1 − g(x)
1 − g(x)
1 − g(x)
1×1 conv
dropout 0.5

on F
1

1

1

0.5

g(x)

g(x)

g(x)

1

1

1

1

error (%)

remark

6.61

fail

fail

12.35

fail

8.70

9.81

12.86

6.91

12.22

fail

This is a plain net

init bg =0 to −5
init bg =-6

init bg =-7

init bg =0

init bg =-6

Exclusive gating. Following the Highway Networks [6,7] that adopt a gating
mechanism [5], we consider a gating function g(x) = σ(Wgx + bg) where a
transform is represented by weights Wg and biases bg followed by the sigmoid
1+e−x . In a convolutional network g(x) is realized by a 1×1
function σ(x) = 1
convolutional layer. The gating function modulates the signal by element-wise
multiplication.
We investigate the “exclusive” gates as used in [6,7] — the F path is scaled
by g(x) and the shortcut path is scaled by 1−g(x). See Fig 2(c). We ﬁnd that the
initialization of the biases bg is critical for training gated models, and following
the guidelines1 in [6,7], we conduct hyper-parameter search on the initial value of
bg in the range of 0 to -10 with a decrement step of -1 on the training set by cross-
validation. The best value (−6 here) is then used for training on the training
set, leading to a test result of 8.70% (Table 1), which still lags far behind the
ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the
results of using other initialized values, noting that the exclusive gating network
does not converge to a good solution when bg is not appropriately initialized.
The impact of the exclusive gating mechanism is two-fold. When 1 − g(x)
approaches 1, the gated shortcut connections are closer to identity which helps
information propagation; but in this case g(x) approaches 0 and suppresses the
function F. To isolate the eﬀects of the gating functions on the shortcut path
alone, we investigate a non-exclusive gating mechanism in the next.

Shortcut-only gating. In this case the function F is not scaled; only the
shortcut path is gated by 1− g(x). See Fig 2(d). The initialized value of bg is still
essential in this case. When the initialized bg is 0 (so initially the expectation
of 1 − g(x) is 0.5), the network converges to a poor result of 12.86% (Table 1).
This is also caused by higher training error (Fig 3(c)).

1 See also: people.idsia.ch/~rupesh/very_deep_learning/ by [6,7].

7

Figure 3. Training curves on CIFAR-10 of various shortcuts. Solid lines denote test
error (y-axis on the right), and dashed lines denote training loss (y-axis on the left).

When the initialized bg is very negatively biased (e.g., −6), the value of
1− g(x) is closer to 1 and the shortcut connection is nearly an identity mapping.
As such, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.
1×1 convolutional shortcut. Next we experiment with 1×1 convolutional
shortcut connections that replace the identity. This option has been investigated
in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows
good results, suggesting that 1×1 shortcut connections could be useful. But we
ﬁnd that this is not the case when there are many Residual Units. The 110-layer
ResNet has a poorer result (12.22%, Table 1) when using 1×1 convolutional
shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking
so many Residual Units (54 for ResNet-110), even the shortest path may still
impede signal propagation. We witnessed similar phenomena on ImageNet with
ResNet-101 when using 1×1 convolutional shortcuts.

Dropout shortcut. Last we experiment with dropout [11] (at a ratio of 0.5)
which we adopt on the output of the identity shortcut (Fig. 2(f)). The network
fails to converge to a good solution. Dropout statistically imposes a scale of λ
with an expectation of 0.5 on the shortcut, and similar to constant scaling by
0.5, it impedes signal propagation.

0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss110, original110, shortcut−only gating (init b=0)0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss110, original110, 1x1 conv shortcut(d)(c)(a)(b)0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss110, original110, const scaling (0.5, 0.5)0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss110, original110, exclusive gating (init b=−6)8

Table 2. Classiﬁcation error (%) on the CIFAR-10 test set with diﬀerent usages of
activation functions.

case

Fig.

ResNet-110 ResNet-164

original Residual Unit [1]

Fig. 4(a)

BN after addition

ReLU before addition

Fig. 4(b)

Fig. 4(c)

ReLU-only pre-activation

Fig. 4(d)

full pre-activation

Fig. 4(e)

6.61

8.17

7.84

6.71

6.37

5.93

6.50

6.14

5.91

5.46

Figure 4. Various usages of activation in Table 2. All these units consist of the same
components — only the orders are diﬀerent.

3.2 Discussions

As indicated by the grey arrows in Fig. 2, the shortcut connections are the
most direct paths for the information to propagate. Multiplicative manipulations
(scaling, gating, 1×1 convolutions, and dropout) on the shortcuts can hamper
information propagation and lead to optimization problems.
It is noteworthy that the gating and 1×1 convolutional shortcuts introduce
more parameters, and should have stronger representational abilities than iden-
tity shortcuts. In fact, the shortcut-only gating and 1×1 convolution cover the
solution space of identity shortcuts (i.e., they could be optimized as identity
shortcuts). However, the degradation of these complex shortcut models is caused
by optimization issues, instead of representational abilities.

4 On the Usage of Activation Functions

Experiments in the above section are consistent with the analysis in Eqn.(5)
and Eqn.(8), both being derived under the assumption that the after-addition

BNReLUweightBNweightadditionReLUxlxl+1ReLUweightBNReLUweightBNadditionxlxl+1BNReLUweightBNweightadditionReLUxlxl+1BNReLUweightBNReLUweightadditionxlxl+1weightBNReLUweightBNReLUadditionxlxl+1(a) original(b) BN after addition(c) ReLU before addition(d) ReLU-onlypre-activation(e) full pre-activation9

activation f is the identity mapping. But in the above experiments f is ReLU
as designed in [1]. Next we investigate the impact of f .

We want to make f an identity mapping, which is done by re-arranging
the activation functions (ReLU and/or BN). The original Residual Unit in [1]
has a shape in Fig. 4(a) — BN is used after each weight layer, and ReLU is
adopted after BN except that the last ReLU in a Residual Unit is after element-
wise addition (f = ReLU). Fig. 4(b-e) show the alternatives we investigated,
explained as following.

4.1 Experiments on Activation

In this section we experiment with ResNet-110 and a 164-layer Bottleneck [1]
architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of
a 1×1 layer for reducing dimension, a 3×3 layer, and a 1×1 layer for storing
dimension. As designed in [1], its computational complexity is similar to the
two-3×3 Residual Unit. More details are in the appendix. The baseline ResNet-
164 has a competitive result of 5.93% on CIFAR-10 (Table 2).

BN after addition. Before turning f into an identity mapping, we go the
opposite way by adopting BN after addition (Fig. 4(b)). In this case f involves
BN and ReLU. The results become considerably worse than the baseline (Ta-
ble 2). Unlike the original design, now the BN layer alters the signal that passes
through the shortcut and impedes information propagation, as reﬂected by the
diﬃculties on reducing training loss at the beginning of training (Fib. 6 left).

ReLU before addition. A na¨ıve choice of making f into an identity map-
ping is to move the ReLU before addition (Fig. 4(c)). However, this leads to a
non-negative output from the transform F, while intuitively a “residual” func-
tion should take values in (−∞, +∞). As a result, the forward propagated sig-
nal is monotonically increasing. This may impact the representational ability,
and the result is worse (7.84%, Table 2) than the baseline. We expect to have
a residual function taking values in (−∞, +∞). This condition is satisﬁed by
other Residual Units including the following ones.

Post-activation or pre-activation? In the original design (Eqn.(1) and
Eqn.(2)), the activation xl+1 = f (yl) aﬀects both paths in the next Residual
Unit: yl+1 = f (yl) + F(f (yl),Wl+1). Next we develop an asymmetric form
where an activation ˆf only aﬀects the F path: yl+1 = yl + F( ˆf (yl),Wl+1), for
any l (Fig. 5 (a) to (b)). By renaming the notations, we have the following form:

xl+1 = xl + F( ˆf (xl),Wl), .

(9)

It is easy to see that Eqn.(9) is similar to Eqn.(4), and can enable a backward
formulation similar to Eqn.(5). For this new Residual Unit as in Eqn.(9), the new
after-addition activation becomes an identity mapping. This design means that
if a new after-addition activation ˆf is asymmetrically adopted, it is equivalent
to recasting ˆf as the pre-activation of the next Residual Unit. This is illustrated
in Fig. 5.

10

Figure 5. Using asymmetric after-addition activation is equivalent to constructing a
pre-activation Residual Unit.

Table 3. Classiﬁcation error (%) on the CIFAR-10/100 test set using the original
Residual Units and our pre-activation Residual Units.

dataset

network

baseline unit pre-activation unit

ResNet-110(1layer)

CIFAR-10

CIFAR-100

ResNet-110

ResNet-164

ResNet-1001

ResNet-164

ResNet-1001

9.90

6.61

5.93

7.61

25.16

27.82

8.91

6.37

5.46

4.92

24.33

22.71

The distinction between post-activation/pre-activation is caused by the pres-
ence of the element-wise addition. For a plain network that has N layers, there
are N − 1 activations (BN/ReLU), and it does not matter whether we think of
them as post- or pre-activations. But for branched layers merged by addition,
the position of activation matters.

We experiment with two such designs: (i) ReLU-only pre-activation (Fig. 4(d)),

and (ii) full pre-activation (Fig. 4(e)) where BN and ReLU are both adopted be-
fore weight layers. Table 2 shows that the ReLU-only pre-activation performs
very similar to the baseline on ResNet-110/164. This ReLU layer is not used in
conjunction with a BN layer, and may not enjoy the beneﬁts of BN [8].

Somehow surprisingly, when BN and ReLU are both used as pre-activation,
the results are improved by healthy margins (Table 2 and Table 3). In Table 3 we
report results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii)
a 110-layer ResNet architecture in which each shortcut skips only 1 layer (i.e.,

originalResidualUnitact.weightweightadditionact.act.act.weightweightadditionact.......(a)adopt output activationonly to weight pathequivalent toact.weightweightadditionact.act.weightweightaddition...act....(b)asymmetricoutput activationpre-activationResidual Unitact.weightweightadditionact.act.weightweightaddition...act....(c)11

Figure 6. Training curves on CIFAR-10. Left: BN after addition (Fig. 4(b)) using
ResNet-110. Right: pre-activation unit (Fig. 4(e)) on ResNet-164. Solid lines denote
test error, and dashed lines denote training loss.

a Residual Unit has only 1 layer), denoted as “ResNet-110(1layer)”, and (iv)
a 1001-layer bottleneck architecture that has 333 Residual Units (111 on each
feature map size), denoted as “ResNet-1001”. We also experiment on CIFAR-
100. Table 3 shows that our “pre-activation” models are consistently better than
the baseline counterparts. We analyze these results in the following.

4.2 Analysis

We ﬁnd that the impact of pre-activation is twofold. First, the optimization
is further eased (comparing with the baseline ResNet) because f is an iden-
tity mapping. Second, using BN as pre-activation improves regularization of the
models.

Ease of optimization. This eﬀect is particularly obvious when training the
1001-layer ResNet. Fig. 1 shows the curves. Using the original design in [1],
the training error is reduced very slowly at the beginning of training. For f =
ReLU, the signal is impacted if it is negative, and when there are many Residual
Units, this eﬀect becomes prominent and Eqn.(3) (so Eqn.(5)) is not a good
approximation. On the contrary, when f is an identity mapping, the signal can
be propagated directly between any two units. Our 1001-layer network reduces
the training loss very quickly (Fig. 1). It also achieves the lowest loss among all
models we investigated, suggesting the success of optimization.

We also ﬁnd that the impact of f = ReLU is not severe when the ResNet
has fewer layers (e.g., 164 in Fig. 6(right)). The training curve seems to suﬀer
a little bit at the beginning of training, but goes into a healthy status soon. By
monitoring the responses we observe that this is because after some training,
the weights are adjusted into a status such that yl in Eqn.(1) is more frequently
above zero and f does not truncate it (xl is always non-negative due to the pre-
vious ReLU, so yl is below zero only when the magnitude of F is very negative).
The truncation, however, is more frequent when there are 1000 layers.

0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss110, original110, BN after add0123456x 10405101520IterationsTest Error (%)  0.0020.020.22Training Loss164, original164, proposed (pre−activation)12

Table 4. Comparisons with state-of-the-art methods on CIFAR-10 (left) and CIFAR-
100 (right). For the ResNets we also report the number of parameters. Our results are
the median of 5 runs with mean±std in the brackets.

method

NIN [12]

DSN [13]

FitNet [14]

Highway [7]

ELU [15]

ResNet-110 [1] (1.7M)

ResNet-1202 [1] (19.4M)

error (%)

8.81

8.22

8.39

7.72

6.55

6.61

7.93

method

NIN [12]

DSN [13]

FitNet [14]

Highway [7]

ELU [15]

ResNet-164 [1] (1.7M)

ResNet-1001 [1] (10.2M)

error (%)

35.68

34.57

35.04

32.39

24.28

25.16

27.82

ResNet-164 [ours] (1.7M)
ResNet-1001 [ours] (10.2M) 4.92 (4.89±0.14)

5.46

ResNet-164 [ours] (1.7M)
ResNet-1001 [ours] (10.2M) 22.71 (22.68±0.22)

24.33

Reducing overﬁtting. Another impact of using the proposed pre-activation
unit is on regularization, as shown in Fig. 6 (right). The pre-activation ver-
sion reaches slightly higher training loss at convergence, but produces lower test
error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and
ResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN’s reg-
ularization eﬀect [8]. In the original Residual Unit (Fig. 4(a)), although the BN
normalizes the signal, this is soon added to the shortcut and thus the merged
signal is not normalized. This unnormalized signal is then used as the input of
the next weight layer. On the contrary, in our pre-activation version, the inputs
to all weight layers have been normalized.

4.3 Comparisons with state-of-the-art results on CIFAR-10/100

Table 4 compares the state-of-the-art methods on CIFAR-10/100, where we
achieve competitive results. We remark that we do not specially tailor the net-
work ﬁlter sizes/numbers, nor use regularization techniques (such as dropout)
which are very eﬀective for these small datasets. We obtain these results via
a simple but essential concept — going deeper. These results demonstrate the
potential of pushing the limits of depth.

4.4 Experiments on ImageNet

Lastly we report experimental results on the 1000-class ImageNet dataset [3]. We
have done preliminary experiments using the skip connections studied in Fig. 3
on ImageNet with ResNet-101 [1] , and observed similar optimization diﬃculties.
The training error of these non-identity shortcut networks is obviously higher
than the original ResNet at the ﬁrst learning rate (similar to Fig. 3), and we
decided to halt training due to limited resources. Nevertheless, we did ﬁnish a
“BN after addition” version (Fig. 4(b)) of ResNet-101 on ImageNet and observed
higher training loss and validation error. This model’s single-crop (224×224)

13

Table 5. Comparisons of single-crop error on the ILSVRC 2012 validation set. All
ResNets are trained using the same hyper-parameters and implementations as [1]).
Our Residual Units are the full pre-activation version (Fig. 4(e)).

method
ResNet-152, original Residual Unit [1]
ResNet-152, original Residual Unit [1]
ResNet-152, proposed Residual Unit
ResNet-200, original Residual Unit [1]
ResNet-200, proposed Residual Unit
Inception v3 [16]

train crop size

224×224
224×224
224×224
224×224
224×224
299×299

test crop size

224×224
320×320
320×320
320×320
320×320
299×299

top-1 (%)

top-5 (%)

23.0
21.3
21.1
21.8
20.7
21.2

6.7
5.5
5.5
6.0
5.3
5.6

validation error is 24.6%/7.5%, vs. the original ResNet-101’s 23.6%/7.1%2. This
comparison is in line with CIFAR’s results in Fig. 6 (left).

Table 5 shows the results of ResNet-152 [1] and ResNet-2003, all trained from
scratch. We notice that the original ResNet paper [1] trained the models using
scale jittering with shorter side s ∈ [256, 480], and so the test of a 224×224 crop
on s = 256 (as did in [1]) is negatively biased. Instead, we test a single 320×320
crop from s = 320, for all original and our ResNets. Even though the ResNets
are trained on smaller crops, they can be easily tested on larger crops because
the ResNets are fully convolutional by design. This size is also close to 299×299
used by Inception v3 [16], allowing a fairer comparison.
The original ResNet-152 [1] has top-1 error of 21.3% on a 320×320 crop, and
our pre-activation counterpart has 21.1%. The gain is not big on ResNet-152
because this model has not shown severe generalization diﬃculties. However,
the original ResNet-200 has an error rate of 21.8%, higher than the baseline
ResNet-152. On the other hand, the original ResNet-200 has lower training loss
than ResNet-152, suggesting that it starts to overﬁt.

Our pre-activation ResNet-200 has an error rate of 20.7%, which is 1.1%
lower than the baseline ResNet-200 and also lower than the two versions of
ResNet-152. The result of our ResNet-200 compares favorably to Inception v3
(Table 5). Concurrent with our work, an Inception-ResNet-v2 model [17] achieves
a single-crop result of 19.9%/4.9%. We expect our observations and the proposed
Residual Unit will help this type and generally other types of ResNets.

5 Conclusions

This paper investigates the propagation formulations behind the connection
mechanisms of deep residual networks. Our derivations imply that identity short-
cut connections and identity after-addition activation are essential for making
information propagation smooth. Ablation experiments demonstrate phenom-
ena that are consistent with our derivations. We also present 1000-layer deep
networks that can be easily trained and achieve improved accuracy.

2 https://github.com/KaimingHe/deep-residual-networks
3 The ResNet-200 has 16 more 3-layer bottleneck Residual Units than ResNet-152,
which are added on the feature map of 28×28.

14

Appendix: Implementation Details

The implementation details and hyper-parameters are the same as those in [1].
Speciﬁcally, on CIFAR we use only the translation and ﬂipping augmentation in
[1] for training. The learning rate starts from 0.1, and is divided by 10 at 32k
and 48k iterations. Following [1], for all CIFAR experiments we warm up the
training by using a smaller learning rate of 0.01 at the beginning 400 iterations
and go back to 0.1 after that, although we remark that this is not necessary for
our proposed Residual Unit. The mini-batch size is 128 on 2 GPUs (64 each),
the weight decay is 0.0001, the momentum is 0.9, and the weights are initialized
as in [18].

On ImageNet, we train the models using the same data augmentation as in
[1]. The learning rate starts from 0.1 (no warming up), and is divided by 10 at
30 and 60 epochs. The mini-batch size is 256 on 8 GPUs (32 each). The weight
decay, momentum, and weight initialization are the same as above.

When using the pre-activation Residual Units (Fig. 4(d)(e) and Fig. 5), we
pay special attention to the ﬁrst and the last Residual Units of the entire net-
work. For the ﬁrst Residual Unit (that follows a stand-alone convolutional layer,
conv1), we adopt the ﬁrst activation right after conv1 and before splitting into
two paths; for the last Residual Unit (followed by average pooling and a fully-
connected classiﬁer), we adopt an extra activation right after its element-wise
addition. These two special cases are the natural outcome when we obtain the
pre-activation network via the modiﬁcation procedure as shown in Fig. 5.

structed following [1]. For example, a

The bottleneck Residual Units (for ResNet-164/1001 on CIFAR) are con-
unit in ResNet-110 is replaced

 unit in ResNet-164, both of which have roughly the same num-

with a

 1×1, 16

3×3, 16
1×1, 64

(cid:35)

(cid:34)

3×3, 16
3×3, 16

ber of parameters. For the bottleneck ResNets, when reducing the feature map
size we use projection shortcuts [1] for increasing dimensions, and when pre-
activation is used, these projection shortcuts are also with pre-activation.

References

1. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR. (2016)

2. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann ma-

chines. In: ICML. (2010)

3. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. IJCV (2015)

4. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,

Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV. (2014)

5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

(1997)

6. Srivastava, R.K., Greﬀ, K., Schmidhuber, J.: Highway networks. In: ICML work-

shop. (2015)

15

7. Srivastava, R.K., Greﬀ, K., Schmidhuber, J.: Training very deep networks.

In:

NIPS. (2015)

8. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015)

9. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,
Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural
computation (1989)

10. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech Report

(2009)

11. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.:
feature detectors.

Improving neural networks by preventing co-adaptation of
arXiv:1207.0580 (2012)

12. Lin, M., Chen, Q., Yan, S.: Network in network. In: ICLR. (2014)
13. Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.: Deeply-supervised nets. In:

AISTATS. (2015)

14. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:

Hints for thin deep nets. In: ICLR. (2015)

15. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network

learning by exponential linear units (ELUs). In: ICLR. (2016)

16. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-

tion architecture for computer vision. In: CVPR. (2016)

17. Szegedy, C., Ioﬀe, S., Vanhoucke, V.: Inception-v4, inception-resnet and the impact

of residual connections on learning. arXiv:1602.07261 (2016)

18. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV. (2015)

